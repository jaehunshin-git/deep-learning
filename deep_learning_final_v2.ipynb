{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI7YcRm0Ae-c"
      },
      "source": [
        "# 개체명 인식(Named Entity Recognition, NER) 모델 학습에 대한 기록\n",
        "\n",
        "이 노트는 한국어 텍스트 데이터에서 특정 정보를 추출(예: 질문 ID, 법령 내용)하기 위한 개체명 인식(NER) 딥러닝 모델을 학습하는 과정에 대한 기록입니다.\n",
        "\n",
        "**학습 목표:**\n",
        "1. Google Colab 환경 설정 및 Google Drive 연동.\n",
        "2. 원본 텍스트 데이터 전처리 (Doccano 임포트 전).\n",
        "3. Doccano를 이용한 웹 기반 데이터 라벨링.\n",
        "4. 라벨링된 데이터를 딥러닝 모델 학습에 적합한 형태로 변환 (Hugging Face `datasets` 라이브러리).\n",
        "5. 사전 학습된 BERT 모델(`klue/bert-base`)을 활용하여 NER 모델 파인튜닝.\n",
        "6. 학습된 모델의 성능 평가 및 추론(Inference) 방법 이해.\n",
        "\n",
        "**참고:** 이 튜토리얼은 Doccano와 Hugging Face `transformers` 라이브러리를 사용합니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2BXJgIVAe-g"
      },
      "source": [
        "---\n",
        "\n",
        "### 0. 프로젝트 개요 및 동기\n",
        "\n",
        "이 프로젝트는 회사에서 할당받은 업무를 효율적으로 해결하기 위해 시작되었습니다. 저에게 주어진 업무는 **각 공공기관 중앙부처의 법령해석 데이터셋을 웹 크롤링하여 PDF 또는 HWP 파일 내의 질의와 회신을 한 세트로 파싱하고 검증**하는 것이었습니다.\n",
        "\n",
        "구체적으로는 다음과 같은 과제를 안고 있었습니다:\n",
        "\n",
        "1.  **질의-회신 세트 개수 세기**: 웹 크롤링된 문서에서 '질의 1개'와 '회신 1개'가 정상적으로 하나의 세트를 이루는지 확인하고, 전체 세트의 개수를 파악해야 했습니다.\n",
        "2.  **질의-회신 연결성 판단**: 질의와 회신 간의 연결이 논리적으로 올바른지, 즉 '질의 1'에 대한 '회신 1'이 제대로 매칭되는지 판단해야 했습니다.\n",
        "3.  **특이사항 분류**: 특히 질의와 회신의 개수가 맞지 않거나, 회신 내용이 법령에 근거하지 않은 답변일 경우 이를 '특이사항'으로 분류하는 업무가 수반되었습니다.\n",
        "\n",
        "문제는 이러한 분류 및 검증 작업을 수행해야 할 질의회신 세트가 **총 1만 2천 개**에 달한다는 점이었습니다. 이는 엄청난 **수작업과 시간 소모**를 요구하는 일이었습니다. 컴퓨터 공학 전공자로서 이러한 비효율적인 업무를 제 전공 지식을 활용하여 자동화하고 해결하고자 하는 강한 동기를 느꼈습니다. 단순히 반복 작업을 줄이는 것을 넘어, 이 프로젝트를 통해 **딥러닝, 데이터 라벨링, 도커(Docker), 그리고 자연어 처리(NLP) 개념**을 실질적으로 배우고 적용하는 것을 목표로 삼았습니다. 이 보고서는 이러한 프로젝트에 대한 기록입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnBNY7kNAe-g"
      },
      "source": [
        "### 프로젝트 전체 플로우 (이미지)\n",
        "\n",
        "아래는 이 튜토리얼에서 진행할 전체 프로젝트의 워크플로우를 이미지로 시각화한 것입니다. (w/ mermaid)\n",
        "![프로젝트 플로우차트](https://github.com/jaehunshin-git/deep-learning/blob/main/deep-learning-flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frk4GrteAe-h"
      },
      "source": [
        "## 프로젝트 전체 플로우 (텍스트 다이어그램)\n",
        "\n",
        "아래는 이 튜토리얼에서 진행할 전체 프로젝트의 워크플로우를 텍스트 형태로 시각화한 것입니다. 각 단계의 흐름을 이해하는 데 도움이 될 것입니다.\n",
        "\n",
        "```text\n",
        "## 프로젝트 전체 플로우\n",
        "\n",
        "1.  시작 - 딥러닝 NER 모델 학습\n",
        "    -> 2. Google Colab 환경 설정\n",
        "        -> 2.1 런타임 유형 GPU 변경\n",
        "        -> 2.2 Google Drive 마운트\n",
        "        -> 2.3 프로젝트 디렉토리 구조 설정\n",
        "        -> 2.4 필요 라이브러리 설치\n",
        "\n",
        "2.4 필요 라이브러리 설치\n",
        "    -> 3. 데이터 준비 - 원본 텍스트 전처리 (로컬 PC)\n",
        "        -> 3.1 원본 dataset.txt 로드\n",
        "        -> 3.2 특정 문자/숫자/워딩 제거 - 정규표현식\n",
        "        -> 3.3 각 질의/회신 쌍 분리 - 개행 문자 추가\n",
        "        -> 3.4 dataset_cleaned_final.txt 저장\n",
        "\n",
        "3.4 dataset_cleaned_final.txt 저장\n",
        "    -> 4. Doccano 데이터 라벨링 (로컬 PC Docker)\n",
        "        -> 4.1 Doccano Docker 설치 및 실행\n",
        "        -> 4.2 Doccano 프로젝트 생성 - 레이블 정의\n",
        "        -> 4.3 dataset_cleaned_final.txt Import - Plain Text\n",
        "        -> 4.4 수동 개체명 라벨링 수행\n",
        "        -> 4.5 라벨링된 데이터 Export - JSONL - Approved Only\n",
        "\n",
        "4.5 라벨링된 데이터 Export - JSONL - Approved Only\n",
        "    -> 5. Google Drive 업로드\n",
        "        -> 6. Colab에서 라벨링된 데이터 로드 및 전처리\n",
        "            -> 6.1 Doccano JSONL 파일 로드\n",
        "            -> 6.2 BIO 태깅을 위한 레이블 목록 생성\n",
        "            -> 6.3 텍스트 토큰화 - 레이블 정렬 - padding, truncation 포함\n",
        "            -> 6.4 Hugging Face Dataset 객체로 변환\n",
        "            -> 6.5 Train/Validation 데이터셋 분할\n",
        "\n",
        "6.5 Train/Validation 데이터셋 분할\n",
        "    -> 7. BERT 기반 NER 모델 학습\n",
        "        -> 7.1 klue/bert-base 모델 및 토크나이저 로드\n",
        "        -> 7.2 성능 지표 Metrics 정의 - seqeval\n",
        "        -> 7.3 학습 인자 Training Arguments 설정 - eval_strategy 등\n",
        "        -> 7.4 Trainer 객체 생성\n",
        "        -> 7.5 trainer.train() 실행\n",
        "\n",
        "7.5 trainer.train() 실행\n",
        "    -> 8. 학습된 모델 평가 및 추론\n",
        "        -> 8.1 학습된 모델 저장\n",
        "        -> 8.2 저장된 모델 및 토크나이저 로드\n",
        "        -> 8.3 단일 텍스트 추론 함수 정의\n",
        "        -> 8.4 예시 문장으로 모델 테스트\n",
        "\n",
        "8.4 예시 문장으로 모델 테스트\n",
        "    -> 9. 결과 분석 - 모델 성능 평가\n",
        "        -> 10. 성능이 충분한가?\n",
        "            -> Yes -> 11. 배포 및 활용\n",
        "            -> No  -> 12. 성능 향상 전략 적용\n",
        "                -> 12.1 더 많은 데이터 라벨링 - 가장 중요!\n",
        "                -> 12.2 하이퍼파라미터 튜닝\n",
        "                -> 12.3 모델 아키텍처/전이 학습 고려\n",
        "                -> 12.1 더 많은 데이터 라벨링 - 가장 중요! -> 4. Doccano 데이터 라벨링 (로컬 PC Docker)\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS3XdVsWAe-h"
      },
      "source": [
        "## 0. Google Colab 환경 설정\n",
        "\n",
        "모델 학습을 위해 Google Colab 환경을 설정합니다.\n",
        "\n",
        "### 0.1 런타임 유형 변경 (GPU 활성화)\n",
        "Colab에서 딥러닝 모델을 빠르게 학습시키려면 GPU를 사용하는 것이 필수적입니다.\n",
        "\n",
        "1.  Colab 상단 메뉴에서 `런타임` (Runtime)을 클릭합니다.\n",
        "2.  `런타임 유형 변경` (Change runtime type)을 선택합니다.\n",
        "3.  `하드웨어 가속기` (Hardware accelerator) 드롭다운 메뉴에서 `GPU`를 선택한 후 `저장` (Save)을 클릭합니다.\n",
        "\n",
        "**주의:**\n",
        "\n",
        "\n",
        "1.   런타임을 변경하거나 Colab 세션이 끊겼다가 다시 연결되면, 이전에 설치했던 라이브러리나 정의했던 변수들이 초기화됩니다. 이 경우 다음 단계를 다시 실행해야 합니다.\n",
        "2.   google.colab 라이브러리는 colab 전용 라이브러리이므로 로컬에서는 설치 및 사용할 수 없습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOsLz5_7KDXz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEDZ_B4ZAe-j"
      },
      "source": [
        "### 0.3 프로젝트 디렉토리 구조 설정\n",
        "Google Drive 내부에 프로젝트를 위한 폴더 구조를 설정합니다. 데이터와 모델을 체계적으로 관리할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M07W0c9lD-T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 프로젝트의 루트 경로 설정 (이전에 당신이 설정한 Google Drive 경로)\n",
        "# Colab Notebooks 폴더 아래에 deep-learning 이라는 폴더를 만들어 이 프로젝트의 루트로 사용한다고 가정합니다.\n",
        "# 이 경로는 사용자의 Google Drive 구조에 맞게 수정해주세요.\n",
        "project_root = '/content/gdrive/MyDrive/Colab Notebooks/deep-learning/'\n",
        "\n",
        "# 데이터 및 모델 저장을 위한 하위 디렉토리 정의\n",
        "data_dir = os.path.join(project_root, 'data')\n",
        "model_dir = os.path.join(project_root, 'model')\n",
        "\n",
        "# 디렉토리 생성 (이미 존재하면 건너뛰기)\n",
        "os.makedirs(project_root, exist_ok=True)\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Project Root: {project_root}\")\n",
        "print(f\"Data Directory: {data_dir}\")\n",
        "print(f\"Model Directory: {model_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP5bg9mhAe-j"
      },
      "source": [
        "### 0.4 필요한 라이브러리 설치\n",
        "BERT 모델을 사용하기 위한 Hugging Face `transformers`와 `datasets` 라이브러리를 설치합니다. `accelerate`는 학습 가속화에 도움을 줍니다.\n",
        "\n",
        "\n",
        "1. `transformers`\n",
        "    - 무엇인가요?\n",
        "\n",
        "        - Hugging Face에서 개발한 가장 인기 있는 딥러닝 라이브러리 중 하나입니다. BERT, GPT-2, RoBERTa 등 다양한 사전 학습된 트랜스포머 기반 모델(Transformer-based models)과 토크나이저(Tokenizer)를 제공합니다. 자연어 처리(NLP) 분야의 최신 모델들을 쉽게 불러와 파인튜닝(fine-tuning)하거나 추론(inference)할 수 있도록 돕습니다.\n",
        "\n",
        "    - 내 프로젝트에서 활용:\n",
        "\n",
        "        - 모델 로드: `AutoModelForTokenClassification.from_pretrained(\"klue/bert-base\")`를 사용하여 한국어에 특화된 BERT 모델인 klue/bert-base를 개체명 인식(Token Classification) 작업에 맞게 불러옵니다.\n",
        "\n",
        "        - 토크나이저 로드: AutoTokenizer.from_pretrained(\"klue/bert-base\")를 통해 텍스트를 모델이 이해할 수 있는 토큰(단어 조각) ID로 변환하는 데 필요한 토크나이저를 로드합니다.\n",
        "\n",
        "        - 모델 학습 관리: Trainer 클래스를 사용하여 학습 인자(TrainingArguments) 설정, 학습 데이터와 평가 데이터 지정, 모델 학습(trainer.train()) 등 복잡한 학습 과정을 편리하게 관리합니다.\n",
        "\n",
        "        - 모델 저장 및 로드: 학습된 모델을 저장하고 나중에 다시 불러와 추론에 활용하는 기능도 이 라이브러리를 통해 수행됩니다.\n",
        "\n",
        "2. `datasets`\n",
        "    - 무엇인가요?\n",
        "        - Hugging Face에서 개발한 또 다른 핵심 라이브러리로, 대규모 데이터셋을 효율적으로 로드, 전처리, 저장할 수 있도록 설계되었습니다. 특히 NLP 작업에 최적화되어 있으며, 다양한 공개 데이터셋을 쉽게 가져올 수 있는 기능도 제공합니다.\n",
        "\n",
        "    - 내 프로젝트에서 활용:\n",
        "\n",
        "        - 데이터 로드: Doccano에서 내보낸 .jsonl 형식의 라벨링된 데이터를 로드하고 Pandas DataFrame을 거쳐 datasets.Dataset 객체로 변환합니다. Dataset 객체는 Hugging Face Trainer에 직접 입력될 수 있는 형태로, 데이터 처리를 효율적으로 만듭니다.\n",
        "\n",
        "        - 데이터 전처리: Dataset 객체에 토크나이징된 input_ids, attention_mask, 그리고 각 토큰에 해당하는 labels (BIO 태그)를 저장하는 데 사용됩니다.\n",
        "\n",
        "        - 데이터 분할: train_test_split 기능을 사용하여 라벨링된 전체 데이터를 학습용(train_dataset)과 평가용(eval_dataset)으로 쉽게 나눕니다.\n",
        "\n",
        "3. `accelerate`\n",
        "    - 무엇인가요?\n",
        "        - Hugging Face에서 개발한 경량 라이브러리로, PyTorch 모델의 학습 과정을 분산 및 혼합 정밀도(mixed-precision) 학습 환경에 맞게 자동으로 조정해 줍니다. 개발자가 복잡한 분산 학습 코드를 직접 작성할 필요 없이, 소수의 코드 변경만으로 GPU 여러 개나 TPU 같은 가속기를 활용할 수 있도록 돕습니다.\n",
        "\n",
        "    - 내 프로젝트에서 활용:\n",
        "\n",
        "        - 당신의 Colab 환경에서 GPU 런타임을 사용하도록 설정했습니다. accelerate 라이브러리는 transformers의 Trainer 내부적으로 활용되어 GPU 자원을 효율적으로 사용하여 모델 학습 속도를 가속화합니다. 이는 대규모 모델과 데이터셋을 학습할 때 필수적인 역할을 합니다.\n",
        "\n",
        "4. `seqeval`\n",
        "    - 무엇인가요?\n",
        "        - 시퀀스 라벨링(Sequence Labeling) 작업(개체명 인식, 품사 태깅 등)의 성능을 평가하기 위한 Python 라이브러리입니다. 특히 precision, recall, f1-score, accuracy와 같은 주요 지표들을 계산하는 데 특화되어 있습니다. 개체명 인식에서는 단순히 개별 토큰의 정확도뿐만 아니라, **전체 개체명 스팬(span)**이 정확하게 예측되었는지를 평가하는 것이 중요하며, seqeval이 이러한 역할을 수행합니다.\n",
        "\n",
        "    - 내 프로젝트에서 활용:\n",
        "\n",
        "        - `load_metric(\"seqeval\")`를 사용하여 평가 지표를 로드합니다.\n",
        "\n",
        "        - `compute_metrics` 함수 내에서 모델의 예측 결과와 실제 레이블을 seqeval 포맷에 맞게 변환하고, 이를 metric.compute() 함수에 전달하여 최종 정밀도, 재현율, F1-점수 등을 계산하는 데 사용됩니다. 이 지표들은 모델의 학습 과정을 모니터링하고 최종 성능을 평가하는 데 핵심적인 역할을 합니다.\n",
        "\n",
        "이 라이브러리들은 모두 Hugging Face 생태계의 일부로, 복잡한 딥러닝 NLP 프로젝트를 효율적으로 개발하고 관리하는 데 큰 도움을 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate seqeval\n",
        "# transformers 버전이 너무 낮으면 Warning이 발생할 수 있으므로, 최신 버전으로 업그레이드합니다.\n",
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "1mUOCBKBohkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTSddOsUAe-k"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 데이터 준비 및 전처리 (Doccano 연동 전)\n",
        "\n",
        "원본 텍스트 파일(`dataset.txt`)을 Doccano에 효율적으로 라벨링하기 위해 사전 전처리 작업을 수행합니다. 이 과정은 불필요한 노이즈를 제거하고, 각 질의/회신 쌍을 Doccano가 개별 문서로 인식할 수 있도록 분리하는 역할을 합니다.\n",
        "\n",
        "**원본 `dataset.txt` 파일의 문제점:**\n",
        "- 불필요한 숫자나 특정 워딩(예: '2021년 소방시설법령 질의회신집') 포함.\n",
        "- 모든 질의/회신이 하나의 긴 텍스트로 이어져 있어, Doccano에서 개별 문서로 처리하기 어려움.\n",
        "\n",
        "이 전처리 스크립트는 **Google Colab이 아닌, 로컬 환경(예: VS Code)**에서 원본 텍스트 파일을 처리한 후, 그 결과 파일을 Google Drive에 업로드하여 Doccano로 가져오는 것을 권장합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "azs-ewTQnKDL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_5d6OsVAe-k"
      },
      "source": [
        "### 1.1 `dataset.txt` 파일 클리닝 및 문서 분리 코드\n",
        "\n",
        "아래 코드를 로컬 컴퓨터의 `.py` 파일로 저장(예: `preprocess_dataset.py`)하고, 원본 `dataset.txt` 파일이 있는 동일한 폴더에서 실행하세요.\n",
        "\n",
        "``` python\n",
        "import os\n",
        "import re\n",
        "\n",
        "def clean_and_prepare_text_for_doccano_final_v2(input_filepath, output_filepath):\n",
        "    \"\"\"\n",
        "    주어진 텍스트 파일에서 다음을 수행합니다:\n",
        "    1. 한 줄에 숫자(0-9) 하나만 있는 라인을 삭제합니다.\n",
        "    2. '2021년 소방시설법령 질의회신집' 문자열을 제거합니다.\n",
        "    3. '질의 N.' (또는 '질의 N')으로 시작하는 줄 앞에 빈 줄(\\n\\n)을 추가합니다.\n",
        "       (단, 파일의 맨 처음 나오는 '질의 1.' 앞에는 추가하지 않습니다.)\n",
        "       이때, 각 '질의 N.' 블록이 정확히 '\\n\\n'으로 구분되도록 합니다.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(input_filepath, 'r', encoding='utf-8') as f_in:\n",
        "            lines = f_in.readlines() # 파일을 줄 단위로 읽어옵니다.\n",
        "\n",
        "        temp_content = [] # 임시로 클리닝된 줄을 저장할 리스트\n",
        "        for line in lines:\n",
        "            # 1단계: '2021년 소방시설법령 질의회신집' 문자열 제거\n",
        "            line = line.replace('2021년 소방시설법령 질의회신집', '')\n",
        "\n",
        "            # 2단계: 한 줄에 숫자 하나만 있는 라인 삭제\n",
        "            if line.strip().isdigit() and len(line.strip()) == 1:\n",
        "                continue # 해당 줄은 건너뛰고 다음 줄로 넘어갑니다.\n",
        "\n",
        "            # 모든 줄의 양쪽 공백 제거 후 임시 리스트에 추가 (원래 줄바꿈도 제거)\n",
        "            temp_content.append(line.strip())\n",
        "\n",
        "        # 임시 리스트의 줄들을 하나의 문자열로 결합 (각 줄 사이에 공백 1개로 연결)\n",
        "        cleaned_raw_text = ' '.join(temp_content).strip()\n",
        "\n",
        "        # 3단계: '질의 N.' 앞에 빈 줄 추가\n",
        "        # '질의 N.' (또는 '질의 N') 패턴을 찾아서 '\\n\\n질의 N.'으로 교체합니다.\n",
        "        # 단, 파일의 맨 처음 나오는 '질의 1.' 앞에는 추가하지 않습니다.\n",
        "\n",
        "        # re.sub의 repl 매개변수에 함수를 사용하여 동적 교체\n",
        "        def replace_query_marker(match):\n",
        "            # match.start() == 0 이면 파일의 맨 처음 '질의 N.'입니다.\n",
        "            if match.start() == 0:\n",
        "                return match.group(0) # '질의 N.' 자체를 반환 (앞에 아무것도 안 붙임)\n",
        "            else:\n",
        "                return '\\n\\n' + match.group(0) # 그 외의 '질의 N.' 앞에는 '\\n\\n'을 붙임\n",
        "\n",
        "        # 패턴: '질의' 다음에 공백(0개 이상), 숫자(1개 이상), 점(선택적)\n",
        "        final_content = re.sub(r'질의\\s*\\d+\\.?', replace_query_marker, cleaned_raw_text)\n",
        "\n",
        "        final_content = re.sub(r'(회신\\s*\\d\\.?)', r'\\n\\1', final_content)\n",
        "        \n",
        "        final_content = re.sub(r'▣.*', '', final_content)  # '▣'로 시작하는 줄 제거\n",
        "        \n",
        "        final_content = re.sub(r'\\s\\d{2,3}\\s', '', final_content)  # 숫자(2-3자리) 제거\n",
        "\n",
        "        # 최종적으로 문자열의 시작과 끝에 불필요한 공백/개행을 제거\n",
        "        final_content = final_content.strip()\n",
        "\n",
        "        with open(output_filepath, 'w', encoding='utf-8') as f_out:\n",
        "            f_out.write(final_content)\n",
        "\n",
        "        print(f\"파일이 성공적으로 처리되어 '{output_filepath}'에 저장되었습니다.\")\n",
        "        print(\"이 파일을 Doccano에 'Plain Text' 형식으로 가져오시면 됩니다.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"오류: 입력 파일 '{input_filepath}'을(를) 찾을 수 없습니다. 경로를 확인하세요.\")\n",
        "    except Exception as e:\n",
        "        print(f\"파일 처리 중 오류 발생: {e}\")\n",
        "\n",
        "# --- 사용 방법 (아래 경로를 당신의 실제 파일 경로로 수정해주세요) ---\n",
        "\n",
        "# 로컬 프로젝트 루트 경로 (VS Code에서 해당 파일이 있는 폴더 경로)\n",
        "# 예: r\"C:\\Users\\JHSHIN\\ProgrammingCodes\\deep-learning\"\n",
        "project_root = r\"C:\\Users\\JHSHIN\\ProgrammingCodes\\deep-learning\"\n",
        "\n",
        "# 원본 입력 파일 경로 (당신이 가지고 있는 원본 dataset.txt 파일)\n",
        "input_file = os.path.join(project_root, 'dataset.txt')\n",
        "\n",
        "# 수정된 내용을 저장할 새로운 출력 파일 경로 (새로운 이름으로 저장하는 것을 권장)\n",
        "output_file = os.path.join(project_root, 'dataset_cleaned.txt') # 파일 이름 변경\n",
        "\n",
        "# 함수 실행\n",
        "clean_and_prepare_text_for_doccano_final_v2(input_file, output_file)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8N8ZjC4Ae-k"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Doccano를 이용한 데이터 라벨링\n",
        "\n",
        "Doccano는 텍스트 어노테이션(라벨링)을 위한 오픈소스 도구입니다. 웹 기반 환경에서 직관적으로 개체명(NER) 라벨링을 수행할 수 있습니다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bb5q7cERn_aD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lM6oGuIAe-l"
      },
      "source": [
        "### 2.1 Doccano 설치 및 실행 (Docker 사용)\n",
        "Doccano는 Docker를 이용하여 가장 쉽게 설치하고 실행할 수 있습니다. 로컬 컴퓨터에 Docker가 설치되어 있어야 합니다.\n",
        "\n",
        "1.  **Docker 설치:** Docker Desktop을 다운로드하여 설치합니다: [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)\n",
        "\n",
        "2.  **Doccano Docker 이미지 다운로드 및 실행:** 터미널 또는 명령 프롬프트에서 다음 명령어를 실행합니다.\n",
        "\n",
        "    ```bash\n",
        "    docker pull doccano/doccano\n",
        "    docker run -it -p 8000:8000 doccano/doccano\n",
        "    ```\n",
        "\n",
        "3.  **Doccano 접속:** 웹 브라우저를 열고 `http://localhost:8000/`으로 접속합니다. 기본 관리자 계정은 `admin / admin`입니다.\n",
        "\n",
        "### 2.2 Doccano 프로젝트 생성 및 설정\n",
        "1.  **새 프로젝트 생성:** Doccano 웹 UI에서 `Create new project` 버튼을 클릭합니다.\n",
        "2.  **프로젝트 이름 및 설명 입력:** 프로젝트 이름을 지정하고(예: '법령 질의회신 NER') 설명을 추가합니다.\n",
        "3.  **프로젝트 유형 선택:** `Sequence Labeling` (개체명 인식을 위한 유형)을 선택합니다.\n",
        "4.  **레이블 정의:** NER 모델이 인식할 개체명 레이블들을 정의합니다. 예를 들어:\n",
        "    * `QUESTION_ID`\n",
        "    * `QUESTION_CONTENT`\n",
        "    * `ANSWER_ID`\n",
        "    * `ANSWER_CONTENT`\n",
        "    * `MISC_HEADER` (예: 고시 번호, 문서 제목)\n",
        "    * `LAW_CONTENT` (예: 특정 법령 조항, 법률 이름)\n",
        "\n",
        "    각 레이블에 단축키와 색상을 지정하면 라벨링 효율을 높일 수 있습니다.\n",
        "\n",
        "### 2.3 클리닝된 데이터 Doccano로 가져오기 (Import)\n",
        "1.  **Import Data 탭 이동:** 생성된 프로젝트 페이지에서 `Import Data` 탭을 클릭합니다.\n",
        "2.  **파일 선택:** `dataset_cleaned_final.txt` 파일을 선택합니다.\n",
        "3.  **파일 형식 선택:** `Plain Text`를 선택합니다. (우리가 `\\n\\n`으로 문서를 분리했기 때문에, Doccano가 이를 개별 문서로 인식합니다.)\n",
        "4.  **Import 시작:** `Import` 버튼을 클릭합니다.\n",
        "\n",
        "### 2.4 개체명 라벨링 수행\n",
        "1.  **Annotate Data 탭 이동:** `Annotate Data` 탭을 클릭합니다.\n",
        "2.  **텍스트 선택 및 라벨 지정:** 각 문서의 텍스트를 읽고, 해당하는 단어나 구절을 드래그하여 선택한 후, 오른쪽에 나타나는 레이블 버튼(또는 지정된 단축키)을 클릭하여 라벨을 지정합니다.\n",
        "3.  **작업 저장 및 다음 문서로 이동:** 한 문서의 라벨링을 마쳤으면 `Submit` 또는 `Save` 버튼을 눌러 저장하고 다음 문서로 넘어갑니다.\n",
        "\n",
        "### 2.5 라벨링 완료 후 데이터 내보내기 (Export)\n",
        "충분한 양의 데이터를 라벨링했다면 (초기 학습을 위해 최소 100개 이상, 실사용을 위해 수백~수천 개 권장), 이제 라벨링된 데이터를 내보냅니다.\n",
        "\n",
        "1.  **Export Data 탭 이동:** 프로젝트 페이지에서 `Export Data` 탭을 클릭합니다.\n",
        "2.  **파일 형식 선택:** `JSONL`을 선택합니다. (Hugging Face `datasets` 라이브러리가 쉽게 로드할 수 있는 형식입니다.)\n",
        "3.  **'Export only approved documents' 체크 (매우 중요!)**:\n",
        "    이 옵션을 **반드시 체크**합니다. 이 옵션은 라벨링이 완료되고 `Approve` (승인)된 문서만 내보내어 학습 데이터의 품질을 보장합니다. 승인되지 않은 문서는 아직 검토가 필요하거나 수정될 여지가 있는 것으로 간주됩니다.\n",
        "    \n",
        "    * **Approve 하는 방법:** `Annotate` 화면에서 각 문서의 라벨링을 마친 후 `Approve` 버튼을 클릭하거나, `Annotation` -> `Guideline` -> `Approve` 로 이동하여 일괄 승인할 수 있습니다.\n",
        "\n",
        "4.  **Export 시작:** `Export` 버튼을 클릭하여 `.jsonl` 파일을 다운로드합니다.\n",
        "\n",
        "**다운로드한 `.jsonl` 파일을 Google Drive의 `data` 폴더(예: `/content/gdrive/MyDrive/Colab Notebooks/deep-learning/data/`)로 업로드합니다.** 파일 이름을 기억해두세요 (예: `after_datalabeling.jsonl`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYHjdUpWAe-l"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKuEbPmPAe-l"
      },
      "source": [
        "## 3. Colab에서 라벨링된 데이터 로드 및 전처리\n",
        "\n",
        "Doccano에서 내보낸 `.jsonl` 파일을 Google Colab으로 가져와 BERT 모델 학습에 적합한 형태로 전처리합니다. 이 과정에서 각 단어(또는 서브워드 토큰)에 BIO(Beginning, Inside, Outside) 태그를 할당합니다.\n",
        "\n",
        "**⚠️ 중요: Colab 런타임이 끊어졌다면, 반드시 `0.2 Google Drive 마운트`, `0.3 프로젝트 디렉토리 구조 설정`, `0.4 필요한 라이브러리 설치` 셀을 다시 실행한 후 이 단계를 진행해주세요.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMQNzsRcAe-l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- 1. Doccano에서 정의한 레이블 목록 (B-, I- 없이) ---\n",
        "# Doccano에서 프로젝트 설정 시 정의했던 레이블 이름들을 여기에 정확히 입력합니다.\n",
        "doccano_raw_labels = [\n",
        "    \"QUESTION_ID\",\n",
        "    \"QUESTION_CONTENT\",\n",
        "    \"ANSWER_ID\",\n",
        "    \"ANSWER_CONTENT\",\n",
        "    \"MISC_HEADER\",\n",
        "    \"LAW_CONTENT\"\n",
        "]\n",
        "\n",
        "# --- 2. 모델 학습을 위한 최종 BIO 레이블 목록 및 매핑 ---\n",
        "# 'O' (Other) 태그는 라벨링되지 않은 모든 토큰을 의미하며 항상 포함됩니다.\n",
        "# 각 원본 레이블에 대해 'B-' (Beginning)와 'I-' (Inside) 태그를 생성합니다.\n",
        "label_list = [\"O\"] # 'Other' 태그는 항상 포함\n",
        "for label in doccano_raw_labels:\n",
        "    label_list.append(f\"B-{label}\") # Beginning 태그: 개체명의 첫 번째 토큰\n",
        "    label_list.append(f\"I-{label}\") # Inside 태그: 개체명의 두 번째 이후 토큰\n",
        "\n",
        "# 레이블 이름과 정수 ID 간의 매핑을 생성합니다.\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "num_labels = len(label_list) # 모델의 출력 레이어 크기에 사용됩니다.\n",
        "\n",
        "print(f\"모델 학습을 위한 최종 레이블 목록: {label_list}\")\n",
        "print(f\"총 레이블 개수: {num_labels}\")\n",
        "\n",
        "# --- 3. Doccano에서 내보낸 JSONL 파일 경로 설정 ---\n",
        "# 이 경로는 0.3단계에서 설정한 data_dir과 일치해야 합니다.\n",
        "# Colab 런타임이 재시작되면 변수가 초기화될 수 있으므로, 방어 코드를 추가하거나 0.3단계 셀을 다시 실행해야 합니다.\n",
        "try:\n",
        "    # project_root가 정의되지 않았다면 (런타임 재시작 등), 기본 경로를 설정\n",
        "    if 'project_root' not in locals():\n",
        "        project_root = '/content/gdrive/MyDrive/Colab Notebooks/deep-learning/'\n",
        "        data_dir = os.path.join(project_root, 'data')\n",
        "        print(\"경로 변수 'project_root'가 정의되지 않아 기본 경로를 사용합니다.\")\n",
        "except NameError:\n",
        "    # NameError 발생 시 (아예 변수 선언이 안 되어 있을 때)\n",
        "    project_root = '/content/gdrive/MyDrive/Colab Notebooks/deep-learning/'\n",
        "    data_dir = os.path.join(project_root, 'data')\n",
        "    print(\"경로 변수 'project_root'가 정의되지 않아 기본 경로를 설정했습니다.\")\n",
        "\n",
        "# Doccano에서 내보낸 실제 JSONL 파일 이름을 여기에 입력하세요.\n",
        "labeled_data_file_path = os.path.join(data_dir, 'after_datalabeling.jsonl')\n",
        "\n",
        "# --- 4. 사용할 토크나이저 ---\n",
        "# 한국어 BERT 모델인 'klue/bert-base' 토크나이저를 로드합니다.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "# --- 5. Doccano JSONL 로드 및 Hugging Face Dataset 형식으로 변환 ---\n",
        "converted_data_for_hf = []\n",
        "try:\n",
        "    with open(labeled_data_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            doc_data = json.loads(line)\n",
        "            text = doc_data['text']\n",
        "            # Doccano의 'label' 필드는 [[start_offset, end_offset, \"LABEL_NAME\"], ...] 형식입니다.\n",
        "            annotations = doc_data.get('label', [])\n",
        "\n",
        "            # 텍스트를 토큰화하고 각 토큰의 원본 텍스트에서의 위치(offset)를 함께 가져옵니다.\n",
        "            # 중요: padding과 truncation을 활성화하여 모든 시퀀스 길이를 통일합니다.\n",
        "            tokenized_output = tokenizer(\n",
        "                text,\n",
        "                return_offsets_mapping=True, # 토큰의 원본 텍스트에서의 시작/끝 오프셋 반환\n",
        "                truncation=True,             # max_length를 초과하는 시퀀스는 잘라냄\n",
        "                max_length=512,              # BERT 모델의 최대 입력 길이 (일반적으로 512)\n",
        "                padding=\"max_length\"       # 모든 시퀀스를 max_length에 맞춰 패딩\n",
        "            )\n",
        "\n",
        "            input_ids = tokenized_output['input_ids']\n",
        "            offsets = tokenized_output['offset_mapping']\n",
        "\n",
        "            # 각 토큰에 해당하는 BIO 레이블을 초기화합니다.\n",
        "            # -100은 손실 계산에서 무시될 특수 토큰(CLS, SEP 등)이나 패딩 토큰에 할당됩니다.\n",
        "            labels = [-100] * len(input_ids)\n",
        "\n",
        "            word_ids = tokenized_output.word_ids(batch_index=0) # 토큰이 어떤 원본 단어에 해당하는지 ID 매핑\n",
        "\n",
        "            # 토큰별로 레이블 할당 (Doccano의 스팬 기반 레이블을 토큰 기반 BIO 레이블로 변환)\n",
        "            for token_idx, word_idx in enumerate(word_ids):\n",
        "                if word_idx is None: # CLS, SEP, 패딩 토큰과 같은 특수 토큰\n",
        "                    labels[token_idx] = -100 # 손실 계산에서 무시\n",
        "                else: # 일반 단어에 해당하는 토큰\n",
        "                    # 현재 토큰의 원본 텍스트에서의 시작/끝 오프셋\n",
        "                    token_start_offset = offsets[token_idx][0]\n",
        "                    token_end_offset = offsets[token_idx][1]\n",
        "\n",
        "                    current_token_label_name = \"O\" # 현재 토큰의 기본 레이블은 \"O\" (Other)\n",
        "\n",
        "                    # 현재 토큰이 어떤 어노테이션에 속하는지 확인합니다.\n",
        "                    for ann_start, ann_end, ann_label_name in annotations:\n",
        "                        # 토큰의 오프셋이 어노테이션 범위 내에 완전히 포함되는 경우\n",
        "                        if ann_start <= token_start_offset and token_end_offset <= ann_end:\n",
        "                            # 만약 현재 토큰의 시작 오프셋이 어노테이션의 시작 오프셋과 같다면 B- 태그\n",
        "                            if ann_start == token_start_offset:\n",
        "                                current_token_label_name = f\"B-{ann_label_name}\"\n",
        "                            # 아니라면 I- 태그\n",
        "                            else:\n",
        "                                current_token_label_name = f\"I-{ann_label_name}\"\n",
        "                            break # 해당 어노테이션을 찾았으니 더 이상 검색할 필요 없음\n",
        "\n",
        "                    labels[token_idx] = label_to_id[current_token_label_name]\n",
        "\n",
        "            # 실제 모델 입력에 필요한 형태로 데이터를 저장합니다.\n",
        "            converted_data_for_hf.append({\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": tokenized_output['attention_mask'],\n",
        "                \"labels\": labels # 이 labels는 ID로 변환된 BIO 태그 리스트\n",
        "            })\n",
        "\n",
        "    # Python 리스트를 Pandas DataFrame으로 변환 후 Hugging Face Dataset으로 변환\n",
        "    processed_df = pd.DataFrame(converted_data_for_hf)\n",
        "\n",
        "    # 특징(features)을 명시적으로 정의하여 Dataset이 올바른 데이터 타입을 갖도록 합니다.\n",
        "    # 이는 특히 ClassLabel과 같은 특정 타입의 데이터에 중요합니다.\n",
        "    features = Features({\n",
        "        \"input_ids\": Sequence(Value(\"int32\")),\n",
        "        \"attention_mask\": Sequence(Value(\"int32\")),\n",
        "        \"labels\": Sequence(ClassLabel(names=label_list)) # labels는 ClassLabel 시퀀스\n",
        "    })\n",
        "    hf_dataset = Dataset.from_pandas(processed_df, features=features)\n",
        "\n",
        "    print(f\"\\nHugging Face Dataset으로 변환된 샘플 수: {len(hf_dataset)}\")\n",
        "    print(\"\\n변환된 Hugging Face Dataset 첫 번째 샘플:\")\n",
        "    print(hf_dataset[0])\n",
        "    print(f\"디코딩된 토큰: {tokenizer.convert_ids_to_tokens(hf_dataset[0]['input_ids'])}\")\n",
        "    decoded_labels = [id_to_label[l_id] if l_id != -100 else 'O' for l_id in hf_dataset[0]['labels']]\n",
        "    print(f\"디코딩된 레이블: {decoded_labels}\")\n",
        "    # 원본 텍스트 디코딩\n",
        "    print(f\"디코딩된 텍스트: {tokenizer.decode(hf_dataset[0]['input_ids'], skip_special_tokens=True)}\")\n",
        "\n",
        "\n",
        "    # --- 6. 데이터셋 분할 (Train/Validation Split) ---\n",
        "    # 모델 학습을 위해 전체 데이터셋을 학습(train) 세트와 평가(validation) 세트로 나눕니다.\n",
        "    # test_size=0.2는 전체 데이터의 20%를 평가 세트로 사용한다는 의미입니다.\n",
        "    # seed는 재현 가능한 결과를 위해 설정합니다.\n",
        "    train_test_split_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = train_test_split_dataset[\"train\"]\n",
        "    eval_dataset = train_test_split_dataset[\"test\"]\n",
        "\n",
        "    print(f\"\\n학습 데이터셋 샘플 수: {len(train_dataset)}\")\n",
        "    print(f\"평가 데이터셋 샘플 수: {len(eval_dataset)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: 레이블링된 파일 '{labeled_data_file_path}'을(를) 찾을 수 없습니다. Google Drive에 업로드했는지 확인하세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"레이블링된 파일을 로드하거나 처리하는 중 오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4vDd7dBAe-l"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzrQKZ-PAe-l"
      },
      "source": [
        "## 4. BERT 기반 NER 모델 학습\n",
        "\n",
        "이제 전처리된 데이터를 사용하여 `klue/bert-base` 모델을 개체명 인식 작업에 맞게 파인튜닝합니다. Hugging Face `Trainer` API를 사용하면 학습 과정을 매우 편리하게 관리할 수 있습니다.\n",
        "\n",
        "**⚠️ 중요: Colab 런타임이 끊어졌다면, 반드시 `0.2 Google Drive 마운트`, `0.3 프로젝트 디렉토리 구조 설정`, `0.4 필요한 라이브러리 설치` 셀과 함께 `3. Colab에서 라벨링된 데이터 로드 및 전처리` 셀을 다시 실행한 후 이 단계를 진행해주세요.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRBb7iNCAe-l"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "# 1. 모델 로드\n",
        "# AutoModelForTokenClassification은 토큰 분류(NER과 같은 작업)를 위한 모델입니다.\n",
        "# num_labels는 당신의 label_list에 있는 최종 레이블(BIO 태그 포함)의 개수입니다.\n",
        "# id_to_label과 label2id는 모델이 숫자 ID와 레이블 이름을 매핑하는 데 사용됩니다.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"klue/bert-base\",\n",
        "    num_labels=num_labels, # 우리의 최종 BIO 레이블 개수\n",
        "    id2label=id_to_label,  # ID를 레이블 이름으로 매핑\n",
        "    label2id=label_to_id   # 레이블 이름을 ID로 매핑\n",
        ")\n",
        "\n",
        "print(\"모델과 토크나이저 로드 완료.\")\n",
        "print(\"\"\"\n",
        "주의: 'Some weights of BertForTokenClassification were not initialized...' 메시지는 정상입니다.\n",
        "이는 사전 학습된 BERT 모델에 토큰 분류를 위한 새로운 레이어(classifier.bias, classifier.weight)가 추가되었기 때문입니다.\n",
        "이 레이어는 아직 학습되지 않았으므로, 이제 당신의 데이터로 파인튜닝해야 합니다.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_kpBta0Ae-m"
      },
      "source": [
        "### 4.2 성능 지표(Metrics) 정의\n",
        "모델 학습 중 또는 학습 완료 후 모델의 성능을 평가할 지표를 정의합니다. NER에서는 주로 **정확도(accuracy), 정밀도(precision), 재현율(recall), F1-점수(F1-score)**를 사용합니다. Hugging Face에서 제공하는 `seqeval` 라이브러리를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbi0ifa2Ae-m"
      },
      "outputs": [],
      "source": [
        "# 평가지표 로드 (Hugging Face에서 제공하는 seqeval 사용)\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "# 모델 예측 결과를 평가지표에 맞게 변환하고 계산하는 함수\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    # 예측된 로짓(logits)에서 가장 높은 확률을 가진 레이블 ID를 선택합니다.\n",
        "    predictions = np.argmax(predictions, axis=2);\n",
        "\n",
        "    # -100은 손실 계산에서 무시되는 값이므로, 실제 레이블과 예측에서 제외합니다.\n",
        "    # 즉, 특수 토큰이나 패딩 토큰에 대한 예측은 평가에서 제외합니다.\n",
        "    true_predictions = [\n",
        "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # seqeval 라이브러리를 사용하여 최종 성능 지표를 계산합니다.\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "print(\"성능 지표 계산 함수 정의 완료.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2EzLz1-Ae-m"
      },
      "source": [
        "### 4.3 학습 인자(Training Arguments) 및 트레이너(Trainer) 설정\n",
        "모델을 어떻게 학습시킬지에 대한 설정(하이퍼파라미터)과, Hugging Face `Trainer` 객체를 생성합니다.\n",
        "\n",
        "**⚠️ 오류 해결 (이전 대화에서 발생했던 문제):**\n",
        "- `evaluation_strategy`가 `eval_strategy`로 이름이 변경되었습니다. 최신 버전에 맞춰 수정합니다.\n",
        "- `model_dir`이 정의되지 않았다는 오류는 `0.3 프로젝트 디렉토리 구조 설정` 셀을 다시 실행하면 해결됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCF2Co_MAe-m"
      },
      "outputs": [],
      "source": [
        "# 학습 인자 설정\n",
        "# output_dir: 학습된 모델과 로그가 저장될 경로\n",
        "# eval_strategy: 'epoch'로 설정하여 각 에포크 종료 시마다 평가 수행\n",
        "# learning_rate: 학습률 (초기값 2e-5가 일반적입니다.)\n",
        "# per_device_train_batch_size: GPU당 학습 배치 크기 (GPU 메모리에 따라 조절 가능)\n",
        "# per_device_eval_batch_size: GPU당 평가 배치 크기\n",
        "# num_train_epochs: 전체 학습 에포크 수 (데이터셋을 몇 번 반복 학습할지, 데이터가 적으면 과적합 주의)\n",
        "# weight_decay: 가중치 감쇠 (과적합 방지 기법)\n",
        "# push_to_hub: 학습된 모델을 Hugging Face Hub에 업로드할지 여부 (지금은 False)\n",
        "# logging_dir: 학습 로그 저장 경로\n",
        "# logging_steps: 몇 스텝마다 로그를 출력할지 (데이터셋이 작으면 로그가 자주 안 나올 수 있음)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_dir, # 모델 저장 경로 (0.3단계에서 설정한 model_dir)\n",
        "    eval_strategy=\"epoch\", # <-- 'evaluation_strategy'가 'eval_strategy'로 변경됨!\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16, # GPU 메모리가 부족하면 8, 4 등으로 줄여보세요.\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3, # 초반에는 적은 에포크로 시작하고, 데이터가 많아지면 늘려볼 수 있습니다.\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=os.path.join(model_dir, 'logs'),\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\", # Colab 환경에서 불필요한 경고 방지 (wandb 등 설정 시 변경)\n",
        ")\n",
        "\n",
        "# Trainer 객체 생성\n",
        "# Trainer는 모델, 학습 인자, 데이터셋, 토크나이저, 성능 지표 계산 함수를 인자로 받아 학습을 관리합니다.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset, # 학습 데이터셋\n",
        "    eval_dataset=eval_dataset,   # 평가 데이터셋\n",
        "    tokenizer=tokenizer,         # 토크나이저 (FutureWarning이 발생할 수 있지만, 현재는 정상 작동)\n",
        "    compute_metrics=compute_metrics # 성능 지표 계산 함수\n",
        ")\n",
        "\n",
        "print(\"학습 인자 및 Trainer 설정 완료.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxdP31EAe-m"
      },
      "source": [
        "### 4.4 모델 학습 시작\n",
        "이제 `trainer.train()` 명령으로 모델 학습을 시작합니다. 학습 과정과 평가 결과는 로그로 출력됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJSRc2JWAe-m"
      },
      "outputs": [],
      "source": [
        "# 모델 학습 시작\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uww8HA2iAe-m"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axq3D2fYAe-n"
      },
      "source": [
        "## 5. 학습된 모델 평가 및 추론 (테스트)\n",
        "\n",
        "학습이 완료된 모델을 저장하고, 새로운 텍스트에 대해 개체명 인식을 수행하는 방법을 알아봅니다. 현재 학습 데이터셋의 양이 매우 적으므로, 초기 예측 결과는 만족스럽지 않을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q23b0epDAe-n"
      },
      "source": [
        "### 5.1 학습된 모델 저장\n",
        "학습이 완료된 모델의 가중치와 설정 파일이 `model_dir` 경로에 저장됩니다. 이렇게 저장된 모델은 나중에 다시 로드하여 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myMA7qNVAe-n"
      },
      "outputs": [],
      "source": [
        "# 학습된 모델 저장\n",
        "trainer.save_model(model_dir) # model_dir은 0.3단계에서 설정했던 모델 저장 경로입니다.\n",
        "\n",
        "print(f\"학습된 모델이 '{model_dir}'에 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU37rrNIAe-n"
      },
      "source": [
        "### 5.2 저장된 모델 및 토크나이저 로드 (추론 준비)\n",
        "저장된 모델을 로드하여 새로운 텍스트에 대한 예측(추론)을 수행할 준비를 합니다. 모델을 CPU/GPU에 로드하고 평가 모드로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnTLpbw-Ae-n"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# 저장된 모델과 토크나이저 로드\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "loaded_model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
        "\n",
        "# 모델을 평가 모드로 설정 (드롭아웃 등을 비활성화하여 일관된 예측을 보장)\n",
        "loaded_model.eval()\n",
        "\n",
        "# GPU가 있다면 GPU로 모델 이동\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model.to(device);\n",
        "\n",
        "print(f\"모델과 토크나이저가 '{model_dir}'에서 로드되었습니다. 현재 device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAyS6G1JAe-n"
      },
      "source": [
        "### 5.3 단일 텍스트에 대한 개체명 인식 추론 함수\n",
        "임의의 텍스트를 입력받아 모델이 개체명을 예측하고, BIO 태그를 사람이 읽기 쉬운 '개체명 스팬' 형태로 변환하여 출력하는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu2r0s19Ae-n"
      },
      "outputs": [],
      "source": [
        "def predict_ner(text, tokenizer, model, id_to_label, device):\n",
        "    # 텍스트 토큰화 (모델 입력에 맞게 패딩, 잘림 적용)\n",
        "    tokenized_input = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\", # PyTorch 텐서 반환\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    ).to(device) # 모델이 있는 디바이스(CPU 또는 GPU)로 입력 텐서 이동\n",
        "\n",
        "    # 모델 예측 수행\n",
        "    with torch.no_grad(): # 추론 시에는 기울기 계산 비활성화 (메모리 절약, 속도 향상)\n",
        "        output = model(**tokenized_input)\n",
        "\n",
        "    # 예측된 로짓(logits)에서 가장 높은 확률을 가진 레이블 ID 추출\n",
        "    # squeeze()는 배치 차원(크기 1)을 제거하고, cpu().numpy()로 넘파이 배열로 변환\n",
        "    predictions = torch.argmax(output.logits, dim=2).squeeze().cpu().numpy()\n",
        "\n",
        "    # 토큰 및 레이블 디코딩\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"].squeeze().cpu().numpy())\n",
        "    predicted_labels = [id_to_label[p_id] for p_id in predictions]\n",
        "\n",
        "    # 특수 토큰 및 패딩 토큰 제거 (실제 단어에 대한 예측만 보기 위함)\n",
        "    # 또한 B-와 I- 태그를 결합하여 개체명 스팬을 출력합니다.\n",
        "\n",
        "    results = []\n",
        "    current_entity = \"\"\n",
        "    current_label = \"\"\n",
        "\n",
        "    # 예측된 토큰과 레이블을 순회하며 개체명 추출\n",
        "    for token, label in zip(tokens, predicted_labels):\n",
        "        if token.startswith(\"##\"): # WordPiece 토크나이저의 서브워드 접두사 제거\n",
        "            token = token[2:]\n",
        "\n",
        "        # CLS, SEP, PAD 토큰 등 특수 토큰 제외\n",
        "        if token in tokenizer.all_special_tokens: # CLS, SEP, PAD 토큰은 건너뜁니다.\n",
        "            if current_entity: # 이전에 수집 중이던 개체명이 있다면 마무리\n",
        "                results.append((current_entity.strip(), current_label))\n",
        "                current_entity = \"\"\n",
        "                current_label = \"\"\n",
        "            continue\n",
        "\n",
        "        if label.startswith(\"B-\"): # 새로운 개체명의 시작 (Beginning)\n",
        "            if current_entity: # 이전에 수집 중이던 개체명이 있다면 먼저 결과에 추가\n",
        "                results.append((current_entity.strip(), current_label))\n",
        "            current_entity = token # 새로운 개체명 시작\n",
        "            current_label = label[2:] # 'B-' 접두사 제거하여 순수 레이블 이름 저장\n",
        "        elif label.startswith(\"I-\") and current_label and label[2:] == current_label:\n",
        "            # 현재 토큰이 이전 개체명의 연속 (Inside)이고, 레이블 유형이 일치할 경우\n",
        "            current_entity += token # 현재 토큰을 기존 개체명에 추가\n",
        "        else: # 'O' 태그이거나, 'I-' 태그인데 이전 레이블과 일치하지 않는 경우\n",
        "            if current_entity: # 이전에 수집 중이던 개체명이 있다면 마무리\n",
        "                results.append((current_entity.strip(), current_label))\n",
        "            current_entity = \"\" # 현재 개체명 초기화\n",
        "            current_label = \"\" # 현재 레이블 초기화\n",
        "\n",
        "    # 반복문 종료 후 마지막에 남아있는 개체명이 있다면 추가\n",
        "    if current_entity:\n",
        "        results.append((current_entity.strip(), current_label))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"개체명 인식 추론 함수 정의 완료.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2u9qvTsAe-n"
      },
      "source": [
        "### 5.4 모델 테스트 (예시 문장)\n",
        "정의한 추론 함수를 사용하여 학습된 모델이 새로운 문장에서 개체명을 얼마나 잘 예측하는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b00YZ_cnOF8"
      },
      "outputs": [],
      "source": [
        "# 테스트할 예시 문장\n",
        "test_text_1 = \"질의 1. 건축물 증축시 소방시설법령 적용에 대한 문의입니다.\"\n",
        "\n",
        "# 추론 함수 실행\n",
        "predicted_entities_1 = predict_ner(test_text_1, loaded_tokenizer, loaded_model, id_to_label, device)\n",
        "\n",
        "print(f\"\\n입력 텍스트: {test_text_1}\")\n",
        "print(f\"예측된 개체명: {predicted_entities_1}\")\n",
        "\n",
        "# 다른 예시\n",
        "test_text_2 = \"회신 1. 소방청 고시 제2023-10호에 따라 안전기준을 준수해야 합니다.\"\n",
        "predicted_entities_2 = predict_ner(test_text_2, loaded_tokenizer, loaded_model, id_to_label, device)\n",
        "print(f\"\\n입력 텍스트: {test_text_2}\")\n",
        "print(f\"예측된 개체명: {predicted_entities_2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTM1wirwAe-n"
      },
      "source": [
        "### 5.5 예측 결과 분석 및 현재 모델의 한계\n",
        "\n",
        "현재의 예측 결과는 아마도 정확도가 매우 낮거나, 기대했던 개체명을 제대로 인식하지 못할 것입니다. (예: 대부분의 단어를 `QUESTION_CONTENT`로 분류하거나, 주요 개체명을 놓치는 등).\n",
        "\n",
        "**이것은 지극히 정상적인 현상입니다.** 현재 모델의 성능이 낮은 주된 이유는 다음과 같습니다:\n",
        "\n",
        "* **매우 적은 학습 데이터:** 우리가 사용한 학습 데이터셋은 12개의 샘플에 불과합니다. BERT와 같은 딥러닝 모델은 수많은 패턴을 학습해야 하므로, 단 12개의 샘플로는 의미 있는 특징을 학습하고 일반화하기에 턱없이 부족합니다.\n",
        "* **'O' 태그 편향:** 모델은 학습 데이터에서 'O'(Other) 태그가 훨씬 많기 때문에, 안전하게 대부분의 토큰을 'O'로 예측하거나, 특정 라벨로 뭉뚱그려 예측하려는 경향이 있습니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXfJLHCvAe-o"
      },
      "source": [
        "## 6. 성능 향상을 위한 추가 단계 (결론)\n",
        "\n",
        "지금까지 개체명 인식 모델 학습을 위한 전체 파이프라인을 성공적으로 구축했습니다. 이제 구축된 파이프라인을 통해 모델의 성능을 향상시키는 데 집중할 차례입니다.\n",
        "\n",
        "### 6.1 더 많은 데이터 라벨링 (가장 중요!)\n",
        "\n",
        "모델 성능 향상의 90% 이상은 **데이터의 양과 질**에 달려 있습니다. 현재 가장 필요한 것은 Doccano로 돌아가서 **더 많은 질의/회신 문서를 라벨링하는 것**입니다.\n",
        "\n",
        "* **목표:** 최소 수백 개에서 수천 개 이상의 질의/회신 쌍을 라벨링하는 것을 목표로 합니다.\n",
        "* **다양성 확보:** 다양한 문맥과 표현을 가진 텍스트에 대해 라벨링하여 모델이 일반화된 패턴을 학습할 수 있도록 도와야 합니다. (예: '소방시설법령'이 다양한 문장에서 어떻게 등장하는지 등)\n",
        "* **일관성 유지:** 라벨링 규칙을 명확히 하고 일관되게 적용하는 것이 중요합니다.\n",
        "\n",
        "**워크플로우 반복:**\n",
        "1.  Doccano에서 추가 데이터 라벨링\n",
        "2.  `Export only approved documents`를 체크하여 `.jsonl` 파일 내보내기\n",
        "3.  Google Drive에 업로드\n",
        "4.  **Colab 노트북의 `3. Colab에서 라벨링된 데이터 로드 및 전처리` 셀부터 다시 실행하여 모델 재학습**\n",
        "\n",
        "이 과정을 반복하면서 모델의 Precision, Recall, F1-score가 점진적으로 향상되는 것을 확인할 수 있을 것입니다.\n",
        "\n",
        "### 6.2 하이퍼파라미터 튜닝\n",
        "충분한 데이터가 확보된 후에는, `4.3 학습 인자(Training Arguments) 및 트레이너(Trainer) 설정` 섹션의 하이퍼파라미터(예: `learning_rate`, `per_device_train_batch_size`, `num_train_epochs`)를 조정하여 모델 성능을 최적화할 수 있습니다.\n",
        "\n",
        "### 6.3 모델 아키텍처 및 전이 학습 고려\n",
        "현재 `klue/bert-base`는 좋은 시작점입니다. 하지만 더 전문적인 도메인(예: 법률)에 특화된 사전 학습 모델이 있다면 이를 활용하는 것도 고려해볼 수 있습니다. (한국어 법률 특화 모델은 제한적일 수 있습니다.)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn8YEB_aAe-o"
      },
      "source": [
        "## 7. 프로젝트 회고 및 배운 점\n",
        "\n",
        "이번 프로젝트를 통해 딥러닝 모델을 실제 업무에 적용하는 전체 과정을 경험하며 많은 것을 배우고 느꼈습니다. 단순히 이론으로만 알던 개념들을 직접 부딪히고 해결하며 얻은 교훈들을 정리했습니다.\n",
        "\n",
        "### 기술적 성장 및 경험\n",
        "\n",
        "* **클라우드 기반 GPU 활용의 필요성:** 딥러닝 모델 학습처럼 대규모 연산이 필요한 작업은 일반 로컬 PC 환경에서 수행하기 어렵다는 것을 체감했습니다. **Google Colab**이 제공하는 무료 **T4 GPU**는 이러한 제약을 극복하고, 비용 효율적으로 모델을 학습하고 실험할 수 있는 훌륭한 대안이었습니다.\n",
        "\n",
        "* **Docker와 컨테이너 환경 경험:** 데이터 라벨링 도구인 **Doccano**를 설치하고 실행하기 위해 **Docker**를 처음 사용해보았습니다. 이를 통해 애플리케이션을 격리된 환경에서 손쉽게 배포하고 실행하는 컨테이너 기술의 강력함을 이해하게 되었고, 복잡한 설치 과정 없이 필요한 도구를 빠르게 구축하는 경험을 쌓을 수 있었습니다.\n",
        "\n",
        "* **엔드-투-엔드(End-to-End) 파이프라인 구축:** 데이터 전처리부터 라벨링, 모델 학습, 평가, 그리고 추론에 이르기까지의 전체 머신러닝 파이프라인을 직접 설계하고 구축했습니다. 각 단계가 어떻게 유기적으로 연결되는지, 그리고 각 단계에서 어떤 점을 고려해야 하는지에 대한 실질적인 이해를 높일 수 있었습니다.\n",
        "\n",
        "### 딥러닝 모델과 데이터에 대한 깊은 이해\n",
        "\n",
        "* **'Garbage In, Garbage Out'의 실감:** 모델의 성능은 결국 데이터의 양과 질에 의해 결정된다는 것을 뼈저리게 느꼈습니다. 특히, **라벨링된 데이터 샘플의 수가 많을수록 모델의 정확도가 비례하여 향상**되는 것을 직접 확인했습니다. 초기 단계에서 적은 수의 샘플로 학습했을 때 모델이 거의 작동하지 않았던 경험은 양질의 데이터 확보가 얼마나 중요한지 깨닫게 해주었습니다.\n",
        "\n",
        "* **사전 학습 모델(Pre-trained Model)의 위력:** `klue/bert-base`와 같은 사전 학습된 모델을 기반으로 파인튜닝하는 것이 왜 효율적인지를 이해했습니다. 밑바닥부터 모든 것을 학습시키는 대신, 이미 방대한 한국어 데이터를 학습한 모델을 활용함으로써 비교적 적은 데이터로도 특정 도메인의 작업을 수행할 수 있다는 전이 학습의 개념을 실제로 적용해볼 수 있었습니다.\n",
        "\n",
        "### 현실적인 한계와 성과\n",
        "\n",
        "* **고품질 학습 데이터셋 구축의 어려움:** 지도 학습(Supervised Learning) 기반의 NER 모델을 훈련시키기 위해 1만 2천 개의 모든 데이터를 라벨링할 필요는 없습니다. 핵심은 모델이 전체 데이터의 패턴을 학습할 수 있을 만큼, **충분한 양의 대표적인 샘플들을 고품질로 라벨링**하는 것입니다. 일단 모델이 잘 학습되면, 나머지 라벨링되지 않은 데이터는 모델이 자동으로 처리해줄 수 있습니다. 하지만, 원하는 성능을 내기 위한 '충분한 양의 샘플'을 만드는 것 자체가 혼자서는 매우 힘든 작업이었습니다. 이로 인해 100% 완벽한 정확도를 가진 모델을 만들지는 못했지만, 지도 학습의 핵심 원리와 데이터의 중요성을 체감하는 계기가 되었습니다.\n",
        "\n",
        "* **그럼에도 불구하고, 성공적인 자동화:** 비록 모델이 완벽하지는 않았지만, 이 프로젝트를 통해 개발한 자동화된 파싱 시스템은 기존의 **수작업으로 질의-회신 쌍을 검증하던 방식에 비해 업무 효율을 압도적으로 향상**시켰습니다. 반복적인 작업을 자동화함으로써 시간을 절약하고, 더 중요한 분석 작업에 집중할 수 있게 되었다는 점에서 이 프로젝트는 매우 성공적이었습니다. 이는 '완벽함'을 추구하기보다 '개선'을 목표로 하는 것의 중요성을 일깨워 주었습니다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}