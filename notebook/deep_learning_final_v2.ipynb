{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI7YcRm0Ae-c"
      },
      "source": [
        "# ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER) ëª¨ë¸ í•™ìŠµì— ëŒ€í•œ ê¸°ë¡\n",
        "\n",
        "ì´ ë…¸íŠ¸ëŠ” í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ì¶”ì¶œ(ì˜ˆ: ì§ˆì˜íšŒì‹ , ë²•ë ¹ ë‚´ìš©)í•˜ê¸° ìœ„í•œ ê°œì²´ëª… ì¸ì‹(NER) ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì— ëŒ€í•œ ê¸°ë¡ì…ë‹ˆë‹¤.\n",
        "\n",
        "**í•™ìŠµ ëª©í‘œ:**\n",
        "\n",
        "1. Google Colab í™˜ê²½ ì„¤ì • ë° Google Drive ì—°ë™.\n",
        "2. ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ (Doccano ì„í¬íŠ¸ ì „).\n",
        "3. Doccanoë¥¼ ì´ìš©í•œ ì›¹ ê¸°ë°˜ ë°ì´í„° ë¼ë²¨ë§.\n",
        "4. ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜ (Hugging Face `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬).\n",
        "5. ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸(`klue/bert-base`)ì„ í™œìš©í•˜ì—¬ NER ëª¨ë¸ íŒŒì¸íŠœë‹.\n",
        "6. í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ ë° ì¶”ë¡ (Inference) ë°©ë²• ì´í•´.\n",
        "\n",
        "**ì°¸ê³ :** ì´ íŠœí† ë¦¬ì–¼ì€ Doccanoì™€ Hugging Face `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2BXJgIVAe-g"
      },
      "source": [
        "---\n",
        "\n",
        "### 0. í”„ë¡œì íŠ¸ ê°œìš” ë° ë™ê¸°\n",
        "\n",
        "ì´ í”„ë¡œì íŠ¸ëŠ” íšŒì‚¬ì—ì„œ í• ë‹¹ë°›ì€ ì—…ë¬´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²°í•˜ê¸° ìœ„í•´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì €ì—ê²Œ ì£¼ì–´ì§„ ì—…ë¬´ëŠ” **ê° ê³µê³µê¸°ê´€ ì¤‘ì•™ë¶€ì²˜ì˜ ë²•ë ¹í•´ì„ ë°ì´í„°ì…‹ì„ ì›¹ í¬ë¡¤ë§í•˜ì—¬ PDF ë˜ëŠ” HWP íŒŒì¼ ë‚´ì˜ ì§ˆì˜ì™€ íšŒì‹ ì„ í•œ ì„¸íŠ¸ë¡œ íŒŒì‹±í•˜ê³  ê²€ì¦**í•˜ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì œë¥¼ ì•ˆê³  ìˆì—ˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "1.  **ì§ˆì˜-íšŒì‹  ì„¸íŠ¸ ê°œìˆ˜ ì„¸ê¸°**: ì›¹ í¬ë¡¤ë§ëœ ë¬¸ì„œì—ì„œ 'ì§ˆì˜ 1ê°œ'ì™€ 'íšŒì‹  1ê°œ'ê°€ ì •ìƒì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì„¸íŠ¸ë¥¼ ì´ë£¨ëŠ”ì§€ í™•ì¸í•˜ê³ , ì „ì²´ ì„¸íŠ¸ì˜ ê°œìˆ˜ë¥¼ íŒŒì•…í•´ì•¼ í–ˆìŠµë‹ˆë‹¤.\n",
        "2.  **ì§ˆì˜-íšŒì‹  ì—°ê²°ì„± íŒë‹¨**: ì§ˆì˜ì™€ íšŒì‹  ê°„ì˜ ì—°ê²°ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ì§€, ì¦‰ 'ì§ˆì˜ 1'ì— ëŒ€í•œ 'íšŒì‹  1'ì´ ì œëŒ€ë¡œ ë§¤ì¹­ë˜ëŠ”ì§€ íŒë‹¨í•´ì•¼ í–ˆìŠµë‹ˆë‹¤.\n",
        "3.  **íŠ¹ì´ì‚¬í•­ ë¶„ë¥˜**: íŠ¹íˆ ì§ˆì˜ì™€ íšŒì‹ ì˜ ê°œìˆ˜ê°€ ë§ì§€ ì•Šê±°ë‚˜, íšŒì‹  ë‚´ìš©ì´ ë²•ë ¹ì— ê·¼ê±°í•˜ì§€ ì•Šì€ ë‹µë³€ì¼ ê²½ìš° ì´ë¥¼ 'íŠ¹ì´ì‚¬í•­'ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ì—…ë¬´ê°€ ìˆ˜ë°˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë¬¸ì œëŠ” ì´ëŸ¬í•œ ë¶„ë¥˜ ë° ê²€ì¦ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•  ì§ˆì˜íšŒì‹  ì„¸íŠ¸ê°€ **ì´ 2ë§Œ ê°œ**ì— ë‹¬í•œë‹¤ëŠ” ì ì´ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì—„ì²­ë‚œ **ìˆ˜ì‘ì—…ê³¼ ì‹œê°„ ì†Œëª¨**ë¥¼ ìš”êµ¬í•˜ëŠ” ì¼ì´ì—ˆìŠµë‹ˆë‹¤. ì»´í“¨í„° ê³µí•™ ì „ê³µìë¡œì„œ ì´ëŸ¬í•œ ë¹„íš¨ìœ¨ì ì¸ ì—…ë¬´ë¥¼ ì œ ì „ê³µ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìë™í™”í•˜ê³  í•´ê²°í•˜ê³ ì í•˜ëŠ” ê°•í•œ ë™ê¸°ë¥¼ ëŠê¼ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ë°˜ë³µ ì‘ì—…ì„ ì¤„ì´ëŠ” ê²ƒì„ ë„˜ì–´, ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ **ë”¥ëŸ¬ë‹, ë°ì´í„° ë¼ë²¨ë§, ë„ì»¤(Docker), ê·¸ë¦¬ê³  ìì—°ì–´ ì²˜ë¦¬(NLP) ê°œë…**ì„ ì‹¤ì§ˆì ìœ¼ë¡œ ë°°ìš°ê³  ì ìš©í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ì‚¼ì•˜ìŠµë‹ˆë‹¤. ì´ ë³´ê³ ì„œëŠ” ì´ëŸ¬í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ê¸°ë¡ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnBNY7kNAe-g"
      },
      "source": [
        "### í”„ë¡œì íŠ¸ ì „ì²´ í”Œë¡œìš° (ì´ë¯¸ì§€)\n",
        "\n",
        "ì•„ë˜ëŠ” ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì§„í–‰í•  ì „ì²´ í”„ë¡œì íŠ¸ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì´ë¯¸ì§€ë¡œ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤. (w/ mermaid)\n",
        "![í”„ë¡œì íŠ¸ í”Œë¡œìš°ì°¨íŠ¸](https://github.com/jaehunshin-git/deep-learning/blob/main/deep-learning-flowchart.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frk4GrteAe-h"
      },
      "source": [
        "## í”„ë¡œì íŠ¸ ì „ì²´ í”Œë¡œìš° (í…ìŠ¤íŠ¸ ë‹¤ì´ì–´ê·¸ë¨)\n",
        "\n",
        "ì•„ë˜ëŠ” ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì§„í–‰í•  ì „ì²´ í”„ë¡œì íŠ¸ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤. ê° ë‹¨ê³„ì˜ íë¦„ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "```text\n",
        "## í”„ë¡œì íŠ¸ ì „ì²´ í”Œë¡œìš°\n",
        "\n",
        "1.  ì‹œì‘ - ë”¥ëŸ¬ë‹ NER ëª¨ë¸ í•™ìŠµ\n",
        "    -> 2. Google Colab í™˜ê²½ ì„¤ì •\n",
        "        -> 2.1 ëŸ°íƒ€ì„ ìœ í˜• GPU ë³€ê²½\n",
        "        -> 2.2 Google Drive ë§ˆìš´íŠ¸\n",
        "        -> 2.3 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •\n",
        "        -> 2.4 í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "2.4 í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "    -> 3. ë°ì´í„° ì¤€ë¹„ - ì›ë³¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë¡œì»¬ PC)\n",
        "        -> 3.1 ì›ë³¸ dataset.txt ë¡œë“œ\n",
        "        -> 3.2 íŠ¹ì • ë¬¸ì/ìˆ«ì/ì›Œë”© ì œê±° - ì •ê·œí‘œí˜„ì‹\n",
        "        -> 3.3 ê° ì§ˆì˜/íšŒì‹  ìŒ ë¶„ë¦¬ - ê°œí–‰ ë¬¸ì ì¶”ê°€\n",
        "        -> 3.4 dataset_cleaned_final.txt ì €ì¥\n",
        "\n",
        "3.4 dataset_cleaned_final.txt ì €ì¥\n",
        "    -> 4. Doccano ë°ì´í„° ë¼ë²¨ë§ (ë¡œì»¬ PC Docker)\n",
        "        -> 4.1 Doccano Docker ì„¤ì¹˜ ë° ì‹¤í–‰\n",
        "        -> 4.2 Doccano í”„ë¡œì íŠ¸ ìƒì„± - ë ˆì´ë¸” ì •ì˜\n",
        "        -> 4.3 dataset_cleaned_final.txt Import - Plain Text\n",
        "        -> 4.4 ìˆ˜ë™ ê°œì²´ëª… ë¼ë²¨ë§ ìˆ˜í–‰\n",
        "        -> 4.5 ë¼ë²¨ë§ëœ ë°ì´í„° Export - JSONL - Approved Only\n",
        "\n",
        "4.5 ë¼ë²¨ë§ëœ ë°ì´í„° Export - JSONL - Approved Only\n",
        "    -> 5. Google Drive ì—…ë¡œë“œ\n",
        "        -> 6. Colabì—ì„œ ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "            -> 6.1 Doccano JSONL íŒŒì¼ ë¡œë“œ\n",
        "            -> 6.2 BIO íƒœê¹…ì„ ìœ„í•œ ë ˆì´ë¸” ëª©ë¡ ìƒì„±\n",
        "            -> 6.3 í…ìŠ¤íŠ¸ í† í°í™” - ë ˆì´ë¸” ì •ë ¬ - padding, truncation í¬í•¨\n",
        "            -> 6.4 Hugging Face Dataset ê°ì²´ë¡œ ë³€í™˜\n",
        "            -> 6.5 Train/Validation ë°ì´í„°ì…‹ ë¶„í• \n",
        "\n",
        "6.5 Train/Validation ë°ì´í„°ì…‹ ë¶„í• \n",
        "    -> 7. BERT ê¸°ë°˜ NER ëª¨ë¸ í•™ìŠµ\n",
        "        -> 7.1 klue/bert-base ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "        -> 7.2 ì„±ëŠ¥ ì§€í‘œ Metrics ì •ì˜ - seqeval\n",
        "        -> 7.3 í•™ìŠµ ì¸ì Training Arguments ì„¤ì • - eval_strategy ë“±\n",
        "        -> 7.4 Trainer ê°ì²´ ìƒì„±\n",
        "        -> 7.5 trainer.train() ì‹¤í–‰\n",
        "\n",
        "7.5 trainer.train() ì‹¤í–‰\n",
        "    -> 8. í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ë° ì¶”ë¡ \n",
        "        -> 8.1 í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
        "        -> 8.2 ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "        -> 8.3 ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¶”ë¡  í•¨ìˆ˜ ì •ì˜\n",
        "        -> 8.4 ì˜ˆì‹œ ë¬¸ì¥ìœ¼ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "8.4 ì˜ˆì‹œ ë¬¸ì¥ìœ¼ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
        "    -> 9. ê²°ê³¼ ë¶„ì„ - ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
        "        -> 10. ì„±ëŠ¥ì´ ì¶©ë¶„í•œê°€?\n",
        "            -> Yes -> 11. ë°°í¬ ë° í™œìš©\n",
        "            -> No  -> 12. ì„±ëŠ¥ í–¥ìƒ ì „ëµ ì ìš©\n",
        "                -> 12.1 ë” ë§ì€ ë°ì´í„° ë¼ë²¨ë§ - ê°€ì¥ ì¤‘ìš”!\n",
        "                -> 12.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
        "                -> 12.3 ëª¨ë¸ ì•„í‚¤í…ì²˜/ì „ì´ í•™ìŠµ ê³ ë ¤\n",
        "                -> 12.1 ë” ë§ì€ ë°ì´í„° ë¼ë²¨ë§ - ê°€ì¥ ì¤‘ìš”! -> 4. Doccano ë°ì´í„° ë¼ë²¨ë§ (ë¡œì»¬ PC Docker)\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS3XdVsWAe-h"
      },
      "source": [
        "## 0. Google Colab í™˜ê²½ ì„¤ì •\n",
        "\n",
        "ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ Google Colab í™˜ê²½ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 0.1 ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ (GPU í™œì„±í™”)\n",
        "\n",
        "Colabì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¤ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
        "\n",
        "1.  Colab ìƒë‹¨ ë©”ë‰´ì—ì„œ `ëŸ°íƒ€ì„` (Runtime)ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
        "2.  `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½` (Change runtime type)ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
        "3.  `í•˜ë“œì›¨ì–´ ê°€ì†ê¸°` (Hardware accelerator) ë“œë¡­ë‹¤ìš´ ë©”ë‰´ì—ì„œ `GPU` í˜¹ì€ `TPU` ë¥¼ ì„ íƒí•œ í›„ `ì €ì¥` (Save)ì„ í´ë¦­í•©ë‹ˆë‹¤. - (`TPU` ì„ íƒ ì‹œ ì—­ë²ˆì—­ ì‹œê°„ ë‹¨ì¶• ê°€ëŠ¥)\n",
        "\n",
        "**ì£¼ì˜:**\n",
        "\n",
        "1.  ëŸ°íƒ€ì„ì„ ë³€ê²½í•˜ê±°ë‚˜ Colab ì„¸ì…˜ì´ ëŠê²¼ë‹¤ê°€ ë‹¤ì‹œ ì—°ê²°ë˜ë©´, ì´ì „ì— ì„¤ì¹˜í–ˆë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ ì •ì˜í–ˆë˜ ë³€ìˆ˜ë“¤ì´ ì´ˆê¸°í™”ë©ë‹ˆë‹¤. ì´ ê²½ìš° ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "2.  google.colab ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” colab ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë¯€ë¡œ ë¡œì»¬ì—ì„œëŠ” ì„¤ì¹˜ ë° ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOsLz5_7KDXz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEDZ_B4ZAe-j"
      },
      "source": [
        "### 0.3 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •\n",
        "\n",
        "Google Drive ë‚´ë¶€ì— í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ í´ë” êµ¬ì¡°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë°ì´í„°ì™€ ëª¨ë¸ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M07W0c9lD-T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# í”„ë¡œì íŠ¸ì˜ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • (ì´ì „ì— ë‹¹ì‹ ì´ ì„¤ì •í•œ Google Drive ê²½ë¡œ)\n",
        "# Colab Notebooks í´ë” ì•„ë˜ì— deep-learning-ner-advanced ì´ë¼ëŠ” í´ë”ë¥¼ ë§Œë“¤ì–´ ì´ í”„ë¡œì íŠ¸ì˜ ë£¨íŠ¸ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
        "# ì´ ê²½ë¡œëŠ” ì‚¬ìš©ìì˜ Google Drive êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning-ner-advanced/\"\n",
        "data_dir = os.path.join(project_root, \"data\")\n",
        "model_dir = os.path.join(project_root, \"model\")\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "print(f\"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}\")\n",
        "print(f\"ë°ì´í„° í´ë”: {data_dir}\")\n",
        "print(f\"ëª¨ë¸ ì €ì¥ í´ë”: {model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP5bg9mhAe-j"
      },
      "source": [
        "### 0.4 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ Hugging Face `transformers`ì™€ `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. `accelerate`ëŠ” í•™ìŠµ ê°€ì†í™”ì— ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
        "\n",
        "1. `transformers`\n",
        "\n",
        "   - ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "     - Hugging Faceì—ì„œ ê°œë°œí•œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. BERT, GPT-2, RoBERTa ë“± ë‹¤ì–‘í•œ ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸(Transformer-based models)ê³¼ í† í¬ë‚˜ì´ì €(Tokenizer)ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ ìµœì‹  ëª¨ë¸ë“¤ì„ ì‰½ê²Œ ë¶ˆëŸ¬ì™€ íŒŒì¸íŠœë‹(fine-tuning)í•˜ê±°ë‚˜ ì¶”ë¡ (inference)í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
        "\n",
        "   - ë‚´ í”„ë¡œì íŠ¸ì—ì„œ í™œìš©:\n",
        "\n",
        "     - ëª¨ë¸ ë¡œë“œ: `AutoModelForTokenClassification.from_pretrained(\"klue/bert-base\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ì— íŠ¹í™”ëœ BERT ëª¨ë¸ì¸ klue/bert-baseë¥¼ ê°œì²´ëª… ì¸ì‹(Token Classification) ì‘ì—…ì— ë§ê²Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "\n",
        "     - í† í¬ë‚˜ì´ì € ë¡œë“œ: AutoTokenizer.from_pretrained(\"klue/bert-base\")ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í°(ë‹¨ì–´ ì¡°ê°) IDë¡œ ë³€í™˜í•˜ëŠ” ë° í•„ìš”í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "\n",
        "     - ëª¨ë¸ í•™ìŠµ ê´€ë¦¬: Trainer í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì¸ì(TrainingArguments) ì„¤ì •, í•™ìŠµ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„° ì§€ì •, ëª¨ë¸ í•™ìŠµ(trainer.train()) ë“± ë³µì¡í•œ í•™ìŠµ ê³¼ì •ì„ í¸ë¦¬í•˜ê²Œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "     - ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ: í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‚˜ì¤‘ì— ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ ì¶”ë¡ ì— í™œìš©í•˜ëŠ” ê¸°ëŠ¥ë„ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n",
        "\n",
        "2. `datasets`\n",
        "\n",
        "   - ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "     - Hugging Faceì—ì„œ ê°œë°œí•œ ë˜ ë‹¤ë¥¸ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œ, ì „ì²˜ë¦¬, ì €ì¥í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ NLP ì‘ì—…ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ê³µê°œ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë„ ì œê³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "   - ë‚´ í”„ë¡œì íŠ¸ì—ì„œ í™œìš©:\n",
        "\n",
        "     - ë°ì´í„° ë¡œë“œ: Doccanoì—ì„œ ë‚´ë³´ë‚¸ .jsonl í˜•ì‹ì˜ ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  Pandas DataFrameì„ ê±°ì³ datasets.Dataset ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. Dataset ê°ì²´ëŠ” Hugging Face Trainerì— ì§ì ‘ ì…ë ¥ë  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ, ë°ì´í„° ì²˜ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
        "\n",
        "     - ë°ì´í„° ì „ì²˜ë¦¬: Dataset ê°ì²´ì— í† í¬ë‚˜ì´ì§•ëœ input_ids, attention_mask, ê·¸ë¦¬ê³  ê° í† í°ì— í•´ë‹¹í•˜ëŠ” labels (BIO íƒœê·¸)ë¥¼ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "     - ë°ì´í„° ë¶„í• : train_test_split ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë¼ë²¨ë§ëœ ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµìš©(train_dataset)ê³¼ í‰ê°€ìš©(eval_dataset)ìœ¼ë¡œ ì‰½ê²Œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
        "\n",
        "3. `accelerate`\n",
        "\n",
        "   - ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "     - Hugging Faceì—ì„œ ê°œë°œí•œ ê²½ëŸ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, PyTorch ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì„ ë¶„ì‚° ë° í˜¼í•© ì •ë°€ë„(mixed-precision) í•™ìŠµ í™˜ê²½ì— ë§ê²Œ ìë™ìœ¼ë¡œ ì¡°ì •í•´ ì¤ë‹ˆë‹¤. ê°œë°œìê°€ ë³µì¡í•œ ë¶„ì‚° í•™ìŠµ ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•  í•„ìš” ì—†ì´, ì†Œìˆ˜ì˜ ì½”ë“œ ë³€ê²½ë§Œìœ¼ë¡œ GPU ì—¬ëŸ¬ ê°œë‚˜ TPU ê°™ì€ ê°€ì†ê¸°ë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
        "\n",
        "   - ë‚´ í”„ë¡œì íŠ¸ì—ì„œ í™œìš©:\n",
        "\n",
        "     - ë‹¹ì‹ ì˜ Colab í™˜ê²½ì—ì„œ GPU ëŸ°íƒ€ì„ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” transformersì˜ Trainer ë‚´ë¶€ì ìœ¼ë¡œ í™œìš©ë˜ì–´ GPU ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì†ë„ë¥¼ ê°€ì†í™”í•©ë‹ˆë‹¤. ì´ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ í•™ìŠµí•  ë•Œ í•„ìˆ˜ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "4. `seqeval`\n",
        "\n",
        "   - ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "     - ì‹œí€€ìŠ¤ ë¼ë²¨ë§(Sequence Labeling) ì‘ì—…(ê°œì²´ëª… ì¸ì‹, í’ˆì‚¬ íƒœê¹… ë“±)ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. íŠ¹íˆ precision, recall, f1-score, accuracyì™€ ê°™ì€ ì£¼ìš” ì§€í‘œë“¤ì„ ê³„ì‚°í•˜ëŠ” ë° íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê°œì²´ëª… ì¸ì‹ì—ì„œëŠ” ë‹¨ìˆœíˆ ê°œë³„ í† í°ì˜ ì •í™•ë„ë¿ë§Œ ì•„ë‹ˆë¼, **ì „ì²´ ê°œì²´ëª… ìŠ¤íŒ¬(span)**ì´ ì •í™•í•˜ê²Œ ì˜ˆì¸¡ë˜ì—ˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©°, seqevalì´ ì´ëŸ¬í•œ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "   - ë‚´ í”„ë¡œì íŠ¸ì—ì„œ í™œìš©:\n",
        "\n",
        "     - `load_metric(\"seqeval\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ì§€í‘œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "\n",
        "     - `compute_metrics` í•¨ìˆ˜ ë‚´ì—ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë ˆì´ë¸”ì„ seqeval í¬ë§·ì— ë§ê²Œ ë³€í™˜í•˜ê³ , ì´ë¥¼ metric.compute() í•¨ìˆ˜ì— ì „ë‹¬í•˜ì—¬ ìµœì¢… ì •ë°€ë„, ì¬í˜„ìœ¨, F1-ì ìˆ˜ ë“±ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ì§€í‘œë“¤ì€ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ìµœì¢… ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì€ ëª¨ë‘ Hugging Face ìƒíƒœê³„ì˜ ì¼ë¶€ë¡œ, ë³µì¡í•œ ë”¥ëŸ¬ë‹ NLP í”„ë¡œì íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê°œë°œí•˜ê³  ê´€ë¦¬í•˜ëŠ” ë° í° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1mUOCBKBohkT",
        "outputId": "1f9e22d4-4f2c-4134-f2c3-26a5af7e7bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch] evaluate datasets accelerate seqeval -q\n",
        "\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForSeq2SeqLM\n",
        "import evaluate\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTSddOsUAe-k"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azs-ewTQnKDL"
      },
      "source": [
        "## 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬ (Doccano ì—°ë™ ì „)\n",
        "\n",
        "ì›ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼(`dataset.txt`)ì„ Doccanoì— íš¨ìœ¨ì ìœ¼ë¡œ ë¼ë²¨ë§í•˜ê¸° ìœ„í•´ ì‚¬ì „ ì „ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³ , ê° ì§ˆì˜/íšŒì‹  ìŒì„ Doccanoê°€ ê°œë³„ ë¬¸ì„œë¡œ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ë¶„ë¦¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì›ë³¸ `dataset.txt` íŒŒì¼ì˜ ë¬¸ì œì :**\n",
        "\n",
        "- ë¶ˆí•„ìš”í•œ ìˆ«ìë‚˜ íŠ¹ì • ì›Œë”©(ì˜ˆ: '2021ë…„ ì†Œë°©ì‹œì„¤ë²•ë ¹ ì§ˆì˜íšŒì‹ ì§‘') í¬í•¨.\n",
        "- ëª¨ë“  ì§ˆì˜/íšŒì‹ ì´ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ë¡œ ì´ì–´ì ¸ ìˆì–´, Doccanoì—ì„œ ê°œë³„ ë¬¸ì„œë¡œ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ì›€.\n",
        "\n",
        "ì´ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ëŠ” **Google Colabì´ ì•„ë‹Œ, ë¡œì»¬ í™˜ê²½(ì˜ˆ: VS Code)**ì—ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²˜ë¦¬í•œ í›„, ê·¸ ê²°ê³¼ íŒŒì¼ì„ Google Driveì— ì—…ë¡œë“œí•˜ì—¬ Doccanoë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_5d6OsVAe-k"
      },
      "source": [
        "### 1.1 `dataset.txt` íŒŒì¼ í´ë¦¬ë‹ ë° ë¬¸ì„œ ë¶„ë¦¬ ì½”ë“œ\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ë¡œì»¬ ì»´í“¨í„°ì˜ `.py` íŒŒì¼ë¡œ ì €ì¥(ì˜ˆ: `preprocess_dataset.py`)í•˜ê³ , ì›ë³¸ `dataset.txt` íŒŒì¼ì´ ìˆëŠ” ë™ì¼í•œ í´ë”ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "import os\n",
        "import re\n",
        "\n",
        "def clean_and_prepare_text_for_doccano_final_v2(input_filepath, output_filepath):\n",
        "    \"\"\"\n",
        "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
        "    1. í•œ ì¤„ì— ìˆ«ì(0-9) í•˜ë‚˜ë§Œ ìˆëŠ” ë¼ì¸ì„ ì‚­ì œí•©ë‹ˆë‹¤.\n",
        "    2. '2021ë…„ ì†Œë°©ì‹œì„¤ë²•ë ¹ ì§ˆì˜íšŒì‹ ì§‘' ë¬¸ìì—´ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "    3. 'ì§ˆì˜ N.' (ë˜ëŠ” 'ì§ˆì˜ N')ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì•ì— ë¹ˆ ì¤„(\\n\\n)ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "       (ë‹¨, íŒŒì¼ì˜ ë§¨ ì²˜ìŒ ë‚˜ì˜¤ëŠ” 'ì§ˆì˜ 1.' ì•ì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)\n",
        "       ì´ë•Œ, ê° 'ì§ˆì˜ N.' ë¸”ë¡ì´ ì •í™•íˆ '\\n\\n'ìœ¼ë¡œ êµ¬ë¶„ë˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(input_filepath, 'r', encoding='utf-8') as f_in:\n",
        "            lines = f_in.readlines() # íŒŒì¼ì„ ì¤„ ë‹¨ìœ„ë¡œ ì½ì–´ì˜µë‹ˆë‹¤.\n",
        "\n",
        "        temp_content = [] # ì„ì‹œë¡œ í´ë¦¬ë‹ëœ ì¤„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "        for line in lines:\n",
        "            # 1ë‹¨ê³„: '2021ë…„ ì†Œë°©ì‹œì„¤ë²•ë ¹ ì§ˆì˜íšŒì‹ ì§‘' ë¬¸ìì—´ ì œê±°\n",
        "            line = line.replace('2021ë…„ ì†Œë°©ì‹œì„¤ë²•ë ¹ ì§ˆì˜íšŒì‹ ì§‘', '')\n",
        "\n",
        "            # 2ë‹¨ê³„: í•œ ì¤„ì— ìˆ«ì í•˜ë‚˜ë§Œ ìˆëŠ” ë¼ì¸ ì‚­ì œ\n",
        "            if line.strip().isdigit() and len(line.strip()) == 1:\n",
        "                continue # í•´ë‹¹ ì¤„ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ì¤„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n",
        "\n",
        "            # ëª¨ë“  ì¤„ì˜ ì–‘ìª½ ê³µë°± ì œê±° í›„ ì„ì‹œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì›ë˜ ì¤„ë°”ê¿ˆë„ ì œê±°)\n",
        "            temp_content.append(line.strip())\n",
        "\n",
        "        # ì„ì‹œ ë¦¬ìŠ¤íŠ¸ì˜ ì¤„ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•© (ê° ì¤„ ì‚¬ì´ì— ê³µë°± 1ê°œë¡œ ì—°ê²°)\n",
        "        cleaned_raw_text = ' '.join(temp_content).strip()\n",
        "\n",
        "        # 3ë‹¨ê³„: 'ì§ˆì˜ N.' ì•ì— ë¹ˆ ì¤„ ì¶”ê°€\n",
        "        # 'ì§ˆì˜ N.' (ë˜ëŠ” 'ì§ˆì˜ N') íŒ¨í„´ì„ ì°¾ì•„ì„œ '\\n\\nì§ˆì˜ N.'ìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤.\n",
        "        # ë‹¨, íŒŒì¼ì˜ ë§¨ ì²˜ìŒ ë‚˜ì˜¤ëŠ” 'ì§ˆì˜ 1.' ì•ì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "\n",
        "        # re.subì˜ repl ë§¤ê°œë³€ìˆ˜ì— í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  êµì²´\n",
        "        def replace_query_marker(match):\n",
        "            # match.start() == 0 ì´ë©´ íŒŒì¼ì˜ ë§¨ ì²˜ìŒ 'ì§ˆì˜ N.'ì…ë‹ˆë‹¤.\n",
        "            if match.start() == 0:\n",
        "                return match.group(0) # 'ì§ˆì˜ N.' ìì²´ë¥¼ ë°˜í™˜ (ì•ì— ì•„ë¬´ê²ƒë„ ì•ˆ ë¶™ì„)\n",
        "            else:\n",
        "                return '\\n\\n' + match.group(0) # ê·¸ ì™¸ì˜ 'ì§ˆì˜ N.' ì•ì—ëŠ” '\\n\\n'ì„ ë¶™ì„\n",
        "\n",
        "        # íŒ¨í„´: 'ì§ˆì˜' ë‹¤ìŒì— ê³µë°±(0ê°œ ì´ìƒ), ìˆ«ì(1ê°œ ì´ìƒ), ì (ì„ íƒì )\n",
        "        final_content = re.sub(r'ì§ˆì˜\\s*\\d+\\.?', replace_query_marker, cleaned_raw_text)\n",
        "\n",
        "        final_content = re.sub(r'(íšŒì‹ \\s*\\d\\.?)', r'\\n\\1', final_content)\n",
        "\n",
        "        final_content = re.sub(r'â–£.*', '', final_content)  # 'â–£'ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì œê±°\n",
        "\n",
        "        final_content = re.sub(r'\\s\\d{2,3}\\s', '', final_content)  # ìˆ«ì(2-3ìë¦¬) ì œê±°\n",
        "\n",
        "        # ìµœì¢…ì ìœ¼ë¡œ ë¬¸ìì—´ì˜ ì‹œì‘ê³¼ ëì— ë¶ˆí•„ìš”í•œ ê³µë°±/ê°œí–‰ì„ ì œê±°\n",
        "        final_content = final_content.strip()\n",
        "\n",
        "        with open(output_filepath, 'w', encoding='utf-8') as f_out:\n",
        "            f_out.write(final_content)\n",
        "\n",
        "        print(f\"íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ '{output_filepath}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "        print(\"ì´ íŒŒì¼ì„ Doccanoì— 'Plain Text' í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ì‹œë©´ ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ '{input_filepath}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    except Exception as e:\n",
        "        print(f\"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "# --- ì‚¬ìš© ë°©ë²• (ì•„ë˜ ê²½ë¡œë¥¼ ë‹¹ì‹ ì˜ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”) ---\n",
        "\n",
        "# ë¡œì»¬ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (VS Codeì—ì„œ í•´ë‹¹ íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œ)\n",
        "# ì˜ˆ: r\"C:\\Users\\JHSHIN\\ProgrammingCodes\\deep-learning\"\n",
        "project_root = r\"C:\\Users\\JHSHIN\\ProgrammingCodes\\deep-learning\"\n",
        "\n",
        "# ì›ë³¸ ì…ë ¥ íŒŒì¼ ê²½ë¡œ (ë‹¹ì‹ ì´ ê°€ì§€ê³  ìˆëŠ” ì›ë³¸ dataset.txt íŒŒì¼)\n",
        "input_file = os.path.join(project_root, 'dataset.txt')\n",
        "\n",
        "# ìˆ˜ì •ëœ ë‚´ìš©ì„ ì €ì¥í•  ìƒˆë¡œìš´ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ìƒˆë¡œìš´ ì´ë¦„ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ê²ƒì„ ê¶Œì¥)\n",
        "output_file = os.path.join(project_root, 'dataset_cleaned.txt') # íŒŒì¼ ì´ë¦„ ë³€ê²½\n",
        "\n",
        "# í•¨ìˆ˜ ì‹¤í–‰\n",
        "clean_and_prepare_text_for_doccano_final_v2(input_file, output_file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHJ-8kTGg2N-"
      },
      "source": [
        "**ì›ë³¸ `dataset.txt` íŒŒì¼ ì˜ˆì‹œ:**\n",
        "\n",
        "ì§ˆì˜ 1\n",
        "ì—°ë©´ì  450ã¡ì¸ íŠ¹ì •ì†Œë°©ëŒ€ìƒë¬¼ì— ìµœì´ˆ ê±´ì¶•ë¬¼ ì‚¬ìš©ìŠ¹ì¸ì‹œì— ë¹„ìƒê²½ë³´ì„¤ë¹„ ì„¤ì¹˜ê°€ ë˜ì§€\n",
        "ì•Šì€ ê²½ìš° ê±´ì¶•í—ˆê°€ì¼ê³¼ ì‚¬ìš©ìŠ¹ì¸ì¼ ì¤‘ ì†Œë°©ì‹œì„¤ì„¤ì¹˜ê¸°ì¤€ ì ìš©ì¼ì€?\n",
        "íšŒì‹  1\n",
        "ê±´ì¶•ë¬¼ ë“±ì˜ ì‹ ì¶•ã†ì¦ì¶•ã†ê°œì¶•ã†ì¬ì¶•ã†ì´ì „ã†ìš©ë„ë³€ê²½ ë˜ëŠ” ëŒ€ìˆ˜ì„ ì˜ í—ˆê°€\n",
        "ã†í˜‘ì˜ ë° ì‚¬ìš©ìŠ¹ì¸ì˜ ê¶Œí•œì´ ìˆëŠ” í–‰ì •ê¸°ê´€ì€ ì†Œë°©ì‹œì„¤ë²• ì œ7ì¡°ì œ1í•­ì—\n",
        "ë”°ë¼ ì†Œì¬ì§€ë¥¼ ê´€í• í•˜ëŠ” ì†Œë°©ë³¸ë¶€ì¥ì´ë‚˜ ì†Œë°©ì„œì¥ì˜ ë™ì˜ë¥¼ ë°›ì•„ì•¼ í•˜ë¯€ë¡œ,\n",
        "ê±´ì¶•í—ˆê°€ë“±ê³¼ ê´€ë ¨í•œ í˜‘ì˜ê³¼ì •ì´ ëˆ„ë½ë˜ì—ˆë‹¤ë©´, ê±´ì¶•í—ˆê°€ ì‹ ì²­ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ\n",
        "ì†Œë°©ì‹œì„¤ì˜ ì„¤ì¹˜ê¸°ì¤€ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
        "ì§ˆì˜ 2\n",
        "ìµœì´ˆ ì‚¬ì—…í—ˆê°€ìŠ¹ì¸ì›”ì´ â€˜13ë…„ 6ì›”ì¸ ëŒ€ìƒë¬¼ì˜ ì‚¬ì—…ì´ ë³€ê²½ë˜ì–´ ìµœì¢… ì‚¬ì—…í—ˆê°€ìŠ¹ì¸ì›”ì´\n",
        "19ë…„ 2ì›”ì¸ ê²½ìš°, ì†Œë°©ì‹œì„¤ë²• ì ìš© ê¸°ì¤€ì¼ì€?\n",
        "íšŒì‹  2\n",
        "ì†Œë°©ì‹œì„¤ì„¤ì¹˜ê¸°ì¤€ ì ìš© ê¸°ì¤€ì¼ì€ ìµœì´ˆ ì‚¬ìš©ìŠ¹ì¸ê³„íš ì‹ ì²­ ì‹œì ì…ë‹ˆë‹¤.2021ë…„ ì†Œë°©ì‹œì„¤ë²•ë ¹ ì§ˆì˜íšŒì‹ ì§‘\n",
        "4\n",
        "ìµœì´ˆ ê±´ì¶•í—ˆê°€ê³¼ì •ì—ì„œ í—ˆê°€ë™ì˜ëœ ì‚¬ì—…ê³„íšì€ ì´í›„ ì‚¬ì—… ë³€ê²½ê³„íšì´ ì‹ ì²­\n",
        "ë˜ì–´ë„ ë³€ê²½ê³„íšì´ ì‹ ì²­ëœ ì‹œì ì˜ ì†Œë°©ì‹œì„¤ë²•ì„ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¶€ì¹™\n",
        "<ëŒ€í†µë ¹ë ¹ ì œ27810í˜¸, 2017.1.26.>í˜¸ì œ2ì¡° ì†Œë°©ì‹œì„¤ ì„¤ì¹˜ì— ê´€í•œ ì ìš©ë¡€ì—\n",
        "ê´€í•œ ì ìš©ë¡€ì— íŠ¹ì •ì†Œë°©ëŒ€ìƒë¬¼ì˜ ì‹ ì¶•ã†ì¦ì¶•ã†ê°œì¶•ã†ì¬ì¶•ã†ì´ì „ã†ìš©ë„ë³€ê²½\n",
        "ã†ëŒ€ìˆ˜ì„ ì˜ í—ˆê°€ã†í˜‘ì˜ë¥¼ ì‹ ì²­í•˜ê±°ë‚˜ ì‹ ê³ í•˜ëŠ” ê²½ìš°ë¡œ ëª…ì‹œí•˜ê³  ìˆì–´, ì‚¬ìš©\n",
        "ìŠ¹ì¸ê³„íšë³€ê²½ ë“± í—ˆê°€ì˜ ë³€ê²½ì‚¬í•­ì€ ê°œì • ê·œì • ì ìš©ëŒ€ìƒì— í•´ë‹¹í•˜ì§€ ì•ŠìŠµ\n",
        "ë‹ˆë‹¤.\n",
        "â–£ ê±´ì¶•í—ˆê°€ë“±ì˜ ë™ì˜ëŒ€ìƒ ë²”ìœ„\n",
        "[ì†Œë°©ì‹œì„¤ë²• ì‹œí–‰ë ¹ ì œ12ì¡°]\n",
        "ê´€ê³„ë²•ë ¹\n",
        "ì œ12ì¡°(ê±´ì¶•í—ˆê°€ë“± ë™ì˜ëŒ€ìƒë¬¼ì˜ ë²”ìœ„ ë“±)\n",
        "â‘  ë²• ì œ7ì¡°ì œ1í•­ì— ë”°ë¼ ê±´ì¶•í—ˆê°€ë“±ì„ í•  ë•Œ ë¯¸ë¦¬ ì†Œë°©ë³¸ë¶€ì¥ ë˜ëŠ” ì†Œë°©ì„œì¥ì˜ ë™ì˜ë¥¼\n",
        "ë°›ì•„ì•¼ í•˜ëŠ” ê±´ì¶•ë¬¼ ë“±ì˜ ë²”ìœ„ëŠ” ë‹¤ìŒ ê° í˜¸ì™€ ê°™ë‹¤.\n",
        "1. ì—°ë©´ì (ã€Œê±´ì¶•ë²• ì‹œí–‰ë ¹ã€ ì œ119ì¡°ì œ1í•­ì œ4í˜¸ì— ë”°ë¼ ì‚°ì •ëœ ë©´ì ì„ ë§í•œë‹¤. ì´í•˜ ê°™\n",
        "ë‹¤)ì´ 400ì œê³±ë¯¸í„° ì´ìƒì¸ ê±´ì¶•ë¬¼. ë‹¤ë§Œ, ë‹¤ìŒ ê° ëª©ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ì‹œì„¤ì€\n",
        "í•´ë‹¹ ëª©ì—ì„œ ì •í•œ ê¸°ì¤€ ì´ìƒì¸ ê±´ì¶•ë¬¼ë¡œ í•œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVsYijB2g2N-"
      },
      "source": [
        "**`preprocessing_dataset.py` ì‹¤í–‰ í›„ `dataset_cleaned.txt` ì˜ˆì‹œ:**\n",
        "\n",
        "\n",
        "ì§ˆì˜ 1 ì—°ë©´ì  450ã¡ì¸ íŠ¹ì •ì†Œë°©ëŒ€ìƒë¬¼ì— ìµœì´ˆ ê±´ì¶•ë¬¼ ì‚¬ìš©ìŠ¹ì¸ì‹œì— ë¹„ìƒê²½ë³´ì„¤ë¹„ ì„¤ì¹˜ê°€ ë˜ì§€ ì•Šì€ ê²½ìš° ê±´ì¶•í—ˆê°€ì¼ê³¼ ì‚¬ìš©ìŠ¹ì¸ì¼ ì¤‘ ì†Œë°©ì‹œì„¤ì„¤ì¹˜ê¸°ì¤€ ì ìš©ì¼ì€? íšŒì‹  1 ê±´ì¶•ë¬¼ ë“±ì˜ ì‹ ì¶•ã†ì¦ì¶•ã†ê°œì¶•ã†ì¬ì¶•ã†ì´ì „ã†ìš©ë„ë³€ê²½ ë˜ëŠ” ëŒ€ìˆ˜ì„ ì˜ í—ˆê°€ ã†í˜‘ì˜ ë° ì‚¬ìš©ìŠ¹ì¸ì˜ ê¶Œí•œì´ ìˆëŠ” í–‰ì •ê¸°ê´€ì€ ì†Œë°©ì‹œì„¤ë²• ì œ7ì¡°ì œ1í•­ì— ë”°ë¼ ì†Œì¬ì§€ë¥¼ ê´€í• í•˜ëŠ” ì†Œë°©ë³¸ë¶€ì¥ì´ë‚˜ ì†Œë°©ì„œì¥ì˜ ë™ì˜ë¥¼ ë°›ì•„ì•¼ í•˜ë¯€ë¡œ, ê±´ì¶•í—ˆê°€ë“±ê³¼ ê´€ë ¨í•œ í˜‘ì˜ê³¼ì •ì´ ëˆ„ë½ë˜ì—ˆë‹¤ë©´, ê±´ì¶•í—ˆê°€ ì‹ ì²­ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì†Œë°©ì‹œì„¤ì˜ ì„¤ì¹˜ê¸°ì¤€ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì§ˆì˜ 2 ìµœì´ˆ ì‚¬ì—…í—ˆê°€ìŠ¹ì¸ì›”ì´ â€˜13ë…„ 6ì›”ì¸ ëŒ€ìƒë¬¼ì˜ ì‚¬ì—…ì´ ë³€ê²½ë˜ì–´ ìµœì¢… ì‚¬ì—…í—ˆê°€ìŠ¹ì¸ì›”ì´ 19ë…„ 2ì›”ì¸ ê²½ìš°, ì†Œë°©ì‹œì„¤ë²• ì ìš© ê¸°ì¤€ì¼ì€? íšŒì‹  2 ì†Œë°©ì‹œì„¤ì„¤ì¹˜ê¸°ì¤€ ì ìš© ê¸°ì¤€ì¼ì€ ìµœì´ˆ ì‚¬ìš©ìŠ¹ì¸ê³„íš ì‹ ì²­ ì‹œì ì…ë‹ˆë‹¤. ìµœì´ˆ ê±´ì¶•í—ˆê°€ê³¼ì •ì—ì„œ í—ˆê°€ë™ì˜ëœ ì‚¬ì—…ê³„íšì€ ì´í›„ ì‚¬ì—… ë³€ê²½ê³„íšì´ ì‹ ì²­ ë˜ì–´ë„ ë³€ê²½ê³„íšì´ ì‹ ì²­ëœ ì‹œì ì˜ ì†Œë°©ì‹œì„¤ë²•ì„ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¶€ì¹™ <ëŒ€í†µë ¹ë ¹ ì œ27810í˜¸, 2017.1.26.>í˜¸ì œ2ì¡° ì†Œë°©ì‹œì„¤ ì„¤ì¹˜ì— ê´€í•œ ì ìš©ë¡€ì— ê´€í•œ ì ìš©ë¡€ì— íŠ¹ì •ì†Œë°©ëŒ€ìƒë¬¼ì˜ ì‹ ì¶•ã†ì¦ì¶•ã†ê°œì¶•ã†ì¬ì¶•ã†ì´ì „ã†ìš©ë„ë³€ê²½ ã†ëŒ€ìˆ˜ì„ ì˜ í—ˆê°€ã†í˜‘ì˜ë¥¼ ì‹ ì²­í•˜ê±°ë‚˜ ì‹ ê³ í•˜ëŠ” ê²½ìš°ë¡œ ëª…ì‹œí•˜ê³  ìˆì–´, ì‚¬ìš© ìŠ¹ì¸ê³„íšë³€ê²½ ë“± í—ˆê°€ì˜ ë³€ê²½ì‚¬í•­ì€ ê°œì • ê·œì • ì ìš©ëŒ€ìƒì— í•´ë‹¹í•˜ì§€ ì•ŠìŠµ ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8N8ZjC4Ae-k"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5q7cERn_aD"
      },
      "source": [
        "## 2. Doccanoë¥¼ ì´ìš©í•œ ë°ì´í„° ë¼ë²¨ë§\n",
        "\n",
        "DoccanoëŠ” í…ìŠ¤íŠ¸ ì–´ë…¸í…Œì´ì…˜(ë¼ë²¨ë§)ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì…ë‹ˆë‹¤. ì›¹ ê¸°ë°˜ í™˜ê²½ì—ì„œ ì§ê´€ì ìœ¼ë¡œ ê°œì²´ëª…(NER) ë¼ë²¨ë§ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8KfePBUg2N-"
      },
      "source": [
        "### **(New) 2.0 ì•½ ì§€ë„ í•™ìŠµ (Weak Supervision)ìœ¼ë¡œ ë¼ë²¨ë§ ê°€ì†í™”** ğŸš€\n",
        "\n",
        "ìˆ˜ì²œ ê°œì˜ ë°ì´í„°ë¥¼ ì²˜ìŒë¶€í„° ìˆ˜ë™ìœ¼ë¡œ ë¼ë²¨ë§í•˜ëŠ” ê²ƒì€ ë§¤ìš° í˜ë“  ì‘ì—…ì…ë‹ˆë‹¤. **ì•½ ì§€ë„ í•™ìŠµ**ì€ ì •ê·œ í‘œí˜„ì‹(Regex)ê³¼ ê°™ì€ ê°„ë‹¨í•œ **ê·œì¹™(Heuristics)ì„ ì´ìš©í•´ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì— ìë™ìœ¼ë¡œ ë¼ë²¨ì„ ë¶€ì—¬**í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "- ì •ê·œí‘œí˜„ì‹(Regex)ê³¼ ê°™ì€ ê°„ë‹¨í•œ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ 'ì§ˆì˜ 1', 'íšŒì‹  1' ë“± ëª…í™•í•œ íŒ¨í„´ì„ ê°€ì§„ ê°œì²´ë¥¼ ìë™ìœ¼ë¡œ ë¼ë²¨ë§í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ì´ë ‡ê²Œ ìƒì„±ëœ \"ì´ˆë²Œ ë¼ë²¨ë§\" ë°ì´í„°ë¥¼ Doccanoì— ì„í¬íŠ¸í•˜ì—¬ ê²€í†  ë° ìˆ˜ì •ë§Œ í•˜ë©´ ë˜ë¯€ë¡œ, ë¼ë²¨ë§ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69bJPq2Hg2N_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "def create_weak_labels_advanced(input_text_path, output_jsonl_path):\n",
        "    \"\"\"\n",
        "    ê³ ë„ë¡œ ìƒì„¸í™”ëœ ì •ê·œì‹ íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©í•´ ìë™ìœ¼ë¡œ ë¼ë²¨ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    - ID, QUESTION_CONTENT, ANSWER_CONTENTëŠ” êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ë¼ë²¨ë§\n",
        "    - LAW_CONTENTëŠ” ìƒì„¸í™”ëœ ë³µí•© íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ì •êµí•˜ê²Œ ë¼ë²¨ë§\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(input_text_path, 'r', encoding='utf-8') as f:\n",
        "            documents = f.read().strip().split('\\n\\n')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ì˜¤ë¥˜: '{input_text_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "        return\n",
        "\n",
        "    # --- ë¼ë²¨ë§ ê·œì¹™(íŒ¨í„´) ìƒì„¸í™” ---\n",
        "    # ID íŒ¨í„´\n",
        "    question_id_pattern = re.compile(r'ì§ˆì˜\\s*\\d+\\.?(?=\\s|$)')\n",
        "    answer_id_pattern = re.compile(r'íšŒì‹ \\s*\\d+\\.?(?=\\s|$)')\n",
        "\n",
        "    # LAW_CONTENTë¥¼ ì°¾ê¸° ìœ„í•œ ê³ ë„ë¡œ ìƒì„¸í™”ëœ íœ´ë¦¬ìŠ¤í‹±(Heuristics) íŒ¨í„´\n",
        "    # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€(ë” êµ¬ì²´ì ì¸) íŒ¨í„´ì„ ë¦¬ìŠ¤íŠ¸ì˜ ìœ„ìª½ì— ë°°ì¹˜í•©ë‹ˆë‹¤.\n",
        "    law_patterns = [\n",
        "        # 1. ã€Œ...ë²•ã€, ã€...ê¸°ì¤€ã€ ë“± íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ê°ì‹¸ì§„ ë²•ë¥ /ê¸°ì¤€ ì´ë¦„ (ê°€ì¥ ê°•ë ¥í•œ íŒ¨í„´)\n",
        "        re.compile(r'(?:ã€Œ|ã€)[^ã€ã€]+(?:ë²•|ë²•ë¥ |ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™|ê¸°ì¤€)(?:ã€|ã€)'),\n",
        "\n",
        "        # 2. 'ì†Œë°©ì‹œì„¤ë²• ì‹œí–‰ë ¹ ì œ12ì¡°ì œ1í•­ì œ1í˜¸' ì™€ ê°™ì´ ë²• ì´ë¦„ê³¼ ì¡°í•­ì´ í•¨ê»˜ ë‚˜ì˜¤ëŠ” íŒ¨í„´\n",
        "        re.compile(r'[^ã€ã€\\s]+(?:ë²•|ë ¹|ê·œì¹™|ê¸°ì¤€)\\s?ì œ\\s?\\d+ì¡°(?:\\s?ì œ\\s?\\d+í•­)?(?:\\s?ì œ\\s?\\d+í˜¸)?'),\n",
        "\n",
        "        # 3. '[ì†Œë°©ì‹œì„¤ë²• ì œ9ì¡°]' ì™€ ê°™ì´ ëŒ€ê´„í˜¸ë¡œ ê°ì‹¸ì§„ ë²•ë¥  ë° ì¡°í•­\n",
        "        re.compile(r'\\[\\s?[^\\]]+(?:ë²•|ë ¹|ê¸°ì¤€|ì¡°)\\s?[^\\]]*\\]'),\n",
        "\n",
        "        # 4. '[ë³„í‘œ5]' ì™€ ê°™ì´ ë³„í‘œ/ì„œì‹ì„ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í„´\n",
        "        re.compile(r'\\[\\s?ë³„í‘œ\\s?\\d+\\s?\\]'),\n",
        "\n",
        "        # 5. 'ì œNì¡° ì œNí•­ ì œNí˜¸' ë“± ë²•ë¥  ì¡°í•­ë§Œ ë‹¨ë…ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” íŒ¨í„´\n",
        "        re.compile(r'ì œ\\s?\\d+ì¡°(?:\\s?ì œ\\s?\\d+í•­)?(?:\\s?ì œ\\s?\\d+í˜¸)?'),\n",
        "\n",
        "        # 6. 'í™”ì¬ì•ˆì „ê¸°ì¤€' ë“± ë‹¨ë…ìœ¼ë¡œ ì“°ì´ëŠ” í•µì‹¬ ë²•ê·œ/ê¸°ì¤€ í‚¤ì›Œë“œ\n",
        "        re.compile(r'í™”ì¬ì•ˆì „ê¸°ì¤€'),\n",
        "\n",
        "        # 7. '...ë²•ì— ë”°ë¼' ë“± ë²•ì  ê·¼ê±°ë¥¼ ì œì‹œí•˜ëŠ” í‘œí˜„ (ê°€ì¥ ë²”ìœ„ê°€ ë„“ìœ¼ë¯€ë¡œ ë§ˆì§€ë§‰ì— ë°°ì¹˜)\n",
        "        re.compile(r'\\S+ë²•[ì—\\s](?:ë”°ë¼|ë”°ë¥´ë©´|ì˜í•˜ë©´|ê·¼ê±°í•˜ì—¬)')\n",
        "    ]\n",
        "\n",
        "    all_labeled_docs = []\n",
        "    print(f\"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œì— ëŒ€í•´ í–¥ìƒëœ ì•½ ì§€ë„ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "    for doc_text in documents:\n",
        "        if not doc_text.strip():\n",
        "            continue\n",
        "\n",
        "        spans = []\n",
        "        markers = []\n",
        "\n",
        "        # 1. ë¬¸ì„œ ë‚´ ëª¨ë“  ID ë§ˆì»¤ì˜ ìœ„ì¹˜ì™€ ì¢…ë¥˜ë¥¼ ì°¾ìŒ\n",
        "        for match in question_id_pattern.finditer(doc_text):\n",
        "            markers.append({'start': match.start(), 'end': match.end(), 'type': 'QUESTION_ID'})\n",
        "        for match in answer_id_pattern.finditer(doc_text):\n",
        "            markers.append({'start': match.start(), 'end': match.end(), 'type': 'ANSWER_ID'})\n",
        "\n",
        "        markers.sort(key=lambda x: x['start'])\n",
        "        if not markers:\n",
        "            continue\n",
        "\n",
        "        # 2. ë§ˆì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ê°€ë©° CONTENT ë¼ë²¨ë§\n",
        "        for i in range(len(markers)):\n",
        "            current_marker = markers[i]\n",
        "            spans.append([current_marker['start'], current_marker['end'], current_marker['type']])\n",
        "\n",
        "            content_start = current_marker['end']\n",
        "            content_end = markers[i+1]['start'] if i + 1 < len(markers) else len(doc_text)\n",
        "\n",
        "            content_text_segment = doc_text[content_start:content_end]\n",
        "            lstrip_len = len(content_text_segment) - len(content_text_segment.lstrip())\n",
        "            content_start += lstrip_len\n",
        "            content_end -= (len(content_text_segment) - len(content_text_segment.rstrip()))\n",
        "\n",
        "            if content_start >= content_end: continue\n",
        "\n",
        "            if current_marker['type'] == 'QUESTION_ID':\n",
        "                spans.append([content_start, content_end, 'QUESTION_CONTENT'])\n",
        "\n",
        "            elif current_marker['type'] == 'ANSWER_ID':\n",
        "                answer_block_text = doc_text[content_start:content_end]\n",
        "                law_spans_in_block = []\n",
        "\n",
        "                # 3. ANSWER_CONTENT ë‚´ì—ì„œ ëª¨ë“  LAW_CONTENT íŒ¨í„´ ì°¾ê¸°\n",
        "                for pattern in law_patterns:\n",
        "                    for match in pattern.finditer(answer_block_text):\n",
        "                        law_start = content_start + match.start()\n",
        "                        law_end = content_start + match.end()\n",
        "                        law_spans_in_block.append([law_start, law_end, 'LAW_CONTENT'])\n",
        "\n",
        "                if not law_spans_in_block:\n",
        "                    spans.append([content_start, content_end, 'ANSWER_CONTENT'])\n",
        "                    continue\n",
        "\n",
        "                # 4. ì°¾ì€ LAW_CONTENTë“¤ì„ ë³‘í•©í•˜ê³ , ê·¸ ì™¸ ë¶€ë¶„ì„ ANSWER_CONTENTë¡œ ë¼ë²¨ë§\n",
        "                law_spans_in_block.sort(key=lambda x: x[0])\n",
        "\n",
        "                # ì¤‘ì²©/ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ ë³‘í•© (Merge overlapping spans)\n",
        "                merged_law_spans = []\n",
        "                if law_spans_in_block:\n",
        "                    current_span = law_spans_in_block[0]\n",
        "                    for next_span in law_spans_in_block[1:]:\n",
        "                        if next_span[0] < current_span[1]: # ê²¹ì¹˜ëŠ” ê²½ìš°\n",
        "                            current_span[1] = max(current_span[1], next_span[1])\n",
        "                        else:\n",
        "                            merged_law_spans.append(current_span)\n",
        "                            current_span = next_span\n",
        "                    merged_law_spans.append(current_span)\n",
        "\n",
        "                # LAW_CONTENTë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ ANSWER_CONTENTë¡œ ì±„ìš°ê¸°\n",
        "                last_end = content_start\n",
        "                for law_start, law_end, law_label in merged_law_spans:\n",
        "                    if law_start > last_end:\n",
        "                        spans.append([last_end, law_start, 'ANSWER_CONTENT'])\n",
        "                    spans.append([law_start, law_end, law_label])\n",
        "                    last_end = law_end\n",
        "\n",
        "                if content_end > last_end:\n",
        "                    spans.append([last_end, content_end, 'ANSWER_CONTENT'])\n",
        "\n",
        "        spans.sort(key=lambda x: x[0])\n",
        "        all_labeled_docs.append({\"text\": doc_text, \"labels\": spans})\n",
        "\n",
        "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
        "        for doc in all_labeled_docs:\n",
        "            f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"ê³ ë„ë¡œ ìƒì„¸í™”ëœ ì•½ ì§€ë„ í•™ìŠµ ì™„ë£Œ! {len(all_labeled_docs)}ê°œì˜ ë¬¸ì„œê°€ '{output_jsonl_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# --- í•¨ìˆ˜ ì‹¤í–‰ ---\n",
        "# ì•„ë˜ ê²½ë¡œë“¤ì€ ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "# project_root = os.path.dirname(os.path.abspath(__file__)) # local í™˜ê²½ì—ì„œ ì‹¤í–‰ ì‹œ ì£¼ì„ í•´ì œ (í˜„ì¬ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •)\n",
        "\n",
        "print(f\"í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ: {project_root}\")\n",
        "\n",
        "after_perprocessing = os.path.join(project_root, 'data', 'dataset_cleaned.txt')\n",
        "weakly_labeled_path = os.path.join(project_root, 'data', 'weakly_labeled_advanced.jsonl')\n",
        "# weakly_labeled_path = os.path.join('data_dir', 'weakly_labeled_advanced.json')\n",
        "\n",
        "# # íŒŒì¼ì´ ì¡´ì¬í•  ë•Œë§Œ ì‹¤í–‰\n",
        "if os.path.exists(after_perprocessing):\n",
        "    create_weak_labels_advanced(after_perprocessing, weakly_labeled_path)\n",
        "else:\n",
        "    print(f\"ì…ë ¥ íŒŒì¼ '{after_perprocessing}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lM6oGuIAe-l"
      },
      "source": [
        "### 2.1 Doccano ì„¤ì¹˜ ë° ì‹¤í–‰ (Docker ì‚¬ìš©)\n",
        "\n",
        "DoccanoëŠ” Dockerë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¥ ì‰½ê²Œ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ ì»´í“¨í„°ì— Dockerê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "1.  **Docker ì„¤ì¹˜:** Docker Desktopì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì„¤ì¹˜í•©ë‹ˆë‹¤: [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)\n",
        "\n",
        "2.  **Doccano Docker ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰:** í„°ë¯¸ë„ ë˜ëŠ” ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "    ```bash\n",
        "    docker pull doccano/doccano\n",
        "    docker run -it -p 8000:8000 doccano/doccano\n",
        "    ```\n",
        "\n",
        "3.  **Doccano ì ‘ì†:** ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ì—´ê³  `http://localhost:8000/`ìœ¼ë¡œ ì ‘ì†í•©ë‹ˆë‹¤. ê¸°ë³¸ ê´€ë¦¬ì ê³„ì •ì€ `admin / admin`ì…ë‹ˆë‹¤.\n",
        "\n",
        "### 2.2 Doccano í”„ë¡œì íŠ¸ ìƒì„± ë° ì„¤ì •\n",
        "\n",
        "1.  **ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±:** Doccano ì›¹ UIì—ì„œ `Create new project` ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
        "2.  **í”„ë¡œì íŠ¸ ì´ë¦„ ë° ì„¤ëª… ì…ë ¥:** í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì§€ì •í•˜ê³ (ì˜ˆ: 'ë²•ë ¹ ì§ˆì˜íšŒì‹  NER') ì„¤ëª…ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "3.  **í”„ë¡œì íŠ¸ ìœ í˜• ì„ íƒ:** `Sequence Labeling` (ê°œì²´ëª… ì¸ì‹ì„ ìœ„í•œ ìœ í˜•)ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
        "4.  **ë ˆì´ë¸” ì •ì˜:** NER ëª¨ë¸ì´ ì¸ì‹í•  ê°œì²´ëª… ë ˆì´ë¸”ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\n",
        "\n",
        "    - `QUESTION_ID`\n",
        "    - `QUESTION_CONTENT`\n",
        "    - `ANSWER_ID`\n",
        "    - `ANSWER_CONTENT`\n",
        "    - `MISC_HEADER` (ì˜ˆ: ê³ ì‹œ ë²ˆí˜¸, ë¬¸ì„œ ì œëª©)\n",
        "    - `LAW_CONTENT` (ì˜ˆ: íŠ¹ì • ë²•ë ¹ ì¡°í•­, ë²•ë¥  ì´ë¦„)\n",
        "\n",
        "    ê° ë ˆì´ë¸”ì— ë‹¨ì¶•í‚¤ì™€ ìƒ‰ìƒì„ ì§€ì •í•˜ë©´ ë¼ë²¨ë§ íš¨ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 2.3 í´ë¦¬ë‹ëœ ë°ì´í„° Doccanoë¡œ ê°€ì ¸ì˜¤ê¸° (Import)\n",
        "\n",
        "1.  **Import Data íƒ­ ì´ë™:** ìƒì„±ëœ í”„ë¡œì íŠ¸ í˜ì´ì§€ì—ì„œ `Import Data` íƒ­ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
        "2.  **íŒŒì¼ ì„ íƒ:** `dataset_cleaned_final.txt` íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
        "3.  **íŒŒì¼ í˜•ì‹ ì„ íƒ:** `Plain Text`ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. (ìš°ë¦¬ê°€ `\\n\\n`ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë¶„ë¦¬í–ˆê¸° ë•Œë¬¸ì—, Doccanoê°€ ì´ë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì¸ì‹í•©ë‹ˆë‹¤.)\n",
        "4.  **Import ì‹œì‘:** `Import` ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 2.4 ê°œì²´ëª… ë¼ë²¨ë§ ìˆ˜í–‰\n",
        "\n",
        "1.  **Annotate Data íƒ­ ì´ë™:** `Annotate Data` íƒ­ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
        "2.  **í…ìŠ¤íŠ¸ ì„ íƒ ë° ë¼ë²¨ ì§€ì •:** ê° ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë‚˜ êµ¬ì ˆì„ ë“œë˜ê·¸í•˜ì—¬ ì„ íƒí•œ í›„, ì˜¤ë¥¸ìª½ì— ë‚˜íƒ€ë‚˜ëŠ” ë ˆì´ë¸” ë²„íŠ¼(ë˜ëŠ” ì§€ì •ëœ ë‹¨ì¶•í‚¤)ì„ í´ë¦­í•˜ì—¬ ë¼ë²¨ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "3.  **ì‘ì—… ì €ì¥ ë° ë‹¤ìŒ ë¬¸ì„œë¡œ ì´ë™:** í•œ ë¬¸ì„œì˜ ë¼ë²¨ë§ì„ ë§ˆì³¤ìœ¼ë©´ `Submit` ë˜ëŠ” `Save` ë²„íŠ¼ì„ ëˆŒëŸ¬ ì €ì¥í•˜ê³  ë‹¤ìŒ ë¬¸ì„œë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n",
        "\n",
        "### 2.5 ë¼ë²¨ë§ ì™„ë£Œ í›„ ë°ì´í„° ë‚´ë³´ë‚´ê¸° (Export)\n",
        "\n",
        "ì¶©ë¶„í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ ë¼ë²¨ë§í–ˆë‹¤ë©´ (ì´ˆê¸° í•™ìŠµì„ ìœ„í•´ ìµœì†Œ 100ê°œ ì´ìƒ, ì‹¤ì‚¬ìš©ì„ ìœ„í•´ ìˆ˜ë°±~ìˆ˜ì²œ ê°œ ê¶Œì¥), ì´ì œ ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ë‚´ë³´ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "1.  **Export Data íƒ­ ì´ë™:** í”„ë¡œì íŠ¸ í˜ì´ì§€ì—ì„œ `Export Data` íƒ­ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
        "2.  **íŒŒì¼ í˜•ì‹ ì„ íƒ:** `JSONL`ì„ ì„ íƒí•©ë‹ˆë‹¤. (Hugging Face `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì‰½ê²Œ ë¡œë“œí•  ìˆ˜ ìˆëŠ” í˜•ì‹ì…ë‹ˆë‹¤.)\n",
        "3.  **'Export only approved documents' ì²´í¬ (ë§¤ìš° ì¤‘ìš”!)**:\n",
        "    ì´ ì˜µì…˜ì„ **ë°˜ë“œì‹œ ì²´í¬**í•©ë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ë¼ë²¨ë§ì´ ì™„ë£Œë˜ê³  `Approve` (ìŠ¹ì¸)ëœ ë¬¸ì„œë§Œ ë‚´ë³´ë‚´ì–´ í•™ìŠµ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤. ìŠ¹ì¸ë˜ì§€ ì•Šì€ ë¬¸ì„œëŠ” ì•„ì§ ê²€í† ê°€ í•„ìš”í•˜ê±°ë‚˜ ìˆ˜ì •ë  ì—¬ì§€ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n",
        "\n",
        "    - **Approve í•˜ëŠ” ë°©ë²•:** `Annotate` í™”ë©´ì—ì„œ ê° ë¬¸ì„œì˜ ë¼ë²¨ë§ì„ ë§ˆì¹œ í›„ `Approve` ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜, `Annotation` -> `Guideline` -> `Approve` ë¡œ ì´ë™í•˜ì—¬ ì¼ê´„ ìŠ¹ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "4.  **Export ì‹œì‘:** `Export` ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ `.jsonl` íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "\n",
        "**ë‹¤ìš´ë¡œë“œí•œ `.jsonl` íŒŒì¼ì„ Google Driveì˜ `data` í´ë”(ì˜ˆ: `/content/gdrive/MyDrive/Colab Notebooks/deep-learning/data/`)ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.** íŒŒì¼ ì´ë¦„ì„ ê¸°ì–µí•´ë‘ì„¸ìš” (ì˜ˆ: `after_datalabeling.jsonl`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYHjdUpWAe-l"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKuEbPmPAe-l"
      },
      "source": [
        "## 3. Colabì—ì„œ ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "\n",
        "Doccanoì—ì„œ ë‚´ë³´ë‚¸ `.jsonl` íŒŒì¼ì„ Google Colabìœ¼ë¡œ ê°€ì ¸ì™€ BERT ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ê° ë‹¨ì–´(ë˜ëŠ” ì„œë¸Œì›Œë“œ í† í°)ì— BIO(Beginning, Inside, Outside) íƒœê·¸ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\n",
        "\n",
        "**âš ï¸ ì¤‘ìš”: Colab ëŸ°íƒ€ì„ì´ ëŠì–´ì¡Œë‹¤ë©´, ë°˜ë“œì‹œ `0.2 Google Drive ë§ˆìš´íŠ¸`, `0.3 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •`, `0.4 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜` ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•œ í›„ ì´ ë‹¨ê³„ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kMQNzsRcAe-l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
        "from transformers import AutoTokenizer\n",
        "from pprint import pprint\n",
        "\n",
        "# --- 1. Doccanoì—ì„œ ì •ì˜í•œ ë ˆì´ë¸” ëª©ë¡ (B-, I- ì—†ì´) ---\n",
        "# Doccanoì—ì„œ í”„ë¡œì íŠ¸ ì„¤ì • ì‹œ ì •ì˜í–ˆë˜ ë ˆì´ë¸” ì´ë¦„ë“¤ì„ ì—¬ê¸°ì— ì •í™•íˆ ì…ë ¥í•©ë‹ˆë‹¤.\n",
        "doccano_raw_labels = [\n",
        "    \"QUESTION_ID\",\n",
        "    \"QUESTION_CONTENT\",\n",
        "    \"ANSWER_ID\",\n",
        "    \"ANSWER_CONTENT\",\n",
        "    # \"MISC_HEADER\",\n",
        "    \"LAW_CONTENT\",\n",
        "]\n",
        "\n",
        "# --- 2. ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ìµœì¢… BIO ë ˆì´ë¸” ëª©ë¡ ë° ë§¤í•‘ ---\n",
        "# 'O' (Other) íƒœê·¸ëŠ” ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ëª¨ë“  í† í°ì„ ì˜ë¯¸í•˜ë©° í•­ìƒ í¬í•¨ë©ë‹ˆë‹¤.\n",
        "# ê° ì›ë³¸ ë ˆì´ë¸”ì— ëŒ€í•´ 'B-' (Beginning)ì™€ 'I-' (Inside) íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "label_list = [\"O\"]  # 'Other' íƒœê·¸ëŠ” í•­ìƒ í¬í•¨\n",
        "for label in doccano_raw_labels:\n",
        "    label_list.append(f\"B-{label}\")  # Beginning íƒœê·¸: ê°œì²´ëª…ì˜ ì²« ë²ˆì§¸ í† í°\n",
        "    label_list.append(f\"I-{label}\")  # Inside íƒœê·¸: ê°œì²´ëª…ì˜ ë‘ ë²ˆì§¸ ì´í›„ í† í°\n",
        "\n",
        "# ë ˆì´ë¸” ì´ë¦„ê³¼ ì •ìˆ˜ ID ê°„ì˜ ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "num_labels = len(label_list)  # ëª¨ë¸ì˜ ì¶œë ¥ ë ˆì´ì–´ í¬ê¸°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "print(f\"ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ìµœì¢… ë ˆì´ë¸” ëª©ë¡: {label_list}\")\n",
        "print(f\"ì´ ë ˆì´ë¸” ê°œìˆ˜: {num_labels}\")\n",
        "\n",
        "pprint(label_to_id)\n",
        "pprint(id_to_label)\n",
        "\n",
        "\n",
        "# --- 3. Doccanoì—ì„œ ë‚´ë³´ë‚¸ JSONL íŒŒì¼ ê²½ë¡œ ì„¤ì • ---\n",
        "# ì´ ê²½ë¡œëŠ” 0.3ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ data_dirê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "# Colab ëŸ°íƒ€ì„ì´ ì¬ì‹œì‘ë˜ë©´ ë³€ìˆ˜ê°€ ì´ˆê¸°í™”ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë°©ì–´ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ 0.3ë‹¨ê³„ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "try:\n",
        "    # project_rootê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ (ëŸ°íƒ€ì„ ì¬ì‹œì‘ ë“±), ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •\n",
        "    if \"project_root\" not in locals():\n",
        "        project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning/\"\n",
        "        data_dir = os.path.join(project_root, \"data\")\n",
        "        print(\"ê²½ë¡œ ë³€ìˆ˜ 'project_root'ê°€ ì •ì˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "except NameError:\n",
        "    # NameError ë°œìƒ ì‹œ (ì•„ì˜ˆ ë³€ìˆ˜ ì„ ì–¸ì´ ì•ˆ ë˜ì–´ ìˆì„ ë•Œ)\n",
        "    project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning/\"\n",
        "    data_dir = os.path.join(project_root, \"data\")\n",
        "    print(\"ê²½ë¡œ ë³€ìˆ˜ 'project_root'ê°€ ì •ì˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# Doccanoì—ì„œ ë‚´ë³´ë‚¸ ì‹¤ì œ JSONL íŒŒì¼ ì´ë¦„ì„ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.\n",
        "labeled_data_file_path = os.path.join(data_dir, \"after_datalabeling.jsonl\")\n",
        "\n",
        "# --- 4. ì‚¬ìš©í•  í† í¬ë‚˜ì´ì € ---\n",
        "# í•œêµ­ì–´ BERT ëª¨ë¸ì¸ 'klue/bert-base' í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "# --- 5. Doccano JSONL ë¡œë“œ ë° Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ---\n",
        "converted_data_for_hf = []\n",
        "try:\n",
        "    with open(labeled_data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            doc_data = json.loads(line)\n",
        "            pprint(doc_data)\n",
        "            text = doc_data[\"text\"]\n",
        "            pprint(text)\n",
        "            # Doccanoì˜ 'label' í•„ë“œëŠ” [[start_offset, end_offset, \"LABEL_NAME\"], ...] í˜•ì‹ì…ë‹ˆë‹¤.\n",
        "            annotations = doc_data.get(\"labels\", [])\n",
        "            pprint(annotations)\n",
        "            # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ê° í† í°ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œì˜ ìœ„ì¹˜(offset)ë¥¼ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "            # ì¤‘ìš”: paddingê³¼ truncationì„ í™œì„±í™”í•˜ì—¬ ëª¨ë“  ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ í†µì¼í•©ë‹ˆë‹¤.\n",
        "            tokenized_output = tokenizer(\n",
        "                text,\n",
        "                return_offsets_mapping=True,  # í† í°ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œì˜ ì‹œì‘/ë ì˜¤í”„ì…‹ ë°˜í™˜\n",
        "                truncation=True,  # max_lengthë¥¼ ì´ˆê³¼í•˜ëŠ” ì‹œí€€ìŠ¤ëŠ” ì˜ë¼ëƒ„\n",
        "                max_length=512,  # BERT ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ (ì¼ë°˜ì ìœ¼ë¡œ 512)\n",
        "                padding=\"max_length\",  # ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ max_lengthì— ë§ì¶° íŒ¨ë”©\n",
        "            )\n",
        "\n",
        "            input_ids = tokenized_output[\"input_ids\"]\n",
        "            offsets = tokenized_output[\"offset_mapping\"]\n",
        "\n",
        "            # ê° í† í°ì— í•´ë‹¹í•˜ëŠ” BIO ë ˆì´ë¸”ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
        "            # -100ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œë  íŠ¹ìˆ˜ í† í°(CLS, SEP ë“±)ì´ë‚˜ íŒ¨ë”© í† í°ì— í• ë‹¹ë©ë‹ˆë‹¤.\n",
        "            labels = [-100] * len(input_ids)\n",
        "\n",
        "            word_ids = tokenized_output.word_ids(\n",
        "                batch_index=0\n",
        "            )  # í† í°ì´ ì–´ë–¤ ì›ë³¸ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ”ì§€ ID ë§¤í•‘\n",
        "\n",
        "            # í† í°ë³„ë¡œ ë ˆì´ë¸” í• ë‹¹ (Doccanoì˜ ìŠ¤íŒ¬ ê¸°ë°˜ ë ˆì´ë¸”ì„ í† í° ê¸°ë°˜ BIO ë ˆì´ë¸”ë¡œ ë³€í™˜)\n",
        "            for token_idx, word_idx in enumerate(word_ids):\n",
        "                if word_idx is None:  # CLS, SEP, íŒ¨ë”© í† í°ê³¼ ê°™ì€ íŠ¹ìˆ˜ í† í°\n",
        "                    labels[token_idx] = -100  # ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œ\n",
        "                else:  # ì¼ë°˜ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” í† í°\n",
        "                    # í˜„ì¬ í† í°ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œì˜ ì‹œì‘/ë ì˜¤í”„ì…‹\n",
        "                    token_start_offset = offsets[token_idx][0]\n",
        "                    token_end_offset = offsets[token_idx][1]\n",
        "\n",
        "                    current_token_label_name = (\n",
        "                        \"O\"  # í˜„ì¬ í† í°ì˜ ê¸°ë³¸ ë ˆì´ë¸”ì€ \"O\" (Other)\n",
        "                    )\n",
        "\n",
        "                    # í˜„ì¬ í† í°ì´ ì–´ë–¤ ì–´ë…¸í…Œì´ì…˜ì— ì†í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "                    for ann_start, ann_end, ann_label_name in annotations:\n",
        "                        # í† í°ì˜ ì˜¤í”„ì…‹ì´ ì–´ë…¸í…Œì´ì…˜ ë²”ìœ„ ë‚´ì— ì™„ì „íˆ í¬í•¨ë˜ëŠ” ê²½ìš°\n",
        "                        if (\n",
        "                            ann_start <= token_start_offset\n",
        "                            and token_end_offset <= ann_end\n",
        "                        ):\n",
        "                            # ë§Œì•½ í˜„ì¬ í† í°ì˜ ì‹œì‘ ì˜¤í”„ì…‹ì´ ì–´ë…¸í…Œì´ì…˜ì˜ ì‹œì‘ ì˜¤í”„ì…‹ê³¼ ê°™ë‹¤ë©´ B- íƒœê·¸\n",
        "                            if ann_start == token_start_offset:\n",
        "                                current_token_label_name = f\"B-{ann_label_name}\"\n",
        "                            # ì•„ë‹ˆë¼ë©´ I- íƒœê·¸\n",
        "                            else:\n",
        "                                current_token_label_name = f\"I-{ann_label_name}\"\n",
        "                            break  # í•´ë‹¹ ì–´ë…¸í…Œì´ì…˜ì„ ì°¾ì•˜ìœ¼ë‹ˆ ë” ì´ìƒ ê²€ìƒ‰í•  í•„ìš” ì—†ìŒ\n",
        "\n",
        "                    labels[token_idx] = label_to_id[current_token_label_name]\n",
        "\n",
        "            # ì‹¤ì œ ëª¨ë¸ ì…ë ¥ì— í•„ìš”í•œ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "            converted_data_for_hf.append(\n",
        "                {\n",
        "                    \"input_ids\": input_ids,\n",
        "                    \"attention_mask\": tokenized_output[\"attention_mask\"],\n",
        "                    \"labels\": labels,  # ì´ labelsëŠ” IDë¡œ ë³€í™˜ëœ BIO íƒœê·¸ ë¦¬ìŠ¤íŠ¸\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Python ë¦¬ìŠ¤íŠ¸ë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜ í›„ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
        "    processed_df = pd.DataFrame(converted_data_for_hf)\n",
        "    # íŠ¹ì§•(features)ì„ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜í•˜ì—¬ Datasetì´ ì˜¬ë°”ë¥¸ ë°ì´í„° íƒ€ì…ì„ ê°–ë„ë¡ í•©ë‹ˆë‹¤.\n",
        "    # ì´ëŠ” íŠ¹íˆ ClassLabelê³¼ ê°™ì€ íŠ¹ì • íƒ€ì…ì˜ ë°ì´í„°ì— ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "    features = Features(\n",
        "        {\n",
        "            \"input_ids\": Sequence(Value(\"int32\")),\n",
        "            \"attention_mask\": Sequence(Value(\"int32\")),\n",
        "            \"labels\": Sequence(\n",
        "                ClassLabel(names=label_list)\n",
        "            ),  # labelsëŠ” ClassLabel ì‹œí€€ìŠ¤\n",
        "        }\n",
        "    )\n",
        "    hf_dataset = Dataset.from_pandas(processed_df, features=features)\n",
        "\n",
        "    print(f\"\\nHugging Face Datasetìœ¼ë¡œ ë³€í™˜ëœ ìƒ˜í”Œ ìˆ˜: {len(hf_dataset)}\")\n",
        "    print(\"\\në³€í™˜ëœ Hugging Face Dataset ì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
        "    print(hf_dataset[0])\n",
        "    print(\n",
        "        f\"ë””ì½”ë”©ëœ í† í°: {tokenizer.convert_ids_to_tokens(hf_dataset[0]['input_ids'])}\"\n",
        "    )\n",
        "    decoded_labels = [\n",
        "        id_to_label[l_id] if l_id != -100 else \"O\" for l_id in hf_dataset[0][\"labels\"]\n",
        "    ]\n",
        "    print(f\"ë””ì½”ë”©ëœ ë ˆì´ë¸”: {decoded_labels}\")\n",
        "    # ì›ë³¸ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
        "    print(\n",
        "        f\"ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: {tokenizer.decode(hf_dataset[0]['input_ids'], skip_special_tokens=True)}\"\n",
        "    )\n",
        "\n",
        "    # --- 6. ë°ì´í„°ì…‹ ë¶„í•  (Train/Validation Split) ---\n",
        "    # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì „ì²´ ë°ì´í„°ì…‹ì„ í•™ìŠµ(train) ì„¸íŠ¸ì™€ í‰ê°€(validation) ì„¸íŠ¸ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
        "    # test_size=0.2ëŠ” ì „ì²´ ë°ì´í„°ì˜ 20%ë¥¼ í‰ê°€ ì„¸íŠ¸ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
        "    # seedëŠ” ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "    train_test_split_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = train_test_split_dataset[\"train\"]\n",
        "    eval_dataset = train_test_split_dataset[\"test\"]\n",
        "\n",
        "    print(f\"\\ní•™ìŠµ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}\")\n",
        "    print(f\"í‰ê°€ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(eval_dataset)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\n",
        "        f\"ì˜¤ë¥˜: ë ˆì´ë¸”ë§ëœ íŒŒì¼ '{labeled_data_file_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Google Driveì— ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"ë ˆì´ë¸”ë§ëœ íŒŒì¼ì„ ë¡œë“œí•˜ê±°ë‚˜ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4vDd7dBAe-l"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzrQKZ-PAe-l"
      },
      "source": [
        "## 4. BERT ê¸°ë°˜ NER ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "ì´ì œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ `klue/bert-base` ëª¨ë¸ì„ ê°œì²´ëª… ì¸ì‹ ì‘ì—…ì— ë§ê²Œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤. Hugging Face `Trainer` APIë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ê³¼ì •ì„ ë§¤ìš° í¸ë¦¬í•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "**âš ï¸ ì¤‘ìš”: Colab ëŸ°íƒ€ì„ì´ ëŠì–´ì¡Œë‹¤ë©´, ë°˜ë“œì‹œ `0.2 Google Drive ë§ˆìš´íŠ¸`, `0.3 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •`, `0.4 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜` ì…€ê³¼ í•¨ê»˜ `3. Colabì—ì„œ ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬` ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•œ í›„ ì´ ë‹¨ê³„ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRBb7iNCAe-l"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. ëª¨ë¸ ë¡œë“œ\n",
        "# AutoModelForTokenClassificationì€ í† í° ë¶„ë¥˜(NERê³¼ ê°™ì€ ì‘ì—…)ë¥¼ ìœ„í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "# num_labelsëŠ” ë‹¹ì‹ ì˜ label_listì— ìˆëŠ” ìµœì¢… ë ˆì´ë¸”(BIO íƒœê·¸ í¬í•¨)ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤.\n",
        "# id_to_labelê³¼ label2idëŠ” ëª¨ë¸ì´ ìˆ«ì IDì™€ ë ˆì´ë¸” ì´ë¦„ì„ ë§¤í•‘í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"klue/bert-base\",\n",
        "    num_labels=num_labels,  # ìš°ë¦¬ì˜ ìµœì¢… BIO ë ˆì´ë¸” ê°œìˆ˜\n",
        "    id2label=id_to_label,  # IDë¥¼ ë ˆì´ë¸” ì´ë¦„ìœ¼ë¡œ ë§¤í•‘\n",
        "    label2id=label_to_id,  # ë ˆì´ë¸” ì´ë¦„ì„ IDë¡œ ë§¤í•‘\n",
        ")\n",
        "\n",
        "print(\"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.\")\n",
        "print(\n",
        "    \"\"\"\n",
        "ì£¼ì˜: 'Some weights of BertForTokenClassification were not initialized...' ë©”ì‹œì§€ëŠ” ì •ìƒì…ë‹ˆë‹¤.\n",
        "ì´ëŠ” ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ì— í† í° ë¶„ë¥˜ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë ˆì´ì–´(classifier.bias, classifier.weight)ê°€ ì¶”ê°€ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "ì´ ë ˆì´ì–´ëŠ” ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ, ì´ì œ ë‹¹ì‹ ì˜ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_kpBta0Ae-m"
      },
      "source": [
        "### 4.2 ì„±ëŠ¥ ì§€í‘œ(Metrics) ì •ì˜\n",
        "\n",
        "ëª¨ë¸ í•™ìŠµ ì¤‘ ë˜ëŠ” í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. NERì—ì„œëŠ” ì£¼ë¡œ **ì •í™•ë„(accuracy), ì •ë°€ë„(precision), ì¬í˜„ìœ¨(recall), F1-ì ìˆ˜(F1-score)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” `seqeval` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbi0ifa2Ae-m"
      },
      "outputs": [],
      "source": [
        "# í‰ê°€ì§€í‘œ ë¡œë“œ (Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” seqeval ì‚¬ìš©)\n",
        "# load_metric is deprecated, use evaluate.load instead\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    print(f\"DEBUG - Inside compute_metrics:\")\n",
        "    print(f\"DEBUG - predictions shape before argmax: {p[0].shape}\") # ë¡œì§“ í˜•íƒœ í™•ì¸\n",
        "    print(f\"DEBUG - labels shape: {labels.shape}\")\n",
        "    print(f\"DEBUG - Sample predictions (after argmax, before filtering): {predictions[0][:10]}\") # ì²« ë²ˆì§¸ ìƒ˜í”Œ 10ê°œ\n",
        "    print(f\"DEBUG - Sample labels (before filtering): {labels[0][:10]}\") # ì²« ë²ˆì§¸ ìƒ˜í”Œ 10ê°œ\n",
        "\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    print(f\"DEBUG - Sample true_predictions (decoded & filtered): {true_predictions[0][:10]}\") # ì²« ë²ˆì§¸ ìƒ˜í”Œ 10ê°œ\n",
        "    print(f\"DEBUG - Sample true_labels (decoded & filtered): {true_labels[0][:10]}\") # ì²« ë²ˆì§¸ ìƒ˜í”Œ 10ê°œ\n",
        "    print(f\"DEBUG - Length of true_predictions for first sample: {len(true_predictions[0])}\")\n",
        "    print(f\"DEBUG - Length of true_labels for first sample: {len(true_labels[0])}\")\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "print(\"ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2EzLz1-Ae-m"
      },
      "source": [
        "### 4.3 í•™ìŠµ ì¸ì(Training Arguments) ë° íŠ¸ë ˆì´ë„ˆ(Trainer) ì„¤ì •\n",
        "\n",
        "ëª¨ë¸ì„ ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¬ì§€ì— ëŒ€í•œ ì„¤ì •(í•˜ì´í¼íŒŒë¼ë¯¸í„°)ê³¼, Hugging Face `Trainer` ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "**âš ï¸ ì˜¤ë¥˜ í•´ê²° (ì´ì „ ëŒ€í™”ì—ì„œ ë°œìƒí–ˆë˜ ë¬¸ì œ):**\n",
        "\n",
        "- `evaluation_strategy`ê°€ `eval_strategy`ë¡œ ì´ë¦„ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì‹  ë²„ì „ì— ë§ì¶° ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
        "- `model_dir`ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ì˜¤ë¥˜ëŠ” `0.3 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •` ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ í•´ê²°ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCF2Co_MAe-m"
      },
      "outputs": [],
      "source": [
        "# í•™ìŠµ ì¸ì ì„¤ì •\n",
        "# output_dir: í•™ìŠµëœ ëª¨ë¸ê³¼ ë¡œê·¸ê°€ ì €ì¥ë  ê²½ë¡œ\n",
        "# eval_strategy: 'epoch'ë¡œ ì„¤ì •í•˜ì—¬ ê° ì—í¬í¬ ì¢…ë£Œ ì‹œë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰\n",
        "# learning_rate: í•™ìŠµë¥  (ì´ˆê¸°ê°’ 2e-5ê°€ ì¼ë°˜ì ì…ë‹ˆë‹¤.)\n",
        "# per_device_train_batch_size: GPUë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥)\n",
        "# per_device_eval_batch_size: GPUë‹¹ í‰ê°€ ë°°ì¹˜ í¬ê¸°\n",
        "# num_train_epochs: ì „ì²´ í•™ìŠµ ì—í¬í¬ ìˆ˜ (ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µ í•™ìŠµí• ì§€, ë°ì´í„°ê°€ ì ìœ¼ë©´ ê³¼ì í•© ì£¼ì˜)\n",
        "# weight_decay: ê°€ì¤‘ì¹˜ ê°ì‡  (ê³¼ì í•© ë°©ì§€ ê¸°ë²•)\n",
        "# push_to_hub: í•™ìŠµëœ ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œí• ì§€ ì—¬ë¶€ (ì§€ê¸ˆì€ False)\n",
        "# logging_dir: í•™ìŠµ ë¡œê·¸ ì €ì¥ ê²½ë¡œ\n",
        "# logging_steps: ëª‡ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ë¥¼ ì¶œë ¥í• ì§€ (ë°ì´í„°ì…‹ì´ ì‘ìœ¼ë©´ ë¡œê·¸ê°€ ìì£¼ ì•ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_dir,  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ (0.3ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ model_dir)\n",
        "    eval_strategy=\"epoch\",  # <-- 'evaluation_strategy'ê°€ 'eval_strategy'ë¡œ ë³€ê²½ë¨!\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,  # GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ 8, 4 ë“±ìœ¼ë¡œ ì¤„ì—¬ë³´ì„¸ìš”.\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,  # ì´ˆë°˜ì—ëŠ” ì ì€ ì—í¬í¬ë¡œ ì‹œì‘í•˜ê³ , ë°ì´í„°ê°€ ë§ì•„ì§€ë©´ ëŠ˜ë ¤ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=os.path.join(model_dir, \"logs\"),\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",  # Colab í™˜ê²½ì—ì„œ ë¶ˆí•„ìš”í•œ ê²½ê³  ë°©ì§€ (wandb ë“± ì„¤ì • ì‹œ ë³€ê²½)\n",
        ")\n",
        "\n",
        "# Trainer ê°ì²´ ìƒì„±\n",
        "# TrainerëŠ” ëª¨ë¸, í•™ìŠµ ì¸ì, ë°ì´í„°ì…‹, í† í¬ë‚˜ì´ì €, ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ë¥¼ ì¸ìë¡œ ë°›ì•„ í•™ìŠµì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # í•™ìŠµ ë°ì´í„°ì…‹\n",
        "    eval_dataset=eval_dataset,  # í‰ê°€ ë°ì´í„°ì…‹\n",
        "    tokenizer=tokenizer,  # í† í¬ë‚˜ì´ì € (FutureWarningì´ ë°œìƒí•  ìˆ˜ ìˆì§€ë§Œ, í˜„ì¬ëŠ” ì •ìƒ ì‘ë™)\n",
        "    compute_metrics=compute_metrics,  # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜\n",
        ")\n",
        "\n",
        "print(\"í•™ìŠµ ì¸ì ë° Trainer ì„¤ì • ì™„ë£Œ.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxdP31EAe-m"
      },
      "source": [
        "### 4.4 ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
        "\n",
        "ì´ì œ `trainer.train()` ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ê³¼ í‰ê°€ ê²°ê³¼ëŠ” ë¡œê·¸ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJSRc2JWAe-m"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uww8HA2iAe-m"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxcOtB7u0Fyj"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        max_length=512 # Doccanoì—ì„œ ì„¤ì •í•œ max_lengthì™€ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "\n",
        "        # --- ë””ë²„ê¹…ìš© print ë¬¸ ì¶”ê°€ ---\n",
        "        print(f\"\\n--- ìƒ˜í”Œ {i} ---\")\n",
        "        print(f\"ì›ë³¸ words: {examples['tokens'][i]}\")\n",
        "        print(f\"ì›ë³¸ ner_tags (word-level): {label}\")\n",
        "        print(f\"í† í°í™”ëœ input_ids (ì¼ë¶€): {tokenized_inputs['input_ids'][i][:10]}...\")\n",
        "        print(f\"word_ids: {word_ids}\")\n",
        "        # --- ë””ë²„ê¹…ìš© print ë¬¸ ë ---\n",
        "\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # íŠ¹ìˆ˜ í† í° (-100)\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # ì²« ë²ˆì§¸ ì„œë¸Œì›Œë“œ í† í° (ë‹¨ì–´ì˜ ì‹œì‘)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # í›„ì† ì„œë¸Œì›Œë“œ í† í° (ë‹¨ì–´ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„)\n",
        "            else:\n",
        "                # B- íƒœê·¸ë¥¼ I- íƒœê·¸ë¡œ ë³€ê²½\n",
        "                # ì›ë˜ ë¼ë²¨ì´ B- íƒœê·¸ì¸ ê²½ìš° í•´ë‹¹ I- íƒœê·¸ IDë¡œ ë³€ê²½í•©ë‹ˆë‹¤.\n",
        "                # ì˜ˆë¥¼ ë“¤ì–´, B-QUESTION_ID (ID 1) -> I-QUESTION_ID (ID 2)\n",
        "                # ì´ ë¶€ë¶„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤: id_to_labelê³¼ label_to_idë¥¼ ì •í™•íˆ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "                current_label = id_to_label[label[word_idx]]\n",
        "                if current_label.startswith(\"B-\"):\n",
        "                    # 'B-'ë¥¼ 'I-'ë¡œ ë°”ê¾¸ê³  í•´ë‹¹ IDë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
        "                    # ì´ ë¡œì§ì´ ì •í™•í•œì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "                    # ì˜ˆ: B-QUESTION_ID -> I-QUESTION_ID (ID 1 -> ID 2)\n",
        "                    i_tag_label = \"I-\" + current_label[2:]\n",
        "                    if i_tag_label in label_to_id: # label_to_id ë”•ì…”ë„ˆë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "                        label_ids.append(label_to_id[i_tag_label])\n",
        "                    else:\n",
        "                        # í•´ë‹¹ I- íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° O íƒœê·¸ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜, ê·¸ëŒ€ë¡œ B- íƒœê·¸ ìœ ì§€ (ê³ ë ¤ í•„ìš”)\n",
        "                        label_ids.append(label[word_idx]) # í˜¹ì€ 0ìœ¼ë¡œ? -> ì˜¤ë¥˜ì˜ ì›ì¸ì¼ ìˆ˜ ìˆìŒ\n",
        "                else:\n",
        "                    label_ids.append(label[word_idx]) # B- íƒœê·¸ê°€ ì•„ë‹Œ ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "        # --- ë””ë²„ê¹…ìš© print ë¬¸ ì¶”ê°€ ---\n",
        "        # print(f\"aligned_labels (token-level): {label_ids}\")\n",
        "        # print(f\"ë””ì½”ë”©ëœ aligned_labels (ì¼ë¶€): {[id_to_label[lid] for lid in label_ids if lid != -100][:20]}\")\n",
        "        # --- ë””ë²„ê¹…ìš© print ë¬¸ ë ---\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axq3D2fYAe-n"
      },
      "source": [
        "## 5. í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ë° ì¶”ë¡  (í…ŒìŠ¤íŠ¸)\n",
        "\n",
        "í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ ì €ì¥í•˜ê³ , ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ê°œì²´ëª… ì¸ì‹ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤. í˜„ì¬ í•™ìŠµ ë°ì´í„°ì…‹ì˜ ì–‘ì´ ë§¤ìš° ì ìœ¼ë¯€ë¡œ, ì´ˆê¸° ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q23b0epDAe-n"
      },
      "source": [
        "### 5.1 í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
        "\n",
        "í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ ì„¤ì • íŒŒì¼ì´ `model_dir` ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì €ì¥ëœ ëª¨ë¸ì€ ë‚˜ì¤‘ì— ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myMA7qNVAe-n"
      },
      "outputs": [],
      "source": [
        "# í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
        "trainer.save_model(model_dir)  # model_dirì€ 0.3ë‹¨ê³„ì—ì„œ ì„¤ì •í–ˆë˜ ëª¨ë¸ ì €ì¥ ê²½ë¡œì…ë‹ˆë‹¤.\n",
        "\n",
        "print(f\"í•™ìŠµëœ ëª¨ë¸ì´ '{model_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU37rrNIAe-n"
      },
      "source": [
        "### 5.2 ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì¶”ë¡  ì¤€ë¹„)\n",
        "\n",
        "ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡(ì¶”ë¡ )ì„ ìˆ˜í–‰í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤. ëª¨ë¸ì„ CPU/GPUì— ë¡œë“œí•˜ê³  í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnTLpbw-Ae-n"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "loaded_model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
        "\n",
        "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (ë“œë¡­ì•„ì›ƒ ë“±ì„ ë¹„í™œì„±í™”í•˜ì—¬ ì¼ê´€ëœ ì˜ˆì¸¡ì„ ë³´ì¥)\n",
        "loaded_model.eval()\n",
        "\n",
        "# GPUê°€ ìˆë‹¤ë©´ GPUë¡œ ëª¨ë¸ ì´ë™\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model.to(device)\n",
        "\n",
        "print(f\"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ '{model_dir}'ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAyS6G1JAe-n"
      },
      "source": [
        "### 5.3 ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°œì²´ëª… ì¸ì‹ ì¶”ë¡  í•¨ìˆ˜\n",
        "\n",
        "ì„ì˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ëª¨ë¸ì´ ê°œì²´ëª…ì„ ì˜ˆì¸¡í•˜ê³ , BIO íƒœê·¸ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ 'ê°œì²´ëª… ìŠ¤íŒ¬' í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu2r0s19Ae-n"
      },
      "outputs": [],
      "source": [
        "def predict_ner(text, tokenizer, model, id_to_label, device):\n",
        "    # í…ìŠ¤íŠ¸ í† í°í™” (ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ íŒ¨ë”©, ì˜ë¦¼ ì ìš©)\n",
        "    tokenized_input = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",  # PyTorch í…ì„œ ë°˜í™˜\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    ).to(\n",
        "        device\n",
        "    )  # ëª¨ë¸ì´ ìˆëŠ” ë””ë°”ì´ìŠ¤(CPU ë˜ëŠ” GPU)ë¡œ ì…ë ¥ í…ì„œ ì´ë™\n",
        "\n",
        "    # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    with torch.no_grad():  # ì¶”ë¡  ì‹œì—ëŠ” ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ)\n",
        "        output = model(**tokenized_input)\n",
        "\n",
        "    # ì˜ˆì¸¡ëœ ë¡œì§“(logits)ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë ˆì´ë¸” ID ì¶”ì¶œ\n",
        "    # squeeze()ëŠ” ë°°ì¹˜ ì°¨ì›(í¬ê¸° 1)ì„ ì œê±°í•˜ê³ , cpu().numpy()ë¡œ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜\n",
        "    predictions = torch.argmax(output.logits, dim=2).squeeze().cpu().numpy()\n",
        "\n",
        "    # í† í° ë° ë ˆì´ë¸” ë””ì½”ë”©\n",
        "    tokens = tokenizer.convert_ids_to_tokens(\n",
        "        tokenized_input[\"input_ids\"].squeeze().cpu().numpy()\n",
        "    )\n",
        "    predicted_labels = [id_to_label[p_id] for p_id in predictions]\n",
        "\n",
        "    # íŠ¹ìˆ˜ í† í° ë° íŒ¨ë”© í† í° ì œê±° (ì‹¤ì œ ë‹¨ì–´ì— ëŒ€í•œ ì˜ˆì¸¡ë§Œ ë³´ê¸° ìœ„í•¨)\n",
        "    # ë˜í•œ B-ì™€ I- íƒœê·¸ë¥¼ ê²°í•©í•˜ì—¬ ê°œì²´ëª… ìŠ¤íŒ¬ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "    results = []\n",
        "    current_entity = \"\"\n",
        "    current_label = \"\"\n",
        "\n",
        "    # ì˜ˆì¸¡ëœ í† í°ê³¼ ë ˆì´ë¸”ì„ ìˆœíšŒí•˜ë©° ê°œì²´ëª… ì¶”ì¶œ\n",
        "    for token, label in zip(tokens, predicted_labels):\n",
        "        if token.startswith(\"##\"):  # WordPiece í† í¬ë‚˜ì´ì €ì˜ ì„œë¸Œì›Œë“œ ì ‘ë‘ì‚¬ ì œê±°\n",
        "            token = token[2:]\n",
        "\n",
        "        # CLS, SEP, PAD í† í° ë“± íŠ¹ìˆ˜ í† í° ì œì™¸\n",
        "        if token in tokenizer.all_special_tokens:  # CLS, SEP, PAD í† í°ì€ ê±´ë„ˆëœë‹ˆë‹¤.\n",
        "            if current_entity:  # ì´ì „ì— ìˆ˜ì§‘ ì¤‘ì´ë˜ ê°œì²´ëª…ì´ ìˆë‹¤ë©´ ë§ˆë¬´ë¦¬\n",
        "                results.append((current_entity.strip(), current_label))\n",
        "                current_entity = \"\"\n",
        "                current_label = \"\"\n",
        "            continue\n",
        "\n",
        "        if label.startswith(\"B-\"):  # ìƒˆë¡œìš´ ê°œì²´ëª…ì˜ ì‹œì‘ (Beginning)\n",
        "            if current_entity:  # ì´ì „ì— ìˆ˜ì§‘ ì¤‘ì´ë˜ ê°œì²´ëª…ì´ ìˆë‹¤ë©´ ë¨¼ì € ê²°ê³¼ì— ì¶”ê°€\n",
        "                results.append((current_entity.strip(), current_label))\n",
        "            current_entity = token  # ìƒˆë¡œìš´ ê°œì²´ëª… ì‹œì‘\n",
        "            current_label = label[2:]  # 'B-' ì ‘ë‘ì‚¬ ì œê±°í•˜ì—¬ ìˆœìˆ˜ ë ˆì´ë¸” ì´ë¦„ ì €ì¥\n",
        "        elif label.startswith(\"I-\") and current_label and label[2:] == current_label:\n",
        "            # í˜„ì¬ í† í°ì´ ì´ì „ ê°œì²´ëª…ì˜ ì—°ì† (Inside)ì´ê³ , ë ˆì´ë¸” ìœ í˜•ì´ ì¼ì¹˜í•  ê²½ìš°\n",
        "            current_entity += token  # í˜„ì¬ í† í°ì„ ê¸°ì¡´ ê°œì²´ëª…ì— ì¶”ê°€\n",
        "        else:  # 'O' íƒœê·¸ì´ê±°ë‚˜, 'I-' íƒœê·¸ì¸ë° ì´ì „ ë ˆì´ë¸”ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
        "            if current_entity:  # ì´ì „ì— ìˆ˜ì§‘ ì¤‘ì´ë˜ ê°œì²´ëª…ì´ ìˆë‹¤ë©´ ë§ˆë¬´ë¦¬\n",
        "                results.append((current_entity.strip(), current_label))\n",
        "            current_entity = \"\"  # í˜„ì¬ ê°œì²´ëª… ì´ˆê¸°í™”\n",
        "            current_label = \"\"  # í˜„ì¬ ë ˆì´ë¸” ì´ˆê¸°í™”\n",
        "\n",
        "    # ë°˜ë³µë¬¸ ì¢…ë£Œ í›„ ë§ˆì§€ë§‰ì— ë‚¨ì•„ìˆëŠ” ê°œì²´ëª…ì´ ìˆë‹¤ë©´ ì¶”ê°€\n",
        "    if current_entity:\n",
        "        results.append((current_entity.strip(), current_label))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"ê°œì²´ëª… ì¸ì‹ ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2u9qvTsAe-n"
      },
      "source": [
        "### 5.4 ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì˜ˆì‹œ ë¬¸ì¥)\n",
        "\n",
        "ì •ì˜í•œ ì¶”ë¡  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµëœ ëª¨ë¸ì´ ìƒˆë¡œìš´ ë¬¸ì¥ì—ì„œ ê°œì²´ëª…ì„ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b00YZ_cnOF8"
      },
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸í•  ì˜ˆì‹œ ë¬¸ì¥\n",
        "test_text_1 = \"\"\"\n",
        "ì§ˆì˜ 5 \"ì²¨ë¶€5\"ê³¼ ê°™ì´ ê°ì§€ê¸° ë°°ì„ ì„ ì‹œê³µí•˜ì—¬ë„ ë¬¸ì œê°€ ì—†ëŠ”ì§€ ì—¬ë¶€. íšŒì‹  5 ã€Œìë™í™”ì¬íƒì§€ì„¤ë¹„ ë° ì‹œê°ê²½ë³´ê¸°ì˜ í™”ì¬ì•ˆì „ê¸°ì¤€(NFSC 203)ã€ ì œ11ì¡°ì œ2í˜¸ ë‚˜ëª©ì— ë”°ë¼ ê°ì§€ê¸° ìƒí˜¸ê°„ ë˜ëŠ” ê°ì§€ê¸°ë¡œë¶€í„° ìˆ˜ì‹ ê¸°ì— ì´ë¥´ëŠ” ê°ì§€ê¸° íšŒë¡œì˜ ë°°ì„ ì€ ã€Œì˜¥ë‚´ì†Œí™”ì „ì„¤ë¹„ì˜ í™”ì¬ì•ˆì „ê¸°ì¤€(NFSC 102)ã€ [ë³„í‘œ 1]ì— ë”°ë¥¸ ë‚´í™”ë°°ì„  ë˜ëŠ” ë‚´ì—´ë°°ì„ ì„ ì‚¬ìš©í•˜ë„ë¡ ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·€í•˜ê»˜ì„œ â€ì²¨ë¶€íŒŒì¼5â€œë¡œ ë¬¸ì˜í•˜ì‹  ê°ì§€ê¸° ë°°ì„  ì‹œê³µë°©ë²•ì€ HFIX ì „ì„ ì„ ê¸ˆì†ì œ ê°€ìš”ì „ì„ ê´€ì— ìˆ˜ë‚©í•˜ì—¬ ì‹œê³µí•œ ë¶€ë¶„ì€ ë‚´ì—´ ë˜ëŠ” ë‚´í™”ë°°ì„  ì‹œê³µë²•ì— ì í•©í•˜ë‚˜, ë‹¨ì—´ì¬ì—ì„œ ì²œì¥ë©´ ë‚œì—°CDì „ì „ê´€ì— ìˆ˜ë‚©í•œ ì‹œê³µë°©ë²•ì€ í™”ì¬ì•ˆì „ ê¸°ì¤€ì—ì„œ ê·œì •í•˜ê³  ìˆëŠ” ë‚´ì—´ã†ë‚´í™”ë°°ì„ ì— í•´ë‹¹í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹µë³€1ì„ ì°¸ê³ í•˜ì—¬ ë‚´í™”ë°°ì„ ì— ë”°ë¥¸ ì‹œê³µì„ í•˜ê±°ë‚˜ ë‚´ì—´ë°°ì„ ì— ë”°ë¥¸ì‹œê³µìœ¼ë¡œ ë°°ì„ ì²˜ë¦¬ë¥¼ í•˜ì—¬ì•¼ í•  ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ì¶”ë¡  í•¨ìˆ˜ ì‹¤í–‰\n",
        "predicted_entities_1 = predict_ner(\n",
        "    test_text_1, loaded_tokenizer, loaded_model, id_to_label, device\n",
        ")\n",
        "\n",
        "print(f\"\\nì…ë ¥ í…ìŠ¤íŠ¸: {test_text_1}\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ê°œì²´ëª…: {predicted_entities_1}\")\n",
        "\n",
        "# ë‹¤ë¥¸ ì˜ˆì‹œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfoXZKYoN6_N"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# í•œêµ­ì–´ -> ì˜ì–´ ë²ˆì—­ ëª¨ë¸ ë¡œë“œ (ì´ì „ê³¼ ë™ì¼)\n",
        "ko_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
        "ko_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\").to(device)\n",
        "\n",
        "# ì˜ì–´ -> í•œêµ­ì–´ ë²ˆì—­ ëª¨ë¸ ë¡œë“œ (ëª¨ë¸ ID ìˆ˜ì •)\n",
        "# 'Helsinki-NLP/opus-mt-en-ko' ëŒ€ì‹  'Helsinki-NLP/opus-mt-tc-big-en-ko' ì‚¬ìš©\n",
        "en_ko_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
        "en_ko_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\").to(device)\n",
        "\n",
        "print(\"ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì™„ë£Œ.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf2RDvLsQ1HW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- 1. Doccanoì—ì„œ ì •ì˜í•œ ë ˆì´ë¸” ëª©ë¡ (B-, I- ì—†ì´) ---\n",
        "doccano_raw_labels = [\n",
        "    \"QUESTION_ID\",\n",
        "    \"QUESTION_CONTENT\",\n",
        "    \"ANSWER_ID\",\n",
        "    \"ANSWER_CONTENT\",\n",
        "    \"LAW_CONTENT\",\n",
        "]\n",
        "\n",
        "# --- 2. ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ìµœì¢… BIO ë ˆì´ë¸” ëª©ë¡ ë° ë§¤í•‘ ---\n",
        "label_list = [\"O\"]\n",
        "for label in doccano_raw_labels:\n",
        "    label_list.append(f\"B-{label}\")\n",
        "    label_list.append(f\"I-{label}\")\n",
        "\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(f\"ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ìµœì¢… ë ˆì´ë¸” ëª©ë¡: {label_list}\")\n",
        "print(f\"ì´ ë ˆì´ë¸” ê°œìˆ˜: {num_labels}\")\n",
        "print(f\"label_to_id: {label_to_id}\")\n",
        "print(f\"id_to_label: {id_to_label}\")\n",
        "\n",
        "# --- 3. Doccanoì—ì„œ ë‚´ë³´ë‚¸ JSONL íŒŒì¼ ê²½ë¡œ ì„¤ì • ---\n",
        "project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning-ner-advanced/\"\n",
        "data_dir = os.path.join(project_root, \"data\")\n",
        "labeled_data_file_path = os.path.join(data_dir, \"after_datalabeling.jsonl\")\n",
        "\n",
        "# --- 4. ì‚¬ìš©í•  í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ë²ˆì—­ ëª¨ë¸ ë¡œë“œ (ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì •í•œ ëª¨ë¸ ID ì‚¬ìš©)\n",
        "ko_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
        "ko_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\").to(device)\n",
        "en_ko_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\") # ìˆ˜ì •ëœ ëª¨ë¸ ID\n",
        "en_ko_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\").to(device)\n",
        "print(\"ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì™„ë£Œ.\")\n",
        "\n",
        "\n",
        "# --- 5. ë°± íŠ¸ëœìŠ¬ë ˆì´ì…˜ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---\n",
        "def back_translate(text, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device):\n",
        "    # í•œêµ­ì–´ -> ì˜ì–´\n",
        "    tokenized_text = ko_en_tokenizer.prepare_seq2seq_batch([text], return_tensors='pt').to(device)\n",
        "    translated_tokens = ko_en_model.generate(**tokenized_text)\n",
        "    english_text = ko_en_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "    # ì˜ì–´ -> í•œêµ­ì–´\n",
        "    tokenized_text = en_ko_tokenizer.prepare_seq2seq_batch([english_text], return_tensors='pt').to(device)\n",
        "    translated_tokens = en_ko_model.generate(**tokenized_text)\n",
        "    korean_text_augmented = en_ko_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "    return korean_text_augmented\n",
        "\n",
        "# --- 6. NER ë°ì´í„°ë¥¼ ì¦ê°•í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ (ìˆ˜ì •ë¨: ë¼ë²¨ ì´ë¦„ì„ IDë¡œ ë³€í™˜) ---\n",
        "def augment_ner_example(example, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device):\n",
        "    text = example['text']\n",
        "    # Ensure labels are sorted by start index\n",
        "    labels = sorted(example.get('labels', []), key=lambda x: x[0])\n",
        "\n",
        "    augmented_text_parts = []\n",
        "    # new_labels will store [start, end, LABEL_ID]\n",
        "    new_labels = []\n",
        "    last_idx = 0\n",
        "    current_offset = 0\n",
        "\n",
        "    for start, end, label_name in labels:\n",
        "        context_part = text[last_idx:start]\n",
        "        if context_part:\n",
        "            augmented_context = back_translate(context_part, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device)\n",
        "            augmented_text_parts.append(augmented_context)\n",
        "            current_offset += len(augmented_context)\n",
        "\n",
        "        entity_part = text[start:end]\n",
        "        augmented_text_parts.append(entity_part)\n",
        "\n",
        "        new_start = current_offset\n",
        "        new_end = current_offset + len(entity_part)\n",
        "\n",
        "        # Convert label_name string to its corresponding ID\n",
        "        # Need to handle cases where the label_name might not be in label_to_id (shouldn't happen with valid Doccano export)\n",
        "        label_id = label_to_id.get(f\"B-{label_name}\", 0) # Use B- tag ID for span start, default to 0 ('O') if not found\n",
        "\n",
        "        new_labels.append([new_start, new_end, label_id]) # Store as [start, end, LABEL_ID]\n",
        "\n",
        "        current_offset += len(entity_part)\n",
        "        last_idx = end\n",
        "\n",
        "    final_context_part = text[last_idx:]\n",
        "    if final_context_part:\n",
        "        augmented_context = back_translate(final_context_part, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device)\n",
        "        augmented_text_parts.append(augmented_context)\n",
        "\n",
        "    new_text = \"\".join(augmented_text_parts)\n",
        "    return {\"text\": new_text, \"labels\": new_labels} # 'labels' now stores [start, end, LABEL_ID]\n",
        "\n",
        "\n",
        "# --- 7. Doccano JSONL ë¡œë“œ ë° ë°ì´í„° ì¦ê°• ---\n",
        "original_data_raw = []\n",
        "try:\n",
        "    with open(labeled_data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            original_data_raw.append(json.loads(line))\n",
        "except FileNotFoundError:\n",
        "    print(f\"ì˜¤ë¥˜: ë ˆì´ë¸”ë§ëœ íŒŒì¼ '{labeled_data_file_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    # Exit gracefully or handle this case appropriately if not running in interactive mode\n",
        "    # In Colab, just printing the error is often sufficient for interactive debugging\n",
        "    pass\n",
        "\n",
        "\n",
        "# Convert original data labels from strings to IDs immediately\n",
        "original_data_processed = []\n",
        "for example in original_data_raw:\n",
        "    processed_labels = []\n",
        "    # Ensure labels exist and are sorted\n",
        "    labels = sorted(example.get('labels', []), key=lambda x: x[0])\n",
        "    for start, end, label_name in labels:\n",
        "         # Convert label_name string to its corresponding ID (using B- tag ID for span start)\n",
        "        label_id = label_to_id.get(f\"B-{label_name}\", 0) # Default to 0 ('O') if not found\n",
        "        processed_labels.append([start, end, label_id]) # Store as [start, end, LABEL_ID]\n",
        "    original_data_processed.append({\"text\": example['text'], \"labels\": processed_labels})\n",
        "\n",
        "\n",
        "N_augment = 5 # Number of samples to augment\n",
        "\n",
        "augmented_examples = []\n",
        "if original_data_processed: # Only augment if original data was loaded successfully\n",
        "    print(f\"ì´ {len(original_data_processed)}ê°œì˜ ìƒ˜í”Œ ì¤‘ {N_augment}ê°œì— ëŒ€í•´ ë°ì´í„° ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "    # Data augmentation execution (returns text and [start, end, LABEL_ID] labels)\n",
        "    for i in tqdm(range(min(N_augment, len(original_data_processed)))):\n",
        "        augmented_example = augment_ner_example(original_data_processed[i], ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device)\n",
        "        augmented_examples.append(augmented_example)\n",
        "\n",
        "    print(\"\\n--- ë°ì´í„° ì¦ê°• ì˜ˆì‹œ ---\")\n",
        "    for i in range(min(5, len(augmented_examples))): # Print up to 5 augmented samples\n",
        "        print(f\"\\n[ìƒ˜í”Œ {i+1}]\")\n",
        "        print(\"ì›ë³¸ í…ìŠ¤íŠ¸:\", original_data_processed[i]['text'])\n",
        "        print(\"ì¦ê°• í…ìŠ¤íŠ¸:\", augmented_examples[i]['text'])\n",
        "        # Decode label IDs back to names for printing clarity\n",
        "        decoded_original_labels = [[s, e, id_to_label[l_id]] for s, e, l_id in original_data_processed[i]['labels']]\n",
        "        decoded_augmented_labels = [[s, e, id_to_label[l_id]] for s, e, l_id in augmented_examples[i]['labels']]\n",
        "        print(\"ì›ë³¸ ë¼ë²¨ (ë””ì½”ë”©):\", decoded_original_labels)\n",
        "        print(\"ì¦ê°• ë¼ë²¨ (ë””ì½”ë”©):\", decoded_augmented_labels)\n",
        "else:\n",
        "    print(\"ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•˜ì—¬ ë°ì´í„° ì¦ê°•ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "\n",
        "\n",
        "# --- 8. ì¦ê°•ëœ ë°ì´í„°ì™€ ì›ë³¸ ë°ì´í„°ë¥¼ í•©ì¹˜ê³  Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ìµœì¢… ë³€í™˜ ---\n",
        "\n",
        "# Combine original (processed with IDs) and augmented data\n",
        "combined_raw_data_processed = original_data_processed + augmented_examples\n",
        "\n",
        "# Step 8.1: Create initial Hugging Face Dataset from the processed list\n",
        "# 'labels' column now contains lists of [start, end, LABEL_ID]\n",
        "# Define the features here to ensure 'labels' is treated correctly as a sequence of integer IDs.\n",
        "# We need a structure that can handle lists of lists of integers.\n",
        "# Define the features for the dataset before converting from list\n",
        "dataset_features = Features({\n",
        "    'text': Value('string'),\n",
        "    'labels': Sequence(Sequence(Value('int32'))) # Use int32 for label IDs\n",
        "})\n",
        "\n",
        "\n",
        "# Create the dataset using the defined features\n",
        "# Dataset.from_list can infer features, but explicitly defining helps avoid errors\n",
        "# and ensures the structure for 'labels' is correctly interpreted.\n",
        "try:\n",
        "    raw_hf_dataset = Dataset.from_list(combined_raw_data_processed, features=dataset_features)\n",
        "    print(f\"\\nì„ì‹œ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜ëœ ìƒ˜í”Œ ìˆ˜ (í† í°í™” ì „): {len(raw_hf_dataset)}\")\n",
        "    print(raw_hf_dataset[0]) # Check the structure and data types\n",
        "except Exception as e:\n",
        "    print(f\"ì˜¤ë¥˜: ì„ì‹œ Hugging Face Dataset ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    # Exit or handle error\n",
        "    pass\n",
        "\n",
        "\n",
        "# Step 8.2: Tokenization and label alignment function (Dataset.map() will use this)\n",
        "def tokenize_and_align_labels_for_dataset(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512, # Use the same max_length as defined earlier\n",
        "        padding=\"max_length\",\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    # examples[\"text\"] is a list, examples[\"labels\"] is a list of lists of [start, end, LABEL_ID] lists\n",
        "    for batch_idx, (text, ner_tags_char_offsets_with_ids) in enumerate(zip(examples[\"text\"], examples[\"labels\"])):\n",
        "        # ner_tags_char_offsets_with_ids is [[start, end, LABEL_ID], ...]\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=batch_idx)\n",
        "        offset_mapping = tokenized_inputs[\"offset_mapping\"][batch_idx]\n",
        "\n",
        "        token_labels_ids = [-100] * len(word_ids) # Initialize token-level label ID list\n",
        "\n",
        "        # Assign BIO labels for each token\n",
        "        # ner_tags_char_offsets_with_ids is assumed to be sorted by start index\n",
        "        for token_idx, current_word_idx in enumerate(word_ids):\n",
        "            if current_word_idx is None: # Special tokens (CLS, SEP, PAD)\n",
        "                token_labels_ids[token_idx] = -100\n",
        "            else: # Regular word tokens\n",
        "                token_start_offset, token_end_offset = offset_mapping[token_idx]\n",
        "                current_token_label_id = 0 # Default label is 'O' (ID 0)\n",
        "\n",
        "                # Check which annotation (entity) the current token falls into\n",
        "                for ann_start, ann_end, ann_label_id in ner_tags_char_offsets_with_ids:\n",
        "                    # If the token's offset is fully within the annotation's range\n",
        "                    if ann_start <= token_start_offset and token_end_offset <= ann_end:\n",
        "                         # If the token's start offset is the same as the annotation's start offset, it's the Beginning token (B-)\n",
        "                        if ann_start == token_start_offset:\n",
        "                             # Get the B- tag ID for this label_id\n",
        "                            # Check if the B- tag exists for this ID (should always if ID is valid)\n",
        "                            b_tag_label = id_to_label[ann_label_id] # Get label name from ID\n",
        "                            if b_tag_label.startswith(\"B-\"): # Ensure it's already a B- tag (from our processing)\n",
        "                                current_token_label_id = ann_label_id # Keep the B- tag ID\n",
        "                            else: # Should not happen if previous step is correct, default to O\n",
        "                                current_token_label_id = 0\n",
        "                        else: # Otherwise, it's an Inside token (I-)\n",
        "                            # Get the I- tag ID for this label_id\n",
        "                            # The original label_id stored in ner_tags_char_offsets_with_ids was the B- tag ID.\n",
        "                            # We need to find the corresponding I- tag ID.\n",
        "                            b_tag_label_name = id_to_label[ann_label_id][2:] # Get the pure label name (e.g., 'QUESTION_ID')\n",
        "                            i_tag_label = f\"I-{b_tag_label_name}\" # Construct the I- tag name (e.g., 'I-QUESTION_ID')\n",
        "                            current_token_label_id = label_to_id.get(i_tag_label, 0) # Get the I- tag ID, default to O if not found\n",
        "                        break # Found the annotation, no need to search further\n",
        "\n",
        "                token_labels_ids[token_idx] = current_token_label_id\n",
        "        labels.append(token_labels_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    # offset_mapping is not needed for model training, remove it\n",
        "    tokenized_inputs.pop(\"offset_mapping\")\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# Step 8.3: Apply map() function for tokenization and label alignment\n",
        "# batch=True for efficiency\n",
        "if 'raw_hf_dataset' in locals() and raw_hf_dataset is not None:\n",
        "    try:\n",
        "        final_hf_dataset = raw_hf_dataset.map(\n",
        "            tokenize_and_align_labels_for_dataset,\n",
        "            batched=True,\n",
        "            remove_columns=raw_hf_dataset.column_names # Remove original 'text', 'labels' columns\n",
        "        )\n",
        "\n",
        "        # Step 8.4: Set format for the final Dataset\n",
        "        final_hf_dataset.set_format(\n",
        "            type=\"torch\",\n",
        "            columns=['input_ids', 'attention_mask', 'labels']\n",
        "        )\n",
        "\n",
        "        print(f\"\\nìµœì¢… Hugging Face Dataset (í† í°í™” ë° ë¼ë²¨ ì •ë ¬ ì™„ë£Œ) ìƒ˜í”Œ ìˆ˜: {len(final_hf_dataset)}\")\n",
        "        print(\"\\nìµœì¢… ë³€í™˜ëœ Hugging Face Dataset ì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
        "        print(final_hf_dataset[0])\n",
        "\n",
        "        # Final train/test split (same as before)\n",
        "        train_test_split_dataset = final_hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "        train_dataset = train_test_split_dataset[\"train\"]\n",
        "        eval_dataset = train_test_split_dataset[\"test\"]\n",
        "\n",
        "        print(f\"\\ní•™ìŠµ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}\")\n",
        "        print(f\"í‰ê°€ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(eval_dataset)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ì˜¤ë¥˜: ìµœì¢… Hugging Face Dataset ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"ì„ì‹œ Hugging Face Datasetì´ ìƒì„±ë˜ì§€ ì•Šì•„ ìµœì¢… ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTM1wirwAe-n"
      },
      "source": [
        "### 5.5 ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ë° í˜„ì¬ ëª¨ë¸ì˜ í•œê³„\n",
        "\n",
        "1. NER ëª¨ë¸ì˜ ì„±ëŠ¥í–¥ìƒ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ ë•Œ\n",
        "\n",
        "    ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì •í™•ë„ê°€ ë§¤ìš° ë‚®ê±°ë‚˜, ê¸°ëŒ€í–ˆë˜ ê°œì²´ëª…ì„ ì œëŒ€ë¡œ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì˜ˆ: ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ë¥¼ `QUESTION_CONTENT`ë¡œ ë¶„ë¥˜í•˜ê±°ë‚˜, ì£¼ìš” ê°œì²´ëª…ì„ ë†“ì¹˜ëŠ” ë“±).\n",
        "\n",
        "    **ì´ê²ƒì€ ì§€ê·¹íˆ ì •ìƒì ì¸ í˜„ìƒì…ë‹ˆë‹¤.** í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë‚®ì€ ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
        "\n",
        "    - **ë§¤ìš° ì ì€ í•™ìŠµ ë°ì´í„°:** ìš°ë¦¬ê°€ ì‚¬ìš©í•œ í•™ìŠµ ë°ì´í„°ì…‹ì€ 15ê°œ(ì•½ 5%)ì˜ ìƒ˜í”Œì— ë¶ˆê³¼í•©ë‹ˆë‹¤. BERTì™€ ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ìˆ˜ë§ì€ íŒ¨í„´ì„ í•™ìŠµí•´ì•¼ í•˜ë¯€ë¡œ, ë‹¨ 12ê°œì˜ ìƒ˜í”Œë¡œëŠ” ì˜ë¯¸ ìˆëŠ” íŠ¹ì§•ì„ í•™ìŠµí•˜ê³  ì¼ë°˜í™”í•˜ê¸°ì— í„±ì—†ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.\n",
        "    - **'O' íƒœê·¸ í¸í–¥:** ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì—ì„œ 'O'(Other) íƒœê·¸ê°€ í›¨ì”¬ ë§ê¸° ë•Œë¬¸ì—, ì•ˆì „í•˜ê²Œ ëŒ€ë¶€ë¶„ì˜ í† í°ì„ 'O'ë¡œ ì˜ˆì¸¡í•˜ê±°ë‚˜, íŠ¹ì • ë¼ë²¨ë¡œ ë­‰ëš±ê·¸ë ¤ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.    \n",
        "      \n",
        "   => **Weakly Supervised Learning** ì„ ì ìš©í•˜ê¸°ë¡œ ê²°ì •.\n",
        "\n",
        "\n",
        "\n",
        "2. NER ëª¨ë¸ì— Weakly Supervised Learning ì„ ì ìš©í–ˆì„ ë•Œ\n",
        "\n",
        "    ì§ˆì˜-íšŒì‹  ì„¸íŠ¸ì—ì„œ 'ì§ˆì˜ 1', 'íšŒì‹  1' ê³¼ ê°™ì€ QUESTION_ID ë° ANSWER_ID ë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë©´ ì—¬ì „íˆ ê·¸ ì™¸ì˜ ê°œì²´ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "    \n",
        "    =>  **Back-Translation** ê¸°ë²•ì„ í™œìš©í•œ **Data Augmentation** ì„ ì ìš©í•˜ê¸°ë¡œ ê²°ì •.\n",
        "\n",
        "3. NER ëª¨ë¸ì— Active Learning ì„ ì ìš©í–ˆì„ ë•Œ\n",
        "\n",
        "\n",
        "    \n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXfJLHCvAe-o"
      },
      "source": [
        "## 6. ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì¶”ê°€ ë‹¨ê³„ (ê²°ë¡ )\n",
        "\n",
        "ì§€ê¸ˆê¹Œì§€ ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ì´ì œ êµ¬ì¶•ëœ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì§‘ì¤‘í•  ì°¨ë¡€ì…ë‹ˆë‹¤.\n",
        "\n",
        "### 6.1 ë” ë§ì€ ë°ì´í„° ë¼ë²¨ë§ (ê°€ì¥ ì¤‘ìš”!)\n",
        "\n",
        "ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì˜ 90% ì´ìƒì€ **ë°ì´í„°ì˜ ì–‘ê³¼ ì§ˆ**ì— ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ê°€ì¥ í•„ìš”í•œ ê²ƒì€ Doccanoë¡œ ëŒì•„ê°€ì„œ **ë” ë§ì€ ì§ˆì˜/íšŒì‹  ë¬¸ì„œë¥¼ ë¼ë²¨ë§í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.\n",
        "\n",
        "- **ëª©í‘œ:** ìµœì†Œ ìˆ˜ë°± ê°œì—ì„œ ìˆ˜ì²œ ê°œ ì´ìƒì˜ ì§ˆì˜/íšŒì‹  ìŒì„ ë¼ë²¨ë§í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n",
        "- **ë‹¤ì–‘ì„± í™•ë³´:** ë‹¤ì–‘í•œ ë¬¸ë§¥ê³¼ í‘œí˜„ì„ ê°€ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë¼ë²¨ë§í•˜ì—¬ ëª¨ë¸ì´ ì¼ë°˜í™”ëœ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 'ì†Œë°©ì‹œì„¤ë²•ë ¹'ì´ ë‹¤ì–‘í•œ ë¬¸ì¥ì—ì„œ ì–´ë–»ê²Œ ë“±ì¥í•˜ëŠ”ì§€ ë“±)\n",
        "- **ì¼ê´€ì„± ìœ ì§€:** ë¼ë²¨ë§ ê·œì¹™ì„ ëª…í™•íˆ í•˜ê³  ì¼ê´€ë˜ê²Œ ì ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì›Œí¬í”Œë¡œìš° ë°˜ë³µ:**\n",
        "\n",
        "1.  Doccanoì—ì„œ ì¶”ê°€ ë°ì´í„° ë¼ë²¨ë§\n",
        "2.  `Export only approved documents`ë¥¼ ì²´í¬í•˜ì—¬ `.jsonl` íŒŒì¼ ë‚´ë³´ë‚´ê¸°\n",
        "3.  Google Driveì— ì—…ë¡œë“œ\n",
        "4.  **Colab ë…¸íŠ¸ë¶ì˜ `3. Colabì—ì„œ ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬` ì…€ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ ì¬í•™ìŠµ**\n",
        "\n",
        "ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ëª¨ë¸ì˜ Precision, Recall, F1-scoreê°€ ì ì§„ì ìœ¼ë¡œ í–¥ìƒë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "### 6.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
        "\n",
        "ì¶©ë¶„í•œ ë°ì´í„°ê°€ í™•ë³´ëœ í›„ì—ëŠ”, `4.3 í•™ìŠµ ì¸ì(Training Arguments) ë° íŠ¸ë ˆì´ë„ˆ(Trainer) ì„¤ì •` ì„¹ì…˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°(ì˜ˆ: `learning_rate`, `per_device_train_batch_size`, `num_train_epochs`)ë¥¼ ì¡°ì •í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 6.3 ëª¨ë¸ ì•„í‚¤í…ì²˜ ë° ì „ì´ í•™ìŠµ ê³ ë ¤\n",
        "\n",
        "í˜„ì¬ `klue/bert-base`ëŠ” ì¢‹ì€ ì‹œì‘ì ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë” ì „ë¬¸ì ì¸ ë„ë©”ì¸(ì˜ˆ: ë²•ë¥ )ì— íŠ¹í™”ëœ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ ìˆë‹¤ë©´ ì´ë¥¼ í™œìš©í•˜ëŠ” ê²ƒë„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•œêµ­ì–´ ë²•ë¥  íŠ¹í™” ëª¨ë¸ì€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn8YEB_aAe-o"
      },
      "source": [
        "## 7. í”„ë¡œì íŠ¸ íšŒê³  ë° ë°°ìš´ ì \n",
        "\n",
        "ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‹¤ì œ ì—…ë¬´ì— ì ìš©í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ê²½í—˜í•˜ë©° ë§ì€ ê²ƒì„ ë°°ìš°ê³  ëŠê¼ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ì´ë¡ ìœ¼ë¡œë§Œ ì•Œë˜ ê°œë…ë“¤ì„ ì§ì ‘ ë¶€ë”ªíˆê³  í•´ê²°í•˜ë©° ì–»ì€ êµí›ˆë“¤ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ê¸°ìˆ ì  ì„±ì¥ ë° ê²½í—˜\n",
        "\n",
        "- **í´ë¼ìš°ë“œ ê¸°ë°˜ GPU í™œìš©ì˜ í•„ìš”ì„±:** ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì²˜ëŸ¼ ëŒ€ê·œëª¨ ì—°ì‚°ì´ í•„ìš”í•œ ì‘ì—…ì€ ì¼ë°˜ ë¡œì»¬ PC í™˜ê²½ì—ì„œ ìˆ˜í–‰í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì„ ì²´ê°í–ˆìŠµë‹ˆë‹¤. **Google Colab**ì´ ì œê³µí•˜ëŠ” ë¬´ë£Œ **T4 GPU**ëŠ” ì´ëŸ¬í•œ ì œì•½ì„ ê·¹ë³µí•˜ê³ , ë¹„ìš© íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì‹¤í—˜í•  ìˆ˜ ìˆëŠ” í›Œë¥­í•œ ëŒ€ì•ˆì´ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- **Dockerì™€ ì»¨í…Œì´ë„ˆ í™˜ê²½ ê²½í—˜:** ë°ì´í„° ë¼ë²¨ë§ ë„êµ¬ì¸ **Doccano**ë¥¼ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•˜ê¸° ìœ„í•´ **Docker**ë¥¼ ì²˜ìŒ ì‚¬ìš©í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê²©ë¦¬ëœ í™˜ê²½ì—ì„œ ì†ì‰½ê²Œ ë°°í¬í•˜ê³  ì‹¤í–‰í•˜ëŠ” ì»¨í…Œì´ë„ˆ ê¸°ìˆ ì˜ ê°•ë ¥í•¨ì„ ì´í•´í•˜ê²Œ ë˜ì—ˆê³ , ë³µì¡í•œ ì„¤ì¹˜ ê³¼ì • ì—†ì´ í•„ìš”í•œ ë„êµ¬ë¥¼ ë¹ ë¥´ê²Œ êµ¬ì¶•í•˜ëŠ” ê²½í—˜ì„ ìŒ“ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- **ì—”ë“œ-íˆ¬-ì—”ë“œ(End-to-End) íŒŒì´í”„ë¼ì¸ êµ¬ì¶•:** ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ë¼ë²¨ë§, ëª¨ë¸ í•™ìŠµ, í‰ê°€, ê·¸ë¦¬ê³  ì¶”ë¡ ì— ì´ë¥´ê¸°ê¹Œì§€ì˜ ì „ì²´ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì„¤ê³„í•˜ê³  êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ê° ë‹¨ê³„ê°€ ì–´ë–»ê²Œ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€, ê·¸ë¦¬ê³  ê° ë‹¨ê³„ì—ì„œ ì–´ë–¤ ì ì„ ê³ ë ¤í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ì‹¤ì§ˆì ì¸ ì´í•´ë¥¼ ë†’ì¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ë”¥ëŸ¬ë‹ ëª¨ë¸ê³¼ ë°ì´í„°ì— ëŒ€í•œ ê¹Šì€ ì´í•´\n",
        "\n",
        "- **'Garbage In, Garbage Out'ì˜ ì‹¤ê°:** ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ê²°êµ­ ë°ì´í„°ì˜ ì–‘ê³¼ ì§ˆì— ì˜í•´ ê²°ì •ëœë‹¤ëŠ” ê²ƒì„ ë¼ˆì €ë¦¬ê²Œ ëŠê¼ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, **ë¼ë²¨ë§ëœ ë°ì´í„° ìƒ˜í”Œì˜ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì •í™•ë„ê°€ ë¹„ë¡€í•˜ì—¬ í–¥ìƒ**ë˜ëŠ” ê²ƒì„ ì§ì ‘ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì ì€ ìˆ˜ì˜ ìƒ˜í”Œë¡œ í•™ìŠµí–ˆì„ ë•Œ ëª¨ë¸ì´ ê±°ì˜ ì‘ë™í•˜ì§€ ì•Šì•˜ë˜ ê²½í—˜ì€ ì–‘ì§ˆì˜ ë°ì´í„° í™•ë³´ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ ê¹¨ë‹«ê²Œ í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- **ì‚¬ì „ í•™ìŠµ ëª¨ë¸(Pre-trained Model)ì˜ ìœ„ë ¥:** `klue/bert-base`ì™€ ê°™ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê²ƒì´ ì™œ íš¨ìœ¨ì ì¸ì§€ë¥¼ ì´í•´í–ˆìŠµë‹ˆë‹¤. ë°‘ë°”ë‹¥ë¶€í„° ëª¨ë“  ê²ƒì„ í•™ìŠµì‹œí‚¤ëŠ” ëŒ€ì‹ , ì´ë¯¸ ë°©ëŒ€í•œ í•œêµ­ì–´ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì„ í™œìš©í•¨ìœ¼ë¡œì¨ ë¹„êµì  ì ì€ ë°ì´í„°ë¡œë„ íŠ¹ì • ë„ë©”ì¸ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì „ì´ í•™ìŠµì˜ ê°œë…ì„ ì‹¤ì œë¡œ ì ìš©í•´ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### í˜„ì‹¤ì ì¸ í•œê³„ì™€ ì„±ê³¼\n",
        "\n",
        "- **ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•ì˜ ì–´ë ¤ì›€:** ì§€ë„ í•™ìŠµ(Supervised Learning) ê¸°ë°˜ì˜ NER ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ 2ë§Œ ê°œì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¼ë²¨ë§í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. í•µì‹¬ì€ ëª¨ë¸ì´ ì „ì²´ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆì„ ë§Œí¼, **ì¶©ë¶„í•œ ì–‘ì˜ ëŒ€í‘œì ì¸ ìƒ˜í”Œë“¤ì„ ê³ í’ˆì§ˆë¡œ ë¼ë²¨ë§**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¼ë‹¨ ëª¨ë¸ì´ ì˜ í•™ìŠµë˜ë©´, ë‚˜ë¨¸ì§€ ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ë°ì´í„°ëŠ” ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì›í•˜ëŠ” ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•œ 'ì¶©ë¶„í•œ ì–‘ì˜ ìƒ˜í”Œ'ì„ ë§Œë“œëŠ” ê²ƒ ìì²´ê°€ í˜¼ìì„œëŠ” ë§¤ìš° í˜ë“  ì‘ì—…ì´ì—ˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ 100% ì™„ë²½í•œ ì •í™•ë„ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë§Œë“¤ì§€ëŠ” ëª»í–ˆì§€ë§Œ, ì§€ë„ í•™ìŠµì˜ í•µì‹¬ ì›ë¦¬ì™€ ë°ì´í„°ì˜ ì¤‘ìš”ì„±ì„ ì²´ê°í•˜ëŠ” ê³„ê¸°ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- **ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì„±ê³µì ì¸ ìë™í™”:** ë¹„ë¡ ëª¨ë¸ì´ ì™„ë²½í•˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ, ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ê°œë°œí•œ ìë™í™”ëœ íŒŒì‹± ì‹œìŠ¤í…œì€ ê¸°ì¡´ì˜ **ìˆ˜ì‘ì—…ìœ¼ë¡œ ì§ˆì˜-íšŒì‹  ìŒì„ ê²€ì¦í•˜ë˜ ë°©ì‹ì— ë¹„í•´ ì—…ë¬´ íš¨ìœ¨ì„ ì••ë„ì ìœ¼ë¡œ í–¥ìƒ**ì‹œì¼°ìŠµë‹ˆë‹¤. ë°˜ë³µì ì¸ ì‘ì—…ì„ ìë™í™”í•¨ìœ¼ë¡œì¨ ì‹œê°„ì„ ì ˆì•½í•˜ê³ , ë” ì¤‘ìš”í•œ ë¶„ì„ ì‘ì—…ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤ëŠ” ì ì—ì„œ ì´ í”„ë¡œì íŠ¸ëŠ” ë§¤ìš° ì„±ê³µì ì´ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” 'ì™„ë²½í•¨'ì„ ì¶”êµ¬í•˜ê¸°ë³´ë‹¤ 'ê°œì„ 'ì„ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ì¼ê¹¨ì›Œ ì£¼ì—ˆìŠµë‹ˆë‹¤.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}