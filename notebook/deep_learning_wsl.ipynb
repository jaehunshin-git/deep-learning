{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI7YcRm0Ae-c"
   },
   "source": [
    "# 개체명 인식(Named Entity Recognition, NER) 모델 학습에 대한 기록\n",
    "\n",
    "이 노트는 한국어 텍스트 데이터에서 특정 정보를 추출(예: 질의회신, 법령 내용)하기 위한 개체명 인식(NER) 딥러닝 모델을 학습하는 과정에 대한 기록입니다.\n",
    "\n",
    "**학습 목표:**\n",
    "\n",
    "1. Google Colab 환경 설정 및 Google Drive 연동.\n",
    "2. 원본 텍스트 데이터 전처리 (Doccano 임포트 전).\n",
    "3. Doccano를 이용한 웹 기반 데이터 라벨링.\n",
    "4. 라벨링된 데이터를 딥러닝 모델 학습에 적합한 형태로 변환 (Hugging Face `datasets` 라이브러리).\n",
    "5. 사전 학습된 BERT 모델(`klue/bert-base`)을 활용하여 NER 모델 파인튜닝.\n",
    "6. 학습된 모델의 성능 평가 및 추론(Inference) 방법 이해.\n",
    "\n",
    "**참고:** 이 튜토리얼은 Doccano와 Hugging Face `transformers` 라이브러리를 사용합니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2BXJgIVAe-g"
   },
   "source": [
    "---\n",
    "\n",
    "### 0. 프로젝트 개요 및 동기\n",
    "\n",
    "이 프로젝트는 회사에서 할당받은 업무를 효율적으로 해결하기 위해 시작되었습니다. 저에게 주어진 업무는 **각 공공기관 중앙부처의 법령해석 데이터셋을 웹 크롤링하여 PDF 또는 HWP 파일 내의 질의와 회신을 한 세트로 파싱하고 검증**하는 것이었습니다.\n",
    "\n",
    "구체적으로는 다음과 같은 과제를 안고 있었습니다:\n",
    "\n",
    "1.  **질의-회신 세트 개수 세기**: 웹 크롤링된 문서에서 '질의 1개'와 '회신 1개'가 정상적으로 하나의 세트를 이루는지 확인하고, 전체 세트의 개수를 파악해야 했습니다.\n",
    "2.  **질의-회신 연결성 판단**: 질의와 회신 간의 연결이 논리적으로 올바른지, 즉 '질의 1'에 대한 '회신 1'이 제대로 매칭되는지 판단해야 했습니다.\n",
    "3.  **특이사항 분류**: 특히 질의와 회신의 개수가 맞지 않거나, 회신 내용이 법령에 근거하지 않은 답변일 경우 이를 '특이사항'으로 분류하는 업무가 수반되었습니다.\n",
    "\n",
    "문제는 이러한 분류 및 검증 작업을 수행해야 할 질의회신 세트가 **총 2만 개**에 달한다는 점이었습니다. 이는 엄청난 **수작업과 시간 소모**를 요구하는 일이었습니다. 컴퓨터 공학 전공자로서 이러한 비효율적인 업무를 제 전공 지식을 활용하여 자동화하고 해결하고자 하는 강한 동기를 느꼈습니다. 단순히 반복 작업을 줄이는 것을 넘어, 이 프로젝트를 통해 **딥러닝, 데이터 라벨링, 도커(Docker), 그리고 자연어 처리(NLP) 개념**을 실질적으로 배우고 적용하는 것을 목표로 삼았습니다. 이 보고서는 이러한 프로젝트에 대한 기록입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnBNY7kNAe-g"
   },
   "source": [
    "### 프로젝트 전체 플로우 (이미지)\n",
    "\n",
    "아래는 이 튜토리얼에서 진행할 전체 프로젝트의 워크플로우를 이미지로 시각화한 것입니다. (w/ mermaid)\n",
    "![프로젝트 플로우차트](https://github.com/jaehunshin-git/deep-learning/blob/main/deep-learning-flowchart.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Frk4GrteAe-h"
   },
   "source": [
    "## 프로젝트 전체 플로우 (텍스트 다이어그램)\n",
    "\n",
    "아래는 이 튜토리얼에서 진행할 전체 프로젝트의 워크플로우를 텍스트 형태로 시각화한 것입니다. 각 단계의 흐름을 이해하는 데 도움이 될 것입니다.\n",
    "\n",
    "```text\n",
    "## 프로젝트 전체 플로우\n",
    "\n",
    "1.  시작 - 딥러닝 NER 모델 학습\n",
    "    -> 2. Google Colab 환경 설정\n",
    "        -> 2.1 런타임 유형 GPU 변경\n",
    "        -> 2.2 Google Drive 마운트\n",
    "        -> 2.3 프로젝트 디렉토리 구조 설정\n",
    "        -> 2.4 필요 라이브러리 설치\n",
    "\n",
    "2.4 필요 라이브러리 설치\n",
    "    -> 3. 데이터 준비 - 원본 텍스트 전처리 (로컬 PC)\n",
    "        -> 3.1 원본 dataset.txt 로드\n",
    "        -> 3.2 특정 문자/숫자/워딩 제거 - 정규표현식\n",
    "        -> 3.3 각 질의/회신 쌍 분리 - 개행 문자 추가\n",
    "        -> 3.4 dataset_cleaned_final.txt 저장\n",
    "\n",
    "3.4 dataset_cleaned_final.txt 저장\n",
    "    -> 4. Doccano 데이터 라벨링 (로컬 PC Docker)\n",
    "        -> 4.1 Doccano Docker 설치 및 실행\n",
    "        -> 4.2 Doccano 프로젝트 생성 - 레이블 정의\n",
    "        -> 4.3 dataset_cleaned_final.txt Import - Plain Text\n",
    "        -> 4.4 수동 개체명 라벨링 수행\n",
    "        -> 4.5 라벨링된 데이터 Export - JSONL - Approved Only\n",
    "\n",
    "4.5 라벨링된 데이터 Export - JSONL - Approved Only\n",
    "    -> 5. Google Drive 업로드\n",
    "        -> 6. Colab에서 라벨링된 데이터 로드 및 전처리\n",
    "            -> 6.1 Doccano JSONL 파일 로드\n",
    "            -> 6.2 BIO 태깅을 위한 레이블 목록 생성\n",
    "            -> 6.3 텍스트 토큰화 - 레이블 정렬 - padding, truncation 포함\n",
    "            -> 6.4 Hugging Face Dataset 객체로 변환\n",
    "            -> 6.5 Train/Validation 데이터셋 분할\n",
    "\n",
    "6.5 Train/Validation 데이터셋 분할\n",
    "    -> 7. BERT 기반 NER 모델 학습\n",
    "        -> 7.1 klue/bert-base 모델 및 토크나이저 로드\n",
    "        -> 7.2 성능 지표 Metrics 정의 - seqeval\n",
    "        -> 7.3 학습 인자 Training Arguments 설정 - eval_strategy 등\n",
    "        -> 7.4 Trainer 객체 생성\n",
    "        -> 7.5 trainer.train() 실행\n",
    "\n",
    "7.5 trainer.train() 실행\n",
    "    -> 8. 학습된 모델 평가 및 추론\n",
    "        -> 8.1 학습된 모델 저장\n",
    "        -> 8.2 저장된 모델 및 토크나이저 로드\n",
    "        -> 8.3 단일 텍스트 추론 함수 정의\n",
    "        -> 8.4 예시 문장으로 모델 테스트\n",
    "\n",
    "8.4 예시 문장으로 모델 테스트\n",
    "    -> 9. 결과 분석 - 모델 성능 평가\n",
    "        -> 10. 성능이 충분한가?\n",
    "            -> Yes -> 11. 배포 및 활용\n",
    "            -> No  -> 12. 성능 향상 전략 적용\n",
    "                -> 12.1 더 많은 데이터 라벨링 - 가장 중요!\n",
    "                -> 12.2 하이퍼파라미터 튜닝\n",
    "                -> 12.3 모델 아키텍처/전이 학습 고려\n",
    "                -> 12.1 더 많은 데이터 라벨링 - 가장 중요! -> 4. Doccano 데이터 라벨링 (로컬 PC Docker)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS3XdVsWAe-h"
   },
   "source": [
    "## 0. Google Colab 환경 설정\n",
    "\n",
    "모델 학습을 위해 Google Colab 환경을 설정합니다.\n",
    "\n",
    "### 0.1 런타임 유형 변경 (GPU 활성화)\n",
    "\n",
    "Colab에서 딥러닝 모델을 빠르게 학습시키려면 GPU를 사용하는 것이 필수적입니다.\n",
    "\n",
    "1.  Colab 상단 메뉴에서 `런타임` (Runtime)을 클릭합니다.\n",
    "2.  `런타임 유형 변경` (Change runtime type)을 선택합니다.\n",
    "3.  `하드웨어 가속기` (Hardware accelerator) 드롭다운 메뉴에서 `GPU` 혹은 `TPU` 를 선택한 후 `저장` (Save)을 클릭합니다. - (`TPU` 선택 시 역번역 시간 단축 가능)\n",
    "\n",
    "**주의:**\n",
    "\n",
    "1.  런타임을 변경하거나 Colab 세션이 끊겼다가 다시 연결되면, 이전에 설치했던 라이브러리나 정의했던 변수들이 초기화됩니다. 이 경우 다음 단계를 다시 실행해야 합니다.\n",
    "2.  google.colab 라이브러리는 colab 전용 라이브러리이므로 로컬에서는 설치 및 사용할 수 없습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOsLz5_7KDXz"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEDZ_B4ZAe-j"
   },
   "source": [
    "### 0.3 프로젝트 디렉토리 구조 설정\n",
    "\n",
    "Google Drive 내부에 프로젝트를 위한 폴더 구조를 설정합니다. 데이터와 모델을 체계적으로 관리할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M07W0c9lD-T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 프로젝트의 루트 경로 설정 (이전에 당신이 설정한 Google Drive 경로)\n",
    "# Colab Notebooks 폴더 아래에 deep-learning-ner-advanced 이라는 폴더를 만들어 이 프로젝트의 루트로 사용한다고 가정합니다.\n",
    "# 이 경로는 사용자의 Google Drive 구조에 맞게 수정해주세요.\n",
    "\n",
    "# 프로젝트 경로 설정\n",
    "project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning-ner-advanced/\"\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "model_dir = os.path.join(project_root, \"model\")\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"프로젝트 루트: {project_root}\")\n",
    "print(f\"데이터 폴더: {data_dir}\")\n",
    "print(f\"모델 저장 폴더: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP5bg9mhAe-j"
   },
   "source": [
    "### 0.4 필요한 라이브러리 설치\n",
    "\n",
    "BERT 모델을 사용하기 위한 Hugging Face `transformers`와 `datasets` 라이브러리를 설치합니다. `accelerate`는 학습 가속화에 도움을 줍니다.\n",
    "\n",
    "1. `transformers`\n",
    "\n",
    "   - 무엇인가요?\n",
    "\n",
    "     - Hugging Face에서 개발한 가장 인기 있는 딥러닝 라이브러리 중 하나입니다. BERT, GPT-2, RoBERTa 등 다양한 사전 학습된 트랜스포머 기반 모델(Transformer-based models)과 토크나이저(Tokenizer)를 제공합니다. 자연어 처리(NLP) 분야의 최신 모델들을 쉽게 불러와 파인튜닝(fine-tuning)하거나 추론(inference)할 수 있도록 돕습니다.\n",
    "\n",
    "   - 내 프로젝트에서 활용:\n",
    "\n",
    "     - 모델 로드: `AutoModelForTokenClassification.from_pretrained(\"klue/bert-base\")`를 사용하여 한국어에 특화된 BERT 모델인 klue/bert-base를 개체명 인식(Token Classification) 작업에 맞게 불러옵니다.\n",
    "\n",
    "     - 토크나이저 로드: AutoTokenizer.from_pretrained(\"klue/bert-base\")를 통해 텍스트를 모델이 이해할 수 있는 토큰(단어 조각) ID로 변환하는 데 필요한 토크나이저를 로드합니다.\n",
    "\n",
    "     - 모델 학습 관리: Trainer 클래스를 사용하여 학습 인자(TrainingArguments) 설정, 학습 데이터와 평가 데이터 지정, 모델 학습(trainer.train()) 등 복잡한 학습 과정을 편리하게 관리합니다.\n",
    "\n",
    "     - 모델 저장 및 로드: 학습된 모델을 저장하고 나중에 다시 불러와 추론에 활용하는 기능도 이 라이브러리를 통해 수행됩니다.\n",
    "\n",
    "2. `datasets`\n",
    "\n",
    "   - 무엇인가요?\n",
    "\n",
    "     - Hugging Face에서 개발한 또 다른 핵심 라이브러리로, 대규모 데이터셋을 효율적으로 로드, 전처리, 저장할 수 있도록 설계되었습니다. 특히 NLP 작업에 최적화되어 있으며, 다양한 공개 데이터셋을 쉽게 가져올 수 있는 기능도 제공합니다.\n",
    "\n",
    "   - 내 프로젝트에서 활용:\n",
    "\n",
    "     - 데이터 로드: Doccano에서 내보낸 .jsonl 형식의 라벨링된 데이터를 로드하고 Pandas DataFrame을 거쳐 datasets.Dataset 객체로 변환합니다. Dataset 객체는 Hugging Face Trainer에 직접 입력될 수 있는 형태로, 데이터 처리를 효율적으로 만듭니다.\n",
    "\n",
    "     - 데이터 전처리: Dataset 객체에 토크나이징된 input_ids, attention_mask, 그리고 각 토큰에 해당하는 labels (BIO 태그)를 저장하는 데 사용됩니다.\n",
    "\n",
    "     - 데이터 분할: train_test_split 기능을 사용하여 라벨링된 전체 데이터를 학습용(train_dataset)과 평가용(eval_dataset)으로 쉽게 나눕니다.\n",
    "\n",
    "3. `accelerate`\n",
    "\n",
    "   - 무엇인가요?\n",
    "\n",
    "     - Hugging Face에서 개발한 경량 라이브러리로, PyTorch 모델의 학습 과정을 분산 및 혼합 정밀도(mixed-precision) 학습 환경에 맞게 자동으로 조정해 줍니다. 개발자가 복잡한 분산 학습 코드를 직접 작성할 필요 없이, 소수의 코드 변경만으로 GPU 여러 개나 TPU 같은 가속기를 활용할 수 있도록 돕습니다.\n",
    "\n",
    "   - 내 프로젝트에서 활용:\n",
    "\n",
    "     - 당신의 Colab 환경에서 GPU 런타임을 사용하도록 설정했습니다. accelerate 라이브러리는 transformers의 Trainer 내부적으로 활용되어 GPU 자원을 효율적으로 사용하여 모델 학습 속도를 가속화합니다. 이는 대규모 모델과 데이터셋을 학습할 때 필수적인 역할을 합니다.\n",
    "\n",
    "4. `seqeval`\n",
    "\n",
    "   - 무엇인가요?\n",
    "\n",
    "     - 시퀀스 라벨링(Sequence Labeling) 작업(개체명 인식, 품사 태깅 등)의 성능을 평가하기 위한 Python 라이브러리입니다. 특히 precision, recall, f1-score, accuracy와 같은 주요 지표들을 계산하는 데 특화되어 있습니다. 개체명 인식에서는 단순히 개별 토큰의 정확도뿐만 아니라, **전체 개체명 스팬(span)**이 정확하게 예측되었는지를 평가하는 것이 중요하며, seqeval이 이러한 역할을 수행합니다.\n",
    "\n",
    "   - 내 프로젝트에서 활용:\n",
    "\n",
    "     - `load_metric(\"seqeval\")`를 사용하여 평가 지표를 로드합니다.\n",
    "\n",
    "     - `compute_metrics` 함수 내에서 모델의 예측 결과와 실제 레이블을 seqeval 포맷에 맞게 변환하고, 이를 metric.compute() 함수에 전달하여 최종 정밀도, 재현율, F1-점수 등을 계산하는 데 사용됩니다. 이 지표들은 모델의 학습 과정을 모니터링하고 최종 성능을 평가하는 데 핵심적인 역할을 합니다.\n",
    "\n",
    "이 라이브러리들은 모두 Hugging Face 생태계의 일부로, 복잡한 딥러닝 NLP 프로젝트를 효율적으로 개발하고 관리하는 데 큰 도움을 줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1mUOCBKBohkT",
    "outputId": "1f9e22d4-4f2c-4134-f2c3-26a5af7e7bdd"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[torch] evaluate datasets accelerate seqeval -q\n",
    "\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTSddOsUAe-k"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azs-ewTQnKDL"
   },
   "source": [
    "## 1. 데이터 준비 및 전처리 (Doccano 연동 전)\n",
    "\n",
    "원본 텍스트 파일(`dataset.txt`)을 Doccano에 효율적으로 라벨링하기 위해 사전 전처리 작업을 수행합니다. 이 과정은 불필요한 노이즈를 제거하고, 각 질의/회신 쌍을 Doccano가 개별 문서로 인식할 수 있도록 분리하는 역할을 합니다.\n",
    "\n",
    "**원본 `dataset.txt` 파일의 문제점:**\n",
    "\n",
    "- 불필요한 숫자나 특정 워딩(예: '2021년 소방시설법령 질의회신집') 포함.\n",
    "- 모든 질의/회신이 하나의 긴 텍스트로 이어져 있어, Doccano에서 개별 문서로 처리하기 어려움.\n",
    "\n",
    "이 전처리 스크립트는 **Google Colab이 아닌, 로컬 환경(예: VS Code)**에서 원본 텍스트 파일을 처리한 후, 그 결과 파일을 Google Drive에 업로드하여 Doccano로 가져오는 것을 권장합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_5d6OsVAe-k"
   },
   "source": [
    "### 1.1 `dataset.txt` 파일 클리닝 및 문서 분리 코드\n",
    "\n",
    "아래 코드를 로컬 컴퓨터의 `.py` 파일로 저장(예: `preprocess_dataset.py`)하고, 원본 `dataset.txt` 파일이 있는 동일한 폴더에서 실행하세요.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_and_prepare_text_for_doccano_final_v2(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 파일에서 다음을 수행합니다:\n",
    "    1. 한 줄에 숫자(0-9) 하나만 있는 라인을 삭제합니다.\n",
    "    2. '2021년 소방시설법령 질의회신집' 문자열을 제거합니다.\n",
    "    3. '질의 N.' (또는 '질의 N')으로 시작하는 줄 앞에 빈 줄(\\n\\n)을 추가합니다.\n",
    "       (단, 파일의 맨 처음 나오는 '질의 1.' 앞에는 추가하지 않습니다.)\n",
    "       이때, 각 '질의 N.' 블록이 정확히 '\\\\n\\\\n'으로 구분되도록 합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as f_in:\n",
    "            lines = f_in.readlines() # 파일을 줄 단위로 읽어옵니다.\n",
    "\n",
    "        temp_content = [] # 임시로 클리닝된 줄을 저장할 리스트\n",
    "        for line in lines:\n",
    "            # 1단계: '2021년 소방시설법령 질의회신집' 문자열 제거\n",
    "            line = line.replace('2021년 소방시설법령 질의회신집', '')\n",
    "\n",
    "            # 2단계: 한 줄에 숫자 하나만 있는 라인 삭제\n",
    "            if line.strip().isdigit() and len(line.strip()) == 1:\n",
    "                continue # 해당 줄은 건너뛰고 다음 줄로 넘어갑니다.\n",
    "\n",
    "            # 모든 줄의 양쪽 공백 제거 후 임시 리스트에 추가 (원래 줄바꿈도 제거)\n",
    "            temp_content.append(line.strip())\n",
    "\n",
    "        # 임시 리스트의 줄들을 하나의 문자열로 결합 (각 줄 사이에 공백 1개로 연결)\n",
    "        cleaned_raw_text = ' '.join(temp_content).strip()\n",
    "\n",
    "        # 3단계: '질의 N.' 앞에 빈 줄 추가\n",
    "        # '질의 N.' (또는 '질의 N') 패턴을 찾아서 '\\\\n\\\\n질의 N.'으로 교체합니다.\n",
    "        # 단, 파일의 맨 처음 나오는 '질의 1.' 앞에는 추가하지 않습니다.\n",
    "\n",
    "        # re.sub의 repl 매개변수에 함수를 사용하여 동적 교체\n",
    "        def replace_query_marker(match):\n",
    "            # match.start() == 0 이면 파일의 맨 처음 '질의 N.'입니다.\n",
    "            if match.start() == 0:\n",
    "                return match.group(0) # '질의 N.' 자체를 반환 (앞에 아무것도 안 붙임)\n",
    "            else:\n",
    "                return '\\\\n\\\\n' + match.group(0) # 그 외의 '질의 N.' 앞에는 '\\\\n\\\\n'을 붙임\n",
    "\n",
    "        # 패턴: '질의' 다음에 공백(0개 이상), 숫자(1개 이상), 점(선택적)\n",
    "        final_content = re.sub(r'질의\\s*\\d+\\.?', replace_query_marker, cleaned_raw_text)\n",
    "\n",
    "        final_content = re.sub(r'(회신\\s*\\d\\.?)', r'\\n\\1', final_content)\n",
    "\n",
    "        final_content = re.sub(r'▣.*', '', final_content)  # '▣'로 시작하는 줄 제거\n",
    "\n",
    "        final_content = re.sub(r'\\s\\d{2,3}\\s', '', final_content)  # 숫자(2-3자리) 제거\n",
    "\n",
    "        # 최종적으로 문자열의 시작과 끝에 불필요한 공백/개행을 제거\n",
    "        final_content = final_content.strip()\n",
    "\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write(final_content)\n",
    "\n",
    "        print(f\"파일이 성공적으로 처리되어 '{output_filepath}'에 저장되었습니다.\")\n",
    "        print(\"이 파일을 Doccano에 'Plain Text' 형식으로 가져오시면 됩니다.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: 입력 파일 '{input_filepath}'을(를) 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 중 오류 발생: {e}\")\n",
    "\n",
    "# --- 사용 방법 (아래 경로를 당신의 실제 파일 경로로 수정해주세요) ---\n",
    "\n",
    "# 로컬 프로젝트 루트 경로 (VS Code에서 해당 파일이 있는 폴더 경로)\n",
    "# 예: r\\\"C:\\\\Users\\\\JHSHIN\\\\ProgrammingCodes\\\\deep-learning\\\"\n",
    "project_root = r\\\"C:\\\\Users\\\\JHSHIN\\\\ProgrammingCodes\\\\deep-learning\\\"\n",
    "\n",
    "# 원본 입력 파일 경로 (당신이 가지고 있는 원본 dataset.txt 파일)\n",
    "input_file = os.path.join(project_root, 'dataset.txt')\n",
    "\n",
    "# 수정된 내용을 저장할 새로운 출력 파일 경로 (새로운 이름으로 저장하는 것을 권장)\n",
    "output_file = os.path.join(project_root, 'dataset_cleaned.txt') # 파일 이름 변경\n",
    "\n",
    "# 함수 실행\n",
    "clean_and_prepare_text_for_doccano_final_v2(input_file, output_file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHJ-8kTGg2N-"
   },
   "source": [
    "**원본 `dataset.txt` 파일 예시:**\n",
    "\n",
    "질의 1\n",
    "연면적 450㎡인 특정소방대상물에 최초 건축물 사용승인시에 비상경보설비 설치가 되지\n",
    "않은 경우 건축허가일과 사용승인일 중 소방시설설치기준 적용일은?\n",
    "회신 1\n",
    "건축물 등의 신축ㆍ증축ㆍ개축ㆍ재축ㆍ이전ㆍ용도변경 또는 대수선의 허가\n",
    "ㆍ협의 및 사용승인의 권한이 있는 행정기관은 소방시설법 제7조제1항에\n",
    "따라 소재지를 관할하는 소방본부장이나 소방서장의 동의를 받아야 하므로,\n",
    "건축허가등과 관련한 협의과정이 누락되었다면, 건축허가 신청일을 기준으로\n",
    "소방시설의 설치기준을 적용합니다.\n",
    "질의 2\n",
    "최초 사업허가승인월이 ‘13년 6월인 대상물의 사업이 변경되어 최종 사업허가승인월이\n",
    "19년 2월인 경우, 소방시설법 적용 기준일은?\n",
    "회신 2\n",
    "소방시설설치기준 적용 기준일은 최초 사용승인계획 신청 시점입니다.2021년 소방시설법령 질의회신집\n",
    "4\n",
    "최초 건축허가과정에서 허가동의된 사업계획은 이후 사업 변경계획이 신청\n",
    "되어도 변경계획이 신청된 시점의 소방시설법을 적용하지 않습니다. 부칙\n",
    "<대통령령 제27810호, 2017.1.26.>호제2조 소방시설 설치에 관한 적용례에\n",
    "관한 적용례에 특정소방대상물의 신축ㆍ증축ㆍ개축ㆍ재축ㆍ이전ㆍ용도변경\n",
    "ㆍ대수선의 허가ㆍ협의를 신청하거나 신고하는 경우로 명시하고 있어, 사용\n",
    "승인계획변경 등 허가의 변경사항은 개정 규정 적용대상에 해당하지 않습\n",
    "니다.\n",
    "▣ 건축허가등의 동의대상 범위\n",
    "[소방시설법 시행령 제12조]\n",
    "관계법령\n",
    "제12조(건축허가등 동의대상물의 범위 등)\n",
    "① 법 제7조제1항에 따라 건축허가등을 할 때 미리 소방본부장 또는 소방서장의 동의를\n",
    "받아야 하는 건축물 등의 범위는 다음 각 호와 같다.\n",
    "1. 연면적(「건축법 시행령」 제119조제1항제4호에 따라 산정된 면적을 말한다. 이하 같\n",
    "다)이 400제곱미터 이상인 건축물. 다만, 다음 각 목의 어느 하나에 해당하는 시설은\n",
    "해당 목에서 정한 기준 이상인 건축물로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVsYijB2g2N-"
   },
   "source": [
    "**`preprocessing_dataset.py` 실행 후 `dataset_cleaned.txt` 예시:**\n",
    "\n",
    "\n",
    "질의 1 연면적 450㎡인 특정소방대상물에 최초 건축물 사용승인시에 비상경보설비 설치가 되지 않은 경우 건축허가일과 사용승인일 중 소방시설설치기준 적용일은? 회신 1 건축물 등의 신축ㆍ증축ㆍ개축ㆍ재축ㆍ이전ㆍ용도변경 또는 대수선의 허가 ㆍ협의 및 사용승인의 권한이 있는 행정기관은 소방시설법 제7조제1항에 따라 소재지를 관할하는 소방본부장이나 소방서장의 동의를 받아야 하므로, 건축허가등과 관련한 협의과정이 누락되었다면, 건축허가 신청일을 기준으로 소방시설의 설치기준을 적용합니다.\n",
    "\n",
    "질의 2 최초 사업허가승인월이 ‘13년 6월인 대상물의 사업이 변경되어 최종 사업허가승인월이 19년 2월인 경우, 소방시설법 적용 기준일은? 회신 2 소방시설설치기준 적용 기준일은 최초 사용승인계획 신청 시점입니다. 최초 건축허가과정에서 허가동의된 사업계획은 이후 사업 변경계획이 신청 되어도 변경계획이 신청된 시점의 소방시설법을 적용하지 않습니다. 부칙 <대통령령 제27810호, 2017.1.26.>호제2조 소방시설 설치에 관한 적용례에 관한 적용례에 특정소방대상물의 신축ㆍ증축ㆍ개축ㆍ재축ㆍ이전ㆍ용도변경 ㆍ대수선의 허가ㆍ협의를 신청하거나 신고하는 경우로 명시하고 있어, 사용 승인계획변경 등 허가의 변경사항은 개정 규정 적용대상에 해당하지 않습 니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8N8ZjC4Ae-k"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb5q7cERn_aD"
   },
   "source": [
    "## 2. Doccano를 이용한 데이터 라벨링\n",
    "\n",
    "Doccano는 텍스트 어노테이션(라벨링)을 위한 오픈소스 도구입니다. 웹 기반 환경에서 직관적으로 개체명(NER) 라벨링을 수행할 수 있습니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8KfePBUg2N-"
   },
   "source": [
    "### **(New) 2.0 약 지도 학습 (Weak Supervision)으로 라벨링 가속화** 🚀\n",
    "\n",
    "수천 개의 데이터를 처음부터 수동으로 라벨링하는 것은 매우 힘든 작업입니다. **약 지도 학습**은 정규 표현식(Regex)과 같은 간단한 **규칙(Heuristics)을 이용해 대량의 데이터에 자동으로 라벨을 부여**하는 기법입니다.\n",
    "\n",
    "- 정규표현식(Regex)과 같은 간단한 규칙을 사용하여 '질의 1', '회신 1' 등 명확한 패턴을 가진 개체를 자동으로 라벨링합니다.\n",
    "\n",
    "- 이렇게 생성된 \"초벌 라벨링\" 데이터를 Doccano에 임포트하여 검토 및 수정만 하면 되므로, 라벨링 시간을 획기적으로 단축할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69bJPq2Hg2N_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "def create_weak_labels_advanced(input_text_path, output_jsonl_path):\n",
    "    \"\"\"\n",
    "    고도로 상세화된 정규식 휴리스틱을 사용해 자동으로 라벨을 생성합니다.\n",
    "    - ID, QUESTION_CONTENT, ANSWER_CONTENT는 구조 기반으로 라벨링\n",
    "    - LAW_CONTENT는 상세화된 복합 패턴을 적용하여 정교하게 라벨링\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_text_path, 'r', encoding='utf-8') as f:\n",
    "            documents = f.read().strip().split('\\\\n\\\\n')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{input_text_path}' 파일을 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "        return\n",
    "\n",
    "    # --- 라벨링 규칙(패턴) 상세화 ---\n",
    "    # ID 패턴\n",
    "    question_id_pattern = re.compile(r'질의\\s*\\d+\\.?(?=\\s|$)')\n",
    "    answer_id_pattern = re.compile(r'회신\\s*\\d+\\.?(?=\\s|$)')\n",
    "\n",
    "    # LAW_CONTENT를 찾기 위한 고도로 상세화된 휴리스틱(Heuristics) 패턴\n",
    "    # 우선순위가 높은(더 구체적인) 패턴을 리스트의 위쪽에 배치합니다.\n",
    "    law_patterns = [\n",
    "        # 1. 「...법」, 『...기준』 등 특수기호로 감싸진 법률/기준 이름 (가장 강력한 패턴)\n",
    "        re.compile(r'(?:「|『)[^」』]+(?:법|법률|시행령|시행규칙|기준)(?:」|』)'),\n",
    "\n",
    "        # 2. '소방시설법 시행령 제12조제1항제1호' 와 같이 법 이름과 조항이 함께 나오는 패턴\n",
    "        re.compile(r'[^」』\\s]+(?:법|령|규칙|기준)\\s?제\\s?\\d+조(?:\\s?제\\s?\\d+항)?(?:\\s?제\\s?\\d+호)?'),\n",
    "\n",
    "        # 3. '[소방시설법 제9조]' 와 같이 대괄호로 감싸진 법률 및 조항\n",
    "        re.compile(r'\\[\\s?[^\\]]+(?:법|령|기준|조)\\s?[^\\]]*\\]'),\n",
    "\n",
    "        # 4. '[별표5]' 와 같이 별표/서식을 나타내는 패턴\n",
    "        re.compile(r'\\[\\s?별표\\s?\\d+\\s?\\]'),\n",
    "\n",
    "        # 5. '제N조 제N항 제N호' 등 법률 조항만 단독으로 나오는 패턴\n",
    "        re.compile(r'제\\s?\\d+조(?:\\s?제\\s?\\d+항)?(?:\\s?제\\s?\\d+호)?'),\n",
    "\n",
    "        # 6. '화재안전기준' 등 단독으로 쓰이는 핵심 법규/기준 키워드\n",
    "        re.compile(r'화재안전기준'),\n",
    "\n",
    "        # 7. '...법에 따라' 등 법적 근거를 제시하는 표현 (가장 범위가 넓으므로 마지막에 배치)\n",
    "        re.compile(r'\\S+법[에\\s](?:따라|따르면|의하면|근거하여)')\n",
    "    ]\n",
    "\n",
    "    all_labeled_docs = []\n",
    "    print(f\"총 {len(documents)}개의 문서에 대해 향상된 약 지도 학습을 시작합니다...\")\n",
    "\n",
    "    for doc_text in documents:\n",
    "        if not doc_text.strip():\n",
    "            continue\n",
    "\n",
    "        spans = []\n",
    "        markers = []\n",
    "\n",
    "        # 1. 문서 내 모든 ID 마커의 위치와 종류를 찾음\n",
    "        for match in question_id_pattern.finditer(doc_text):\n",
    "            markers.append({'start': match.start(), 'end': match.end(), 'type': 'QUESTION_ID'})\n",
    "        for match in answer_id_pattern.finditer(doc_text):\n",
    "            markers.append({'start': match.start(), 'end': match.end(), 'type': 'ANSWER_ID'})\n",
    "\n",
    "        markers.sort(key=lambda x: x['start'])\n",
    "        if not markers:\n",
    "            continue\n",
    "\n",
    "        # 2. 마커를 기준으로 잘라가며 CONTENT 라벨링\n",
    "        for i in range(len(markers)):\n",
    "            current_marker = markers[i]\n",
    "            spans.append([current_marker['start'], current_marker['end'], current_marker['type']])\n",
    "\n",
    "            content_start = current_marker['end']\n",
    "            content_end = markers[i+1]['start'] if i + 1 < len(markers) else len(doc_text)\n",
    "\n",
    "            content_text_segment = doc_text[content_start:content_end]\n",
    "            lstrip_len = len(content_text_segment) - len(content_text_segment.lstrip())\n",
    "            content_start += lstrip_len\n",
    "            content_end -= (len(content_text_segment) - len(content_text_segment.rstrip()))\n",
    "\n",
    "            if content_start >= content_end: continue\n",
    "\n",
    "            if current_marker['type'] == 'QUESTION_ID':\n",
    "                spans.append([content_start, content_end, 'QUESTION_CONTENT'])\n",
    "\n",
    "            elif current_marker['type'] == 'ANSWER_ID':\n",
    "                answer_block_text = doc_text[content_start:content_end]\n",
    "                law_spans_in_block = []\n",
    "\n",
    "                # 3. ANSWER_CONTENT 내에서 모든 LAW_CONTENT 패턴 찾기\n",
    "                for pattern in law_patterns:\n",
    "                    for match in pattern.finditer(answer_block_text):\n",
    "                        law_start = content_start + match.start()\n",
    "                        law_end = content_start + match.end()\n",
    "                        law_spans_in_block.append([law_start, law_end, 'LAW_CONTENT'])\n",
    "\n",
    "                if not law_spans_in_block:\n",
    "                    spans.append([content_start, content_end, 'ANSWER_CONTENT'])\n",
    "                    continue\n",
    "\n",
    "                # 4. 찾은 LAW_CONTENT들을 병합하고, 그 외 부분을 ANSWER_CONTENT로 라벨링\n",
    "                law_spans_in_block.sort(key=lambda x: x[0])\n",
    "\n",
    "                # 중첩/겹치는 부분을 병합 (Merge overlapping spans)\n",
    "                merged_law_spans = []\n",
    "                if law_spans_in_block:\n",
    "                    current_span = law_spans_in_block[0]\n",
    "                    for next_span in law_spans_in_block[1:]:\n",
    "                        if next_span[0] < current_span[1]: # 겹치는 경우\n",
    "                            current_span[1] = max(current_span[1], next_span[1])\n",
    "                        else:\n",
    "                            merged_law_spans.append(current_span)\n",
    "                            current_span = next_span\n",
    "                    merged_law_spans.append(current_span)\n",
    "\n",
    "                # LAW_CONTENT를 제외한 나머지 부분을 ANSWER_CONTENT로 채우기\n",
    "                last_end = content_start\n",
    "                for law_start, law_end, law_label in merged_law_spans:\n",
    "                    if law_start > last_end:\n",
    "                        spans.append([last_end, law_start, 'ANSWER_CONTENT'])\n",
    "                    spans.append([law_start, law_end, law_label])\n",
    "                    last_end = law_end\n",
    "\n",
    "                if content_end > last_end:\n",
    "                    spans.append([last_end, content_end, 'ANSWER_CONTENT'])\n",
    "\n",
    "        spans.sort(key=lambda x: x[0])\n",
    "        all_labeled_docs.append({\"text\": doc_text, \"labels\": spans})\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for doc in all_labeled_docs:\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"고도로 상세화된 약 지도 학습 완료! {len(all_labeled_docs)}개의 문서가 '{output_jsonl_path}'에 저장되었습니다.\")\n",
    "\n",
    "# --- 함수 실행 ---\n",
    "# 아래 경로들은 실제 환경에 맞게 설정해야 합니다.\n",
    "# project_root = os.path.dirname(os.path.abspath(__file__)) # local 환경에서 실행 시 주석 해제 (현재 파일의 경로를 기준으로 설정)\n",
    "\n",
    "print(f\"프로젝트 루트 경로: {project_root}\")\n",
    "\n",
    "after_perprocessing = os.path.join(project_root, 'data', 'dataset_cleaned.txt')\n",
    "weakly_labeled_path = os.path.join(project_root, 'data', 'weakly_labeled_advanced.jsonl')\n",
    "# weakly_labeled_path = os.path.join('data_dir', 'weakly_labeled_advanced.json')\n",
    "\n",
    "# # 파일이 존재할 때만 실행\n",
    "if os.path.exists(after_perprocessing):\n",
    "    create_weak_labels_advanced(after_perprocessing, weakly_labeled_path)\n",
    "else:\n",
    "    print(f\"입력 파일 '{after_perprocessing}'를 찾을 수 없어 실행을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lM6oGuIAe-l"
   },
   "source": [
    "### 2.1 Doccano 설치 및 실행 (Docker 사용)\n",
    "\n",
    "Doccano는 Docker를 이용하여 가장 쉽게 설치하고 실행할 수 있습니다. 로컬 컴퓨터에 Docker가 설치되어 있어야 합니다.\n",
    "\n",
    "1.  **Docker 설치:** Docker Desktop을 다운로드하여 설치합니다: [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)\n",
    "\n",
    "2.  **Doccano Docker 이미지 다운로드 및 실행:** 터미널 또는 명령 프롬프트에서 다음 명령어를 실행합니다.\n",
    "\n",
    "    ```bash\n",
    "    docker pull doccano/doccano\n",
    "    docker run -it -p 8000:8000 doccano/doccano\n",
    "    ```\n",
    "\n",
    "3.  **Doccano 접속:** 웹 브라우저를 열고 `http://localhost:8000/`으로 접속합니다. 기본 관리자 계정은 `admin / admin`입니다.\n",
    "\n",
    "### 2.2 Doccano 프로젝트 생성 및 설정\n",
    "\n",
    "1.  **새 프로젝트 생성:** Doccano 웹 UI에서 `Create new project` 버튼을 클릭합니다.\n",
    "2.  **프로젝트 이름 및 설명 입력:** 프로젝트 이름을 지정하고(예: '법령 질의회신 NER') 설명을 추가합니다.\n",
    "3.  **프로젝트 유형 선택:** `Sequence Labeling` (개체명 인식을 위한 유형)을 선택합니다.\n",
    "4.  **레이블 정의:** NER 모델이 인식할 개체명 레이블들을 정의합니다. 예를 들어:\n",
    "\n",
    "    - `QUESTION_ID`\n",
    "    - `QUESTION_CONTENT`\n",
    "    - `ANSWER_ID`\n",
    "    - `ANSWER_CONTENT`\n",
    "    - `MISC_HEADER` (예: 고시 번호, 문서 제목)\n",
    "    - `LAW_CONTENT` (예: 특정 법령 조항, 법률 이름)\n",
    "\n",
    "    각 레이블에 단축키와 색상을 지정하면 라벨링 효율을 높일 수 있습니다.\n",
    "\n",
    "### 2.3 클리닝된 데이터 Doccano로 가져오기 (Import)\n",
    "\n",
    "1.  **Import Data 탭 이동:** 생성된 프로젝트 페이지에서 `Import Data` 탭을 클릭합니다.\n",
    "2.  **파일 선택:** `dataset_cleaned_final.txt` 파일을 선택합니다.\n",
    "3.  **파일 형식 선택:** `Plain Text`를 선택합니다. (우리가 `\\n\\n`으로 문서를 분리했기 때문에, Doccano가 이를 개별 문서로 인식합니다.)\n",
    "4.  **Import 시작:** `Import` 버튼을 클릭합니다.\n",
    "\n",
    "### 2.4 개체명 라벨링 수행\n",
    "\n",
    "1.  **Annotate Data 탭 이동:** `Annotate Data` 탭을 클릭합니다.\n",
    "2.  **텍스트 선택 및 라벨 지정:** 각 문서의 텍스트를 읽고, 해당하는 단어나 구절을 드래그하여 선택한 후, 오른쪽에 나타나는 레이블 버튼(또는 지정된 단축키)을 클릭하여 라벨을 지정합니다.\n",
    "3.  **작업 저장 및 다음 문서로 이동:** 한 문서의 라벨링을 마쳤으면 `Submit` 또는 `Save` 버튼을 눌러 저장하고 다음 문서로 넘어갑니다.\n",
    "\n",
    "### 2.5 라벨링 완료 후 데이터 내보내기 (Export)\n",
    "\n",
    "충분한 양의 데이터를 라벨링했다면 (초기 학습을 위해 최소 100개 이상, 실사용을 위해 수백~수천 개 권장), 이제 라벨링된 데이터를 내보냅니다.\n",
    "\n",
    "1.  **Export Data 탭 이동:** 프로젝트 페이지에서 `Export Data` 탭을 클릭합니다.\n",
    "2.  **파일 형식 선택:** `JSONL`을 선택합니다. (Hugging Face `datasets` 라이브러리가 쉽게 로드할 수 있는 형식입니다.)\n",
    "3.  **'Export only approved documents' 체크 (매우 중요!)**:\n",
    "    이 옵션을 **반드시 체크**합니다. 이 옵션은 라벨링이 완료되고 `Approve` (승인)된 문서만 내보내어 학습 데이터의 품질을 보장합니다. 승인되지 않은 문서는 아직 검토가 필요하거나 수정될 여지가 있는 것으로 간주됩니다.\n",
    "\n",
    "    - **Approve 하는 방법:** `Annotate` 화면에서 각 문서의 라벨링을 마친 후 `Approve` 버튼을 클릭하거나, `Annotation` -> `Guideline` -> `Approve` 로 이동하여 일괄 승인할 수 있습니다.\n",
    "\n",
    "4.  **Export 시작:** `Export` 버튼을 클릭하여 `.jsonl` 파일을 다운로드합니다.\n",
    "\n",
    "**다운로드한 `.jsonl` 파일을 Google Drive의 `data` 폴더(예: `/content/gdrive/MyDrive/Colab Notebooks/deep-learning/data/`)로 업로드합니다.** 파일 이름을 기억해두세요 (예: `after_datalabeling.jsonl`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYHjdUpWAe-l"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKuEbPmPAe-l"
   },
   "source": [
    "## 3. Colab에서 라벨링된 데이터 로드 및 전처리\n",
    "\n",
    "Doccano에서 내보낸 `.jsonl` 파일을 Google Colab으로 가져와 BERT 모델 학습에 적합한 형태로 전처리합니다. 이 과정에서 각 단어(또는 서브워드 토큰)에 BIO(Beginning, Inside, Outside) 태그를 할당합니다.\n",
    "\n",
    "**⚠️ 중요: Colab 런타임이 끊어졌다면, 반드시 `0.2 Google Drive 마운트`, `0.3 프로젝트 디렉토리 구조 설정`, `0.4 필요한 라이브러리 설치` 셀을 다시 실행한 후 이 단계를 진행해주세요.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kMQNzsRcAe-l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "# --- 1. Doccano에서 정의한 레이블 목록 (B-, I- 없이) ---\n",
    "# Doccano에서 프로젝트 설정 시 정의했던 레이블 이름들을 여기에 정확히 입력합니다.\n",
    "doccano_raw_labels = [\n",
    "    \"QUESTION_ID\",\n",
    "    \"QUESTION_CONTENT\",\n",
    "    \"ANSWER_ID\",\n",
    "    \"ANSWER_CONTENT\",\n",
    "    # \"MISC_HEADER\",\n",
    "    \"LAW_CONTENT\",\n",
    "]\n",
    "\n",
    "# --- 2. 모델 학습을 위한 최종 BIO 레이블 목록 및 매핑 ---\n",
    "# 'O' (Other) 태그는 라벨링되지 않은 모든 토큰을 의미하며 항상 포함됩니다.\n",
    "# 각 원본 레이블에 대해 'B-' (Beginning)와 'I-' (Inside) 태그를 생성합니다.\n",
    "label_list = [\"O\"]  # 'Other' 태그는 항상 포함\n",
    "for label in doccano_raw_labels:\n",
    "    label_list.append(f\"B-{label}\")  # Beginning 태그: 개체명의 첫 번째 토큰\n",
    "    label_list.append(f\"I-{label}\")  # Inside 태그: 개체명의 두 번째 이후 토큰\n",
    "\n",
    "# 레이블 이름과 정수 ID 간의 매핑을 생성합니다.\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list)  # 모델의 출력 레이어 크기에 사용됩니다.\n",
    "\n",
    "print(f\"모델 학습을 위한 최종 레이블 목록: {label_list}\")\n",
    "print(f\"총 레이블 개수: {num_labels}\")\n",
    "\n",
    "pprint(label_to_id)\n",
    "pprint(id_to_label)\n",
    "\n",
    "\n",
    "# --- 3. Doccano에서 내보낸 JSONL 파일 경로 설정 ---\n",
    "# 이 경로는 0.3단계에서 설정한 data_dir과 일치해야 합니다.\n",
    "# Colab 런타임이 재시작되면 변수가 초기화될 수 있으므로, 방어 코드를 추가하거나 0.3단계 셀을 다시 실행해야 합니다.\n",
    "try:\n",
    "    # project_root가 정의되지 않았다면 (런타임 재시작 등), 기본 경로를 설정\n",
    "    if \"project_root\" not in locals():\n",
    "        project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning/\"\n",
    "        data_dir = os.path.join(project_root, \"data\")\n",
    "        print(\"경로 변수 'project_root'가 정의되지 않아 기본 경로를 사용합니다.\")\n",
    "except NameError:\n",
    "    # NameError 발생 시 (아예 변수 선언이 안 되어 있을 때)\n",
    "    project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning/\"\n",
    "    data_dir = os.path.join(project_root, \"data\")\n",
    "    print(\"경로 변수 'project_root'가 정의되지 않아 기본 경로를 설정했습니다.\")\n",
    "\n",
    "# Doccano에서 내보낸 실제 JSONL 파일 이름을 여기에 입력하세요.\n",
    "labeled_data_file_path = os.path.join(data_dir, \"after_datalabeling.jsonl\")\n",
    "\n",
    "# --- 4. 사용할 토크나이저 ---\n",
    "# 한국어 BERT 모델인 'klue/bert-base' 토크나이저를 로드합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# --- 5. Doccano JSONL 로드 및 Hugging Face Dataset 형식으로 변환 ---\n",
    "converted_data_for_hf = []\n",
    "try:\n",
    "    with open(labeled_data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc_data = json.loads(line)\n",
    "            pprint(doc_data)\n",
    "            text = doc_data[\"text\"]\n",
    "            pprint(text)\n",
    "            # Doccano의 'label' 필드는 [[start_offset, end_offset, \"LABEL_NAME\"], ...] 형식입니다.\n",
    "            annotations = doc_data.get(\"labels\", [])\n",
    "            pprint(annotations)\n",
    "            # 텍스트를 토큰화하고 각 토큰의 원본 텍스트에서의 위치(offset)를 함께 가져옵니다.\n",
    "            # 중요: padding과 truncation을 활성화하여 모든 시퀀스 길이를 통일합니다.\n",
    "            tokenized_output = tokenizer(\n",
    "                text,\n",
    "                return_offsets_mapping=True,  # 토큰의 원본 텍스트에서의 시작/끝 오프셋 반환\n",
    "                truncation=True,  # max_length를 초과하는 시퀀스는 잘라냄\n",
    "                max_length=512,  # BERT 모델의 최대 입력 길이 (일반적으로 512)\n",
    "                padding=\"max_length\",  # 모든 시퀀스를 max_length에 맞춰 패딩\n",
    "            )\n",
    "\n",
    "            input_ids = tokenized_output[\"input_ids\"]\n",
    "            offsets = tokenized_output[\"offset_mapping\"]\n",
    "\n",
    "            # 각 토큰에 해당하는 BIO 레이블을 초기화합니다.\n",
    "            # -100은 손실 계산에서 무시될 특수 토큰(CLS, SEP 등)이나 패딩 토큰에 할당됩니다.\n",
    "            labels = [-100] * len(input_ids)\n",
    "\n",
    "            word_ids = tokenized_output.word_ids(\n",
    "                batch_index=0\n",
    "            )  # 토큰이 어떤 원본 단어에 해당하는지 ID 매핑\n",
    "\n",
    "            # 토큰별로 레이블 할당 (Doccano의 스팬 기반 레이블을 토큰 기반 BIO 레이블로 변환)\n",
    "            for token_idx, word_idx in enumerate(word_ids):\n",
    "                if word_idx is None:  # CLS, SEP, 패딩 토큰과 같은 특수 토큰\n",
    "                    labels[token_idx] = -100  # 손실 계산에서 무시\n",
    "                else:  # 일반 단어에 해당하는 토큰\n",
    "                    # 현재 토큰의 원본 텍스트에서의 시작/끝 오프셋\n",
    "                    token_start_offset = offsets[token_idx][0]\n",
    "                    token_end_offset = offsets[token_idx][1]\n",
    "\n",
    "                    current_token_label_name = (\n",
    "                        \"O\"  # 현재 토큰의 기본 레이블은 \"O\" (Other)\n",
    "                    )\n",
    "\n",
    "                    # 현재 토큰이 어떤 어노테이션에 속하는지 확인합니다.\n",
    "                    for ann_start, ann_end, ann_label_name in annotations:\n",
    "                        # 토큰의 오프셋이 어노테이션 범위 내에 완전히 포함되는 경우\n",
    "                        if (\n",
    "                            ann_start <= token_start_offset\n",
    "                            and token_end_offset <= ann_end\n",
    "                        ):\n",
    "                            # 만약 현재 토큰의 시작 오프셋이 어노테이션의 시작 오프셋과 같다면 B- 태그\n",
    "                            if ann_start == token_start_offset:\n",
    "                                current_token_label_name = f\"B-{ann_label_name}\"\n",
    "                            # 아니라면 I- 태그\n",
    "                            else:\n",
    "                                current_token_label_name = f\"I-{ann_label_name}\"\n",
    "                            break  # 해당 어노테이션을 찾았으니 더 이상 검색할 필요 없음\n",
    "\n",
    "                    labels[token_idx] = label_to_id[current_token_label_name]\n",
    "\n",
    "            # 실제 모델 입력에 필요한 형태로 데이터를 저장합니다.\n",
    "            converted_data_for_hf.append(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": tokenized_output[\"attention_mask\"],\n",
    "                    \"labels\": labels,  # 이 labels는 ID로 변환된 BIO 태그 리스트\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Python 리스트를 Pandas DataFrame으로 변환 후 Hugging Face Dataset으로 변환\n",
    "    processed_df = pd.DataFrame(converted_data_for_hf)\n",
    "    # 특징(features)을 명시적으로 정의하여 Dataset이 올바른 데이터 타입을 갖도록 합니다.\n",
    "    # 이는 특히 ClassLabel과 같은 특정 타입의 데이터에 중요합니다.\n",
    "    features = Features(\n",
    "        {\n",
    "            \"input_ids\": Sequence(Value(\"int32\")),\n",
    "            \"attention_mask\": Sequence(Value(\"int32\")),\n",
    "            \"labels\": Sequence(\n",
    "                ClassLabel(names=label_list)\n",
    "            ),  # labels는 ClassLabel 시퀀스\n",
    "        }\n",
    "    )\n",
    "    hf_dataset = Dataset.from_pandas(processed_df, features=features)\n",
    "\n",
    "    print(f\"\\nHugging Face Dataset으로 변환된 샘플 수: {len(hf_dataset)}\")\n",
    "    print(\"\\n변환된 Hugging Face Dataset 첫 번째 샘플:\")\n",
    "    print(hf_dataset[0])\n",
    "    print(\n",
    "        f\"디코딩된 토큰: {tokenizer.convert_ids_to_tokens(hf_dataset[0]['input_ids'])}\"\n",
    "    )\n",
    "    decoded_labels = [\n",
    "        id_to_label[l_id] if l_id != -100 else \"O\" for l_id in hf_dataset[0][\"labels\"]\n",
    "    ]\n",
    "    print(f\"디코딩된 레이블: {decoded_labels}\")\n",
    "    # 원본 텍스트 디코딩\n",
    "    print(\n",
    "        f\"디코딩된 텍스트: {tokenizer.decode(hf_dataset[0]['input_ids'], skip_special_tokens=True)}\"\n",
    "    )\n",
    "\n",
    "    # --- 6. 데이터셋 분할 (Train/Validation Split) ---\n",
    "    # 모델 학습을 위해 전체 데이터셋을 학습(train) 세트와 평가(validation) 세트로 나눕니다.\n",
    "    # test_size=0.2는 전체 데이터의 20%를 평가 세트로 사용한다는 의미입니다.\n",
    "    # seed는 재현 가능한 결과를 위해 설정합니다.\n",
    "    train_test_split_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test_split_dataset[\"train\"]\n",
    "    eval_dataset = train_test_split_dataset[\"test\"]\n",
    "\n",
    "    print(f\"\\n학습 데이터셋 샘플 수: {len(train_dataset)}\")\n",
    "    print(f\"평가 데이터셋 샘플 수: {len(eval_dataset)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"오류: 레이블링된 파일 '{labeled_data_file_path}'을(를) 찾을 수 없습니다. Google Drive에 업로드했는지 확인하세요.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"레이블링된 파일을 로드하거나 처리하는 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4vDd7dBAe-l"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzrQKZ-PAe-l"
   },
   "source": [
    "## 4. BERT 기반 NER 모델 학습\n",
    "\n",
    "이제 전처리된 데이터를 사용하여 `klue/bert-base` 모델을 개체명 인식 작업에 맞게 파인튜닝합니다. Hugging Face `Trainer` API를 사용하면 학습 과정을 매우 편리하게 관리할 수 있습니다.\n",
    "\n",
    "**⚠️ 중요: Colab 런타임이 끊어졌다면, 반드시 `0.2 Google Drive 마운트`, `0.3 프로젝트 디렉토리 구조 설정`, `0.4 필요한 라이브러리 설치` 셀과 함께 `3. Colab에서 라벨링된 데이터 로드 및 전처리` 셀을 다시 실행한 후 이 단계를 진행해주세요.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRBb7iNCAe-l"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. 모델 로드\n",
    "# AutoModelForTokenClassification은 토큰 분류(NER과 같은 작업)를 위한 모델입니다.\n",
    "# num_labels는 당신의 label_list에 있는 최종 레이블(BIO 태그 포함)의 개수입니다.\n",
    "# id_to_label과 label2id는 모델이 숫자 ID와 레이블 이름을 매핑하는 데 사용됩니다.\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"klue/bert-base\",\n",
    "    num_labels=num_labels,  # 우리의 최종 BIO 레이블 개수\n",
    "    id2label=id_to_label,  # ID를 레이블 이름으로 매핑\n",
    "    label2id=label_to_id,  # 레이블 이름을 ID로 매핑\n",
    ")\n",
    "\n",
    "print(\"모델과 토크나이저 로드 완료.\")\n",
    "print(\n",
    "    \"\"\"\n",
    "주의: 'Some weights of BertForTokenClassification were not initialized...' 메시지는 정상입니다.\n",
    "이는 사전 학습된 BERT 모델에 토큰 분류를 위한 새로운 레이어(classifier.bias, classifier.weight)가 추가되었기 때문입니다.\n",
    "이 레이어는 아직 학습되지 않았으므로, 이제 당신의 데이터로 파인튜닝해야 합니다.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_kpBta0Ae-m"
   },
   "source": [
    "### 4.2 성능 지표(Metrics) 정의\n",
    "\n",
    "모델 학습 중 또는 학습 완료 후 모델의 성능을 평가할 지표를 정의합니다. NER에서는 주로 **정확도(accuracy), 정밀도(precision), 재현율(recall), F1-점수(F1-score)**를 사용합니다. Hugging Face에서 제공하는 `seqeval` 라이브러리를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jbi0ifa2Ae-m"
   },
   "outputs": [],
   "source": [
    "# 평가지표 로드 (Hugging Face에서 제공하는 seqeval 사용)\n",
    "# load_metric은 더 이상 사용되지 않으므로, evaluate.load를 대신 사용합니다.\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    print(f\"DEBUG - compute_metrics 내부:\")\n",
    "    print(f\"DEBUG - argmax 전 예측 형태: {p[0].shape}\") # 로짓 형태 확인\n",
    "    print(f\"DEBUG - 레이블 형태: {labels.shape}\")\n",
    "    print(f\"DEBUG - 샘플 예측 (argmax 후, 필터링 전): {predictions[0][:10]}\") # 첫 10개 샘플\n",
    "    print(f\"DEBUG - 샘플 레이블 (필터링 전): {labels[0][:10]}\") # 첫 10개 샘플\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    print(f\"DEBUG - 샘플 true_predictions (디코딩 및 필터링 후): {true_predictions[0][:10]}\") # 첫 10개 샘플\n",
    "    print(f\"DEBUG - 샘플 true_labels (디코딩 및 필터링 후): {true_labels[0][:10]}\") # 첫 10개 샘플\n",
    "    print(f\"DEBUG - 첫 샘플의 true_predictions 길이: {len(true_predictions[0])}\")\n",
    "    print(f\"DEBUG - 첫 샘플의 true_labels 길이: {len(true_labels[0])}\")\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "print(\"성능 지표 계산 함수 정의 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2EzLz1-Ae-m"
   },
   "source": [
    "### 4.3 학습 인자(Training Arguments) 및 트레이너(Trainer) 설정\n",
    "\n",
    "모델을 어떻게 학습시킬지에 대한 설정(하이퍼파라미터)과, Hugging Face `Trainer` 객체를 생성합니다.\n",
    "\n",
    "**⚠️ 오류 해결 (이전 대화에서 발생했던 문제):**\n",
    "\n",
    "- `evaluation_strategy`가 `eval_strategy`로 이름이 변경되었습니다. 최신 버전에 맞춰 수정합니다.\n",
    "- `model_dir`이 정의되지 않았다는 오류는 `0.3 프로젝트 디렉토리 구조 설정` 셀을 다시 실행하면 해결됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCF2Co_MAe-m"
   },
   "outputs": [],
   "source": [
    "# 학습 인자 설정\n",
    "# output_dir: 학습된 모델과 로그가 저장될 경로\n",
    "# eval_strategy: 'epoch'로 설정하여 각 에포크 종료 시마다 평가 수행\n",
    "# learning_rate: 학습률 (초기값 2e-5가 일반적입니다.)\n",
    "# per_device_train_batch_size: GPU당 학습 배치 크기 (GPU 메모리에 따라 조절 가능)\n",
    "# per_device_eval_batch_size: GPU당 평가 배치 크기\n",
    "# num_train_epochs: 전체 학습 에포크 수 (데이터셋을 몇 번 반복 학습할지, 데이터가 적으면 과적합 주의)\n",
    "# weight_decay: 가중치 감쇠 (과적합 방지 기법)\n",
    "# push_to_hub: 학습된 모델을 Hugging Face Hub에 업로드할지 여부 (지금은 False)\n",
    "# logging_dir: 학습 로그 저장 경로\n",
    "# logging_steps: 몇 스텝마다 로그를 출력할지 (데이터셋이 작으면 로그가 자주 안 나올 수 있음)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,  # 모델 저장 경로 (0.3단계에서 설정한 model_dir)\n",
    "    eval_strategy=\"epoch\",  # <-- 'evaluation_strategy'가 'eval_strategy'로 변경됨!\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # GPU 메모리가 부족하면 8, 4 등으로 줄여보세요.\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,  # 초반에는 적은 에포크로 시작하고, 데이터가 많아지면 늘려볼 수 있습니다.\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=os.path.join(model_dir, \"logs\"),\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # Colab 환경에서 불필요한 경고 방지 (wandb 등 설정 시 변경)\n",
    ")\n",
    "\n",
    "# Trainer 객체 생성\n",
    "# Trainer는 모델, 학습 인자, 데이터셋, 토크나이저, 성능 지표 계산 함수를 인자로 받아 학습을 관리합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # 학습 데이터셋\n",
    "    eval_dataset=eval_dataset,  # 평가 데이터셋\n",
    "    tokenizer=tokenizer,  # 토크나이저 (FutureWarning이 발생할 수 있지만, 현재는 정상 작동)\n",
    "    compute_metrics=compute_metrics,  # 성능 지표 계산 함수\n",
    ")\n",
    "\n",
    "print(\"학습 인자 및 Trainer 설정 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrxdP31EAe-m"
   },
   "source": [
    "### 4.4 모델 학습 시작\n",
    "\n",
    "이제 `trainer.train()` 명령으로 모델 학습을 시작합니다. 학습 과정과 평가 결과는 로그로 출력됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJSRc2JWAe-m"
   },
   "outputs": [],
   "source": [
    "# 모델 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uww8HA2iAe-m"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxcOtB7u0Fyj"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512 # Doccano에서 설정한 max_length와 동일하게 설정\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        # --- 디버깅용 print 문 추가 ---\n",
    "        print(f\"\\n--- 샘플 {i} ---\")\n",
    "        print(f\"원본 words: {examples['tokens'][i]}\")\n",
    "        print(f\"원본 ner_tags (단어 수준): {label}\")\n",
    "        print(f\"토큰화된 input_ids (일부): {tokenized_inputs['input_ids'][i][:10]}...\")\n",
    "        print(f\"word_ids: {word_ids}\")\n",
    "        # --- 디버깅용 print 문 끝 ---\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # 특수 토큰 (-100)\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # 첫 번째 서브워드 토큰 (단어의 시작)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # 후속 서브워드 토큰 (단어의 나머지 부분)\n",
    "            else:\n",
    "                # B- 태그를 I- 태그로 변경\n",
    "                # 원래 라벨이 B- 태그인 경우 해당 I- 태그 ID로 변경합니다.\n",
    "                # 예를 들어, B-QUESTION_ID (ID 1) -> I-QUESTION_ID (ID 2)\n",
    "                # 이 부분이 중요합니다: id_to_label과 label_to_id를 정확히 사용해야 합니다.\n",
    "                current_label = id_to_label[label[word_idx]]\n",
    "                if current_label.startswith(\"B-\"):\n",
    "                    # 'B-'를 'I-'로 바꾸고 해당 ID를 찾습니다.\n",
    "                    # 이 로직이 정확한지 확인해야 합니다.\n",
    "                    # 예: B-QUESTION_ID -> I-QUESTION_ID (ID 1 -> ID 2)\n",
    "                    i_tag_label = \"I-\" + current_label[2:]\n",
    "                    if i_tag_label in label_to_id: # label_to_id 딕셔너리가 필요합니다.\n",
    "                        label_ids.append(label_to_id[i_tag_label])\n",
    "                    else:\n",
    "                        # 해당 I- 태그가 없는 경우 O 태그로 처리하거나, 그대로 B- 태그 유지 (고려 필요)\n",
    "                        label_ids.append(label[word_idx]) # 혹은 0으로? -> 오류의 원인일 수 있음\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx]) # B- 태그가 아닌 경우 그대로 유지\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        # --- 디버깅용 print 문 추가 ---\n",
    "        # print(f\"정렬된 레이블 (토큰 수준): {label_ids}\")\n",
    "        # print(f\"디코딩된 정렬된 레이블 (일부): {[id_to_label[lid] for lid in label_ids if lid != -100][:20]}\")\n",
    "        # --- 디버깅용 print 문 끝 ---\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axq3D2fYAe-n"
   },
   "source": [
    "## 5. 학습된 모델 평가 및 추론 (테스트)\n",
    "\n",
    "학습이 완료된 모델을 저장하고, 새로운 텍스트에 대해 개체명 인식을 수행하는 방법을 알아봅니다. 현재 학습 데이터셋의 양이 매우 적으므로, 초기 예측 결과는 만족스럽지 않을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q23b0epDAe-n"
   },
   "source": [
    "### 5.1 학습된 모델 저장\n",
    "\n",
    "학습이 완료된 모델의 가중치와 설정 파일이 `model_dir` 경로에 저장됩니다. 이렇게 저장된 모델은 나중에 다시 로드하여 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myMA7qNVAe-n"
   },
   "outputs": [],
   "source": [
    "# 학습된 모델 저장\n",
    "trainer.save_model(model_dir)  # model_dir은 0.3단계에서 설정했던 모델 저장 경로입니다.\n",
    "\n",
    "print(f\"학습된 모델이 '{model_dir}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU37rrNIAe-n"
   },
   "source": [
    "### 5.2 저장된 모델 및 토크나이저 로드 (추론 준비)\n",
    "\n",
    "저장된 모델을 로드하여 새로운 텍스트에 대한 예측(추론)을 수행할 준비를 합니다. 모델을 CPU/GPU에 로드하고 평가 모드로 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnTLpbw-Ae-n"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 저장된 모델과 토크나이저 로드\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "\n",
    "# 모델을 평가 모드로 설정 (드롭아웃 등을 비활성화하여 일관된 예측을 보장)\n",
    "loaded_model.eval()\n",
    "\n",
    "# GPU가 있다면 GPU로 모델 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "print(f\"모델과 토크나이저가 '{model_dir}'에서 로드되었습니다. 현재 device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAyS6G1JAe-n"
   },
   "source": [
    "### 5.3 단일 텍스트에 대한 개체명 인식 추론 함수\n",
    "\n",
    "임의의 텍스트를 입력받아 모델이 개체명을 예측하고, BIO 태그를 사람이 읽기 쉬운 '개체명 스팬' 형태로 변환하여 출력하는 함수를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu2r0s19Ae-n"
   },
   "outputs": [],
   "source": [
    "def predict_ner(text, tokenizer, model, id_to_label, device):\n",
    "    # 텍스트 토큰화 (모델 입력에 맞게 패딩, 잘림 적용)\n",
    "    tokenized_input = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # PyTorch 텐서 반환\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # 모델이 있는 디바이스(CPU 또는 GPU)로 입력 텐서 이동\n",
    "\n",
    "    # 모델 예측 수행\n",
    "    with torch.no_grad():  # 추론 시에는 기울기 계산 비활성화 (메모리 절약, 속도 향상)\n",
    "        output = model(**tokenized_input)\n",
    "\n",
    "    # 예측된 로짓(logits)에서 가장 높은 확률을 가진 레이블 ID 추출\n",
    "    # squeeze()는 배치 차원(크기 1)을 제거하고, cpu().numpy()로 넘파이 배열로 변환\n",
    "    predictions = torch.argmax(output.logits, dim=2).squeeze().cpu().numpy()\n",
    "\n",
    "    # 토큰 및 레이블 디코딩\n",
    "    tokens = tokenizer.convert_ids_to_tokens(\n",
    "        tokenized_input[\"input_ids\"].squeeze().cpu().numpy()\n",
    "    )\n",
    "    predicted_labels = [id_to_label[p_id] for p_id in predictions]\n",
    "\n",
    "    # 특수 토큰 및 패딩 토큰 제거 (실제 단어에 대한 예측만 보기 위함)\n",
    "    # 또한 B-와 I- 태그를 결합하여 개체명 스팬을 출력합니다.\n",
    "\n",
    "    results = []\n",
    "    current_entity = \"\"\n",
    "    current_label = \"\"\n",
    "\n",
    "    # 예측된 토큰과 레이블을 순회하며 개체명 추출\n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token.startswith(\"##\"):  # WordPiece 토크나이저의 서브워드 접두사 제거\n",
    "            token = token[2:]\n",
    "\n",
    "        # CLS, SEP, PAD 토큰 등 특수 토큰 제외\n",
    "        if token in tokenizer.all_special_tokens:  # CLS, SEP, PAD 토큰은 건너뜁니다.\n",
    "            if current_entity:  # 이전에 수집 중이던 개체명이 있다면 마무리\n",
    "                results.append((current_entity.strip(), current_label))\n",
    "                current_entity = \"\"\n",
    "                current_label = \"\"\n",
    "            continue\n",
    "\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:  # 이전에 수집 중이던 개체명이 있다면 먼저 결과에 추가\n",
    "                results.append((current_entity.strip(), current_label))\n",
    "            current_entity = token  # 새로운 개체명 시작\n",
    "            current_label = label[2:]  # 'B-' 접두사 제거하여 순수 레이블 이름 저장\n",
    "        elif label.startswith(\"I-\") and current_label and label[2:] == current_label:\n",
    "            # 현재 토큰이 이전 개체명의 연속 (Inside)이고, 레이블 유형이 일치할 경우\n",
    "            current_entity += token  # 현재 토큰을 기존 개체명에 추가\n",
    "        else:  # 'O' 태그이거나, 'I-' 태그인데 이전 레이블과 일치하지 않는 경우\n",
    "            if current_entity:  # 이전에 수집 중이던 개체명이 있다면 마무리\n",
    "                results.append((current_entity.strip(), current_label))\n",
    "            current_entity = \"\"  # 현재 개체명 초기화\n",
    "            current_label = \"\"  # 현재 레이블 초기화\n",
    "\n",
    "    # 반복문 종료 후 마지막에 남아있는 개체명이 있다면 추가\n",
    "    if current_entity:\n",
    "        results.append((current_entity.strip(), current_label))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"개체명 인식 추론 함수 정의 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2u9qvTsAe-n"
   },
   "source": [
    "### 5.4 모델 테스트 (예시 문장)\n",
    "\n",
    "정의한 추론 함수를 사용하여 학습된 모델이 새로운 문장에서 개체명을 얼마나 잘 예측하는지 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b00YZ_cnOF8"
   },
   "outputs": [],
   "source": [
    "# 테스트할 예시 문장\n",
    "test_text_1 = \"\"\"\n",
    "질의 5 \"첨부5\"과 같이 감지기 배선을 시공하여도 문제가 없는지 여부. 회신 5 「자동화재탐지설비 및 시각경보기의 화재안전기준(NFSC 203)」 제11조제2호 나목에 따라 감지기 상호간 또는 감지기로부터 수신기에 이르는 감지기 회로의 배선은 「옥내소화전설비의 화재안전기준(NFSC 102)」 [별표 1]에 따른 내화배선 또는 내열배선을 사용하도록 정하고 있습니다. 귀하께서 ”첨부파일5“로 문의하신 감지기 배선 시공방법은 HFIX 전선을 금속제 가요전선관에 수납하여 시공한 부분은 내열 또는 내화배선 시공법에 적합하나, 단열재에서 천장면 난연CD전전관에 수납한 시공방법은 화재안전 기준에서 규정하고 있는 내열ㆍ내화배선에 해당하지 않습니다. 따라서 답변1을 참고하여 내화배선에 따른 시공을 하거나 내열배선에 따른시공으로 배선처리를 하여야 할 것으로 판단됩니다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 추론 함수 실행\n",
    "predicted_entities_1 = predict_ner(\n",
    "    test_text_1, loaded_tokenizer, loaded_model, id_to_label, device\n",
    ")\n",
    "\n",
    "print(f\"\\n입력 텍스트: {test_text_1}\")\n",
    "print(f\"예측된 개체명: {predicted_entities_1}\")\n",
    "\n",
    "# 다른 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfoXZKYoN6_N"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "# 한국어 -> 영어 번역 모델 로드 (이전과 동일)\n",
    "ko_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "ko_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\").to(device)\n",
    "\n",
    "# 영어 -> 한국어 번역 모델 로드 (모델 ID 수정)\n",
    "# 'Helsinki-NLP/opus-mt-en-ko' 대신 'Helsinki-NLP/opus-mt-tc-big-en-ko' 사용\n",
    "en_ko_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
    "en_ko_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\").to(device)\n",
    "\n",
    "print(\"번역 모델 로딩 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf2RDvLsQ1HW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. Doccano에서 정의한 레이블 목록 (B-, I- 없이) ---\n",
    "doccano_raw_labels = [\n",
    "    \"QUESTION_ID\",\n",
    "    \"QUESTION_CONTENT\",\n",
    "    \"ANSWER_ID\",\n",
    "    \"ANSWER_CONTENT\",\n",
    "    \"LAW_CONTENT\",\n",
    "]\n",
    "\n",
    "# --- 2. 모델 학습을 위한 최종 BIO 레이블 목록 및 매핑 ---\n",
    "label_list = [\"O\"]\n",
    "for label in doccano_raw_labels:\n",
    "    label_list.append(f\"B-{label}\")\n",
    "    label_list.append(f\"I-{label}\")\n",
    "\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "print(f\"모델 학습을 위한 최종 레이블 목록: {label_list}\")\n",
    "print(f\"총 레이블 개수: {num_labels}\")\n",
    "print(f\"label_to_id: {label_to_id}\")\n",
    "print(f\"id_to_label: {id_to_label}\")\n",
    "\n",
    "# --- 3. Doccano에서 내보낸 JSONL 파일 경로 설정 ---\n",
    "project_root = \"/content/gdrive/MyDrive/Colab Notebooks/deep-learning-ner-advanced/\"\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "labeled_data_file_path = os.path.join(data_dir, \"after_datalabeling.jsonl\")\n",
    "\n",
    "# --- 4. 사용할 토크나이저 및 모델 로드 ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 번역 모델 로드 (이전 단계에서 수정한 모델 ID 사용)\n",
    "ko_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "ko_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\").to(device)\n",
    "en_ko_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\") # 수정된 모델 ID\n",
    "en_ko_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\").to(device)\n",
    "print(\"번역 모델 로딩 완료.\")\n",
    "\n",
    "\n",
    "# --- 5. 역번역 함수 (이전과 동일) ---\n",
    "def back_translate(text, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device):\n",
    "    # 한국어 -> 영어\n",
    "    tokenized_text = ko_en_tokenizer.prepare_seq2seq_batch([text], return_tensors='pt').to(device)\n",
    "    translated_tokens = ko_en_model.generate(**tokenized_text)\n",
    "    english_text = ko_en_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    # 영어 -> 한국어\n",
    "    tokenized_text = en_ko_tokenizer.prepare_seq2seq_batch([english_text], return_tensors='pt').to(device)\n",
    "    translated_tokens = en_ko_model.generate(**tokenized_text)\n",
    "    korean_text_augmented = en_ko_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    return korean_text_augmented\n",
    "\n",
    "# --- 6. NER 데이터를 증강하는 메인 함수 (수정됨: 라벨 이름을 ID로 변환) ---\n",
    "def augment_ner_example(example, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device):\n",
    "    text = example['text']\n",
    "    # 레이블이 시작 인덱스 순으로 정렬되었는지 확인\n",
    "    labels = sorted(example.get('labels', []), key=lambda x: x[0])\n",
    "\n",
    "    augmented_text_parts = []\n",
    "    # new_labels는 [start, end, LABEL_ID]를 저장합니다.\n",
    "    new_labels = []\n",
    "    last_idx = 0\n",
    "    current_offset = 0\n",
    "\n",
    "    for start, end, label_name in labels:\n",
    "        context_part = text[last_idx:start]\n",
    "        if context_part:\n",
    "            augmented_context = back_translate(context_part, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device)\n",
    "            augmented_text_parts.append(augmented_context)\n",
    "            current_offset += len(augmented_context)\n",
    "\n",
    "        entity_part = text[start:end]\n",
    "        augmented_text_parts.append(entity_part)\n",
    "\n",
    "        new_start = current_offset\n",
    "        new_end = current_offset + len(entity_part)\n",
    "\n",
    "        # label_name 문자열을 해당하는 ID로 변환\n",
    "        # label_name이 label_to_id에 없는 경우 처리 필요 (유효한 Doccano 내보내기에서는 발생하지 않아야 함)\n",
    "        label_id = label_to_id.get(f\"B-{label_name}\", 0) # 스팬 시작에 B- 태그 ID를 사용하고, 찾을 수 없는 경우 0('O')으로 기본값 설정\n",
    "\n",
    "        new_labels.append([new_start, new_end, label_id]) # [start, end, LABEL_ID]로 저장\n",
    "\n",
    "        current_offset += len(entity_part)\n",
    "        last_idx = end\n",
    "\n",
    "    final_context_part = text[last_idx:]\n",
    "    if final_context_part:\n",
    "        augmented_context = back_translate(final_context_part, ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device)\n",
    "        augmented_text_parts.append(augmented_context)\n",
    "\n",
    "    new_text = \".\".join(augmented_text_parts)\n",
    "    return {\"text\": new_text, \"labels\": new_labels} # 'labels'는 이제 [start, end, LABEL_ID]를 저장합니다.\n",
    "\n",
    "\n",
    "# --- 7. Doccano JSONL 로드 및 데이터 증강 ---\n",
    "original_data_raw = []\n",
    "try:\n",
    "    with open(labeled_data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            original_data_raw.append(json.loads(line))\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 레이블링된 파일 '{labeled_data_file_path}'을(를) 찾을 수 없습니다.\")\n",
    "    # 대화형 모드가 아닌 경우 정상적으로 종료하거나 이 경우를 적절하게 처리\n",
    "    # Colab에서는 오류를 인쇄하는 것만으로도 대화형 디버깅에 충분한 경우가 많습니다.\n",
    "    pass\n",
    "\n",
    "\n",
    "# 원본 데이터 레이블을 문자열에서 ID로 즉시 변환\n",
    "original_data_processed = []\n",
    "for example in original_data_raw:\n",
    "    processed_labels = []\n",
    "    # 레이블이 존재하고 정렬되었는지 확인\n",
    "    labels = sorted(example.get('labels', []), key=lambda x: x[0])\n",
    "    for start, end, label_name in labels:\n",
    "         # label_name 문자열을 해당하는 ID로 변환 (스팬 시작에 B- 태그 ID 사용)\n",
    "        label_id = label_to_id.get(f\"B-{label_name}\", 0) # 찾을 수 없는 경우 0('O')으로 기본값 설정\n",
    "        processed_labels.append([start, end, label_id]) # [start, end, LABEL_ID]로 저장\n",
    "    original_data_processed.append({\"text\": example['text'], \"labels\": processed_labels})\n",
    "\n",
    "\n",
    "N_augment = 5 # 증강할 샘플 수\n",
    "\n",
    "augmented_examples = []\n",
    "if original_data_processed: # 원본 데이터가 성공적으로 로드된 경우에만 증강\n",
    "    print(f\"총 {len(original_data_processed)}개의 샘플 중 {N_augment}개에 대해 데이터 증강을 시작합니다...\")\n",
    "\n",
    "    # 데이터 증강 실행 (텍스트 및 [start, end, LABEL_ID] 레이블 반환)\n",
    "    for i in tqdm(range(min(N_augment, len(original_data_processed)))):\n",
    "        augmented_example = augment_ner_example(original_data_processed[i], ko_en_model, ko_en_tokenizer, en_ko_model, en_ko_tokenizer, device)\n",
    "        augmented_examples.append(augmented_example)\n",
    "\n",
    "    print(\"\\n--- 데이터 증강 예시 ---\")\n",
    "    for i in range(min(5, len(augmented_examples))): # 최대 5개의 증강된 샘플 인쇄\n",
    "        print(f\"\\n[샘플 {i+1}]\")\n",
    "        print(\"원본 텍스트:\", original_data_processed[i]['text'])\n",
    "        print(\"증강 텍스트:\", augmented_examples[i]['text'])\n",
    "        # 인쇄 명확성을 위해 레이블 ID를 다시 이름으로 디코딩\n",
    "        decoded_original_labels = [[s, e, id_to_label[l_id]] for s, e, l_id in original_data_processed[i]['labels']]\n",
    "        decoded_augmented_labels = [[s, e, id_to_label[l_id]] for s, e, l_id in augmented_examples[i]['labels']]\n",
    "        print(\"원본 라벨 (디코딩):\", decoded_original_labels)\n",
    "        print(\"증강 라벨 (디코딩):\", decoded_augmented_labels)\n",
    "else:\n",
    "    print(\"원본 데이터를 로드하지 못하여 데이터 증강을 건너뜁니다.\")\n",
    "\n",
    "\n",
    "# --- 8. 증강된 데이터와 원본 데이터를 합치고 Hugging Face Dataset 형식으로 최종 변환 ---\n",
    "\n",
    "# 원본(ID로 처리됨) 및 증강된 데이터 결합\n",
    "combined_raw_data_processed = original_data_processed + augmented_examples\n",
    "\n",
    "# 8.1단계: 처리된 목록에서 초기 Hugging Face 데이터세트 생성\n",
    "# 'labels' 열에는 이제 [start, end, LABEL_ID] 목록이 포함됩니다.\n",
    "# 'labels'가 정수 ID의 시퀀스로 올바르게 처리되도록 여기서 기능을 정의합니다.\n",
    "# 정수 목록의 목록을 처리할 수 있는 구조가 필요합니다.\n",
    "# 목록에서 변환하기 전에 데이터세트의 기능을 정의합니다.\n",
    "dataset_features = Features({\n",
    "    'text': Value('string'),\n",
    "    'labels': Sequence(Sequence(Value('int32'))) # 레이블 ID에 int32 사용\n",
    "})\n",
    "\n",
    "\n",
    "# 정의된 기능을 사용하여 데이터세트 생성\n",
    "# Dataset.from_list는 기능을 유추할 수 있지만 명시적으로 정의하면 오류를 방지하는 데 도움이 됩니다.\n",
    "# 및 'labels'의 구조가 올바르게 해석되도록 합니다.\n",
    "try:\n",
    "    raw_hf_dataset = Dataset.from_list(combined_raw_data_processed, features=dataset_features)\n",
    "    print(f\"\\n임시 Hugging Face Dataset으로 변환된 샘플 수 (토큰화 전): {len(raw_hf_dataset)}\")\n",
    "    print(raw_hf_dataset[0]) # 구조 및 데이터 유형 확인\n",
    "except Exception as e:\n",
    "    print(f\"오류: 임시 Hugging Face Dataset 생성 중 오류 발생: {e}\")\n",
    "    # 종료 또는 오류 처리\n",
    "    pass\n",
    "\n",
    "\n",
    "# 8.2단계: 토큰화 및 레이블 정렬 기능 (Dataset.map()에서 사용)\n",
    "def tokenize_and_align_labels_for_dataset(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512, # 이전에 정의한 것과 동일한 max_length 사용\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    # examples[\"text\"]는 목록이고, examples[\"labels\"]는 [start, end, LABEL_ID] 목록의 목록입니다.\n",
    "    for batch_idx, (text, ner_tags_char_offsets_with_ids) in enumerate(zip(examples[\"text\"], examples[\"labels\"])):\n",
    "        # ner_tags_char_offsets_with_ids는 [[start, end, LABEL_ID], ...]입니다.\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_idx)\n",
    "        offset_mapping = tokenized_inputs[\"offset_mapping\"][batch_idx]\n",
    "\n",
    "        token_labels_ids = [-100] * len(word_ids) # 토큰 수준 레이블 ID 목록 초기화\n",
    "\n",
    "        # 각 토큰에 BIO 레이블 할당\n",
    "        # ner_tags_char_offsets_with_ids는 시작 인덱스별로 정렬된 것으로 가정합니다.\n",
    "        for token_idx, current_word_idx in enumerate(word_ids):\n",
    "            if current_word_idx is None: # 특수 토큰 (CLS, SEP, PAD)\n",
    "                token_labels_ids[token_idx] = -100\n",
    "            else: # 일반 단어 토큰\n",
    "                token_start_offset, token_end_offset = offset_mapping[token_idx]\n",
    "                current_token_label_id = 0 # 기본 레이블은 'O'(ID 0)입니다.\n",
    "\n",
    "                # 현재 토큰이 어떤 주석(엔터티)에 속하는지 확인\n",
    "                for ann_start, ann_end, ann_label_id in ner_tags_char_offsets_with_ids:\n",
    "                    # 토큰의 오프셋이 주석 범위 내에 완전히 포함되는 경우\n",
    "                    if ann_start <= token_start_offset and token_end_offset <= ann_end:\n",
    "                         # 토큰의 시작 오프셋이 주석의 시작 오프셋과 같으면 시작 토큰(B-)입니다.\n",
    "                        if ann_start == token_start_offset:\n",
    "                             # 이 label_id에 대한 B- 태그 ID 가져오기\n",
    "                            # 이 ID에 대한 B- 태그가 있는지 확인합니다(ID가 유효하면 항상 있어야 함).\n",
    "                            b_tag_label = id_to_label[ann_label_id] # ID에서 레이블 이름 가져오기\n",
    "                            if b_tag_label.startswith(\"B-\"): # 이미 B- 태그인지 확인(처리에서)\n",
    "                                current_token_label_id = ann_label_id # B- 태그 ID 유지\n",
    "                            else: # 이전 단계가 올바르지 않으면 발생하지 않아야 하며 기본값은 O입니다.\n",
    "                                current_token_label_id = 0\n",
    "                        else: # 그렇지 않으면 내부 토큰(I-)입니다.\n",
    "                            # 이 label_id에 대한 I- 태그 ID 가져오기\n",
    "                            # ner_tags_char_offsets_with_ids에 저장된 원래 label_id는 B- 태그 ID였습니다.\n",
    "                            # 해당 I- 태그 ID를 찾아야 합니다.\n",
    "                            b_tag_label_name = id_to_label[ann_label_id][2:] # 순수 레이블 이름 가져오기(예: 'QUESTION_ID')\n",
    "                            i_tag_label = f\"I-{b_tag_label_name}\" # I- 태그 이름 구성(예: 'I-QUESTION_ID')\n",
    "                            current_token_label_id = label_to_id.get(i_tag_label, 0) # I- 태그 ID를 가져오고, 찾을 수 없는 경우 기본값 O\n",
    "                        break # 주석을 찾았으므로 더 이상 검색할 필요가 없습니다.\n",
    "\n",
    "                token_labels_ids[token_idx] = current_token_label_id\n",
    "        labels.append(token_labels_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    # offset_mapping은 모델 학습에 필요하지 않으므로 제거합니다.\n",
    "    tokenized_inputs.pop(\"offset_mapping\")\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# 8.3단계: 토큰화 및 레이블 정렬을 위해 map() 함수 적용\n",
    "# 효율성을 위해 batch=True\n",
    "if 'raw_hf_dataset' in locals() and raw_hf_dataset is not None:\n",
    "    try:\n",
    "        final_hf_dataset = raw_hf_dataset.map(\n",
    "            tokenize_and_align_labels_for_dataset,\n",
    "            batched=True,\n",
    "            remove_columns=raw_hf_dataset.column_names # 원래 'text', 'labels' 열 제거\n",
    "        )\n",
    "\n",
    "        # 8.4단계: 최종 데이터세트 형식 설정\n",
    "        final_hf_dataset.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=['input_ids', 'attention_mask', 'labels']\n",
    "        )\n",
    "\n",
    "        print(f\"\\n최종 Hugging Face Dataset (토큰화 및 라벨 정렬 완료) 샘플 수: {len(final_hf_dataset)}\")\n",
    "        print(\"\\n최종 변환된 Hugging Face Dataset 첫 번째 샘플:\")\n",
    "        print(final_hf_dataset[0])\n",
    "\n",
    "        # 최종 학습/테스트 분할 (이전과 동일)\n",
    "        train_test_split_dataset = final_hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        train_dataset = train_test_split_dataset[\"train\"]\n",
    "        eval_dataset = train_test_split_dataset[\"test\"]\n",
    "\n",
    "        print(f\"\\n학습 데이터셋 샘플 수: {len(train_dataset)}\")\n",
    "        print(f\"평가 데이터셋 샘플 수: {len(eval_dataset)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류: 최종 Hugging Face Dataset 처리 중 오류 발생: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"임시 Hugging Face Dataset이 생성되지 않아 최종 처리를 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTM1wirwAe-n"
   },
   "source": [
    "### 5.5 예측 결과 분석 및 현재 모델의 한계\n",
    "\n",
    "1. NER 모델의 성능향상 기법을 사용하지 않고 Doccano 를 활용한 Maunual Labeling 만 진행했을 때\n",
    "\n",
    "    - **문제점:**\n",
    "\n",
    "        Doccano를 사용하여 수동으로 라벨링한 극소수의 데이터(15개, 전체 샘플의 약 5%)만으로 모델을 학습시켰을 때, 예측 결과는 실용성이 거의 없었습니다. 모델은 대부분의 토큰을 QUESTION_CONTENT 또는 O (Other) 태그로 분류하는 등, 개체명의 경계를 전혀 학습하지 못하는 모습을 보였습니다.\n",
    "\n",
    "    - **원인 분석:**\n",
    "\n",
    "        - 절대적인 데이터 부족: BERT와 같은 대규모 언어 모델이 복잡한 패턴을 학습하기에는 15개의 샘플은 턱없이 부족했습니다. 이로 인해 모델이 일반화에 실패하고 특정 레이블에 과도하게 편향되는 문제가 발생했습니다.\n",
    "\n",
    "        - 수동 라벨링의 비효율성: 수백, 수천 개의 데이터를 직접 라벨링하는 것은 시간과 비용 측면에서 비효율적인 접근 방식이었습니다.\n",
    "\n",
    "    - **해결 방안:**\n",
    "\n",
    "        이러한 한계를 극복하고 대량의 데이터에 자동으로 초기 라벨을 부여하기 위해, 규칙 기반의 약 지도 학습(Weakly Supervised Learning, WSL) 기법을 적용하기로 결정했습니다.\n",
    "\n",
    "\n",
    "2. 약 지도 학습(WSL) 적용 후 개선점과 새로운 한계\n",
    "\n",
    "    - **개선된 점:**\n",
    "        정규표현식(Regex) 기반의 WSL을 적용하자, 모델은 QUESTION_ID ('질의 1') 및 ANSWER_ID ('회신 1')와 같이 명확한 패턴을 가진 개체들을 매우 정확하게 예측하기 시작했습니다. 이를 통해 수백 개의 데이터에 대한 기본 라벨링을 자동화하여 학습 데이터의 양을 획기적으로 늘릴 수 있었습니다.\n",
    "\n",
    "\n",
    "    - **남아있는 한계:**\n",
    "        하지만 WSL은 규칙으로 정의하기 어려운 QUESTION_CONTENT, ANSWER_CONTENT, LAW_CONTENT와 같은 내용 기반의 개체명을 인식하는 데는 여전히 한계를 보였습니다. 문맥적 의미를 파악해야 하는 복잡한 개체명을 제대로 분류하지 못했으며, 이는 모델의 전반적인 성능 향상에 걸림돌이 되었습니다.\n",
    "\n",
    "\n",
    "    - **해결 방안:**\n",
    "        학습 데이터의 양은 늘었지만, 질적 다양성이 부족하다고 판단했습니다. 따라서 기존 데이터를 변형하여 새로운 학습 샘플처럼 사용하는 데이터 증강(Data Augmentation) 기법을 도입하기로 결정했습니다. 특히, 문장의 의미는 유지하면서 표현을 다양하게 바꾸는 역번역(Back-Translation) 기법을 활용하여 모델이 더 다양한 문맥을 학습하도록 유도하고자 했습니다.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXfJLHCvAe-o"
   },
   "source": [
    "### 6\\. 성능 향상을 위한 추가 단계 (결론)\n",
    "\n",
    "지금까지 데이터 전처리부터 라벨링, 학습, 추론에 이르는 개체명 인식(NER) 모델의 전체 파이프라인을 성공적으로 구축했습니다. 이 파이프라인은 모델 성능을 반복적으로 개선하기 위한 견고한 기반이 됩니다. 이제 구축된 파이프라인을 활용하여 모델의 성능을 본격적으로 향상시키는 데 집중할 차례입니다.\n",
    "\n",
    "#### 6.1 더 많은 고품질 데이터 확보 (가장 중요!)\n",
    "\n",
    "모델 성능 향상의 가장 핵심적인 요소는 **데이터의 양과 질**입니다. 현재 모델의 성능을 극대화하기 위해 가장 먼저 수행해야 할 작업은 Doccano를 통해 더 많은 데이터를 라벨링하는 것입니다.\n",
    "\n",
    "-   **목표 설정**: 초기 목표로 최소 수백 개, 안정적인 성능을 위해서는 수천 개 이상의 질의/회신 쌍을 라벨링하는 것을 권장합니다. 데이터가 많을수록 모델은 더 다양한 패턴을 학습하여 일반화 성능이 향상됩니다.\n",
    "    \n",
    "-   **다양성과 일관성 확보**:\n",
    "    \n",
    "    -   **다양성**: 단순히 데이터의 양을 늘리는 것뿐만 아니라, 다양한 문맥과 표현을 포함하는 데이터를 라벨링해야 합니다. 예를 들어, '소방시설법'이라는 개체명이 여러 법령 조항과 결합되는 다양한 사례를 학습시켜야 합니다.\n",
    "        \n",
    "    -   **일관성**: 라벨링 규칙을 명확히 정의하고, 모든 데이터에 일관되게 적용하는 것이 매우 중요합니다. 일관성이 떨어지는 데이터는 모델 학습에 혼란을 주어 성능 저하의 원인이 됩니다.\n",
    "        \n",
    "-   **효율적인 라벨링을 위한 Active Learning 도입 (심화)**:\n",
    "    \n",
    "    -   모든 비라벨링 데이터를 무작위로 라벨링하는 것은 비효율적일 수 있습니다. 대신, 현재 학습된 모델을 활용하여 **Active Learning**을 도입하는 것을 고려할 수 있습니다.\n",
    "        \n",
    "    -   **프로세스**:\n",
    "        \n",
    "        1.  현재 모델을 사용하여 라벨링되지 않은 대량의 데이터에 대해 예측을 수행합니다.\n",
    "            \n",
    "        2.  모델이 예측을 가장 **불확실하게(low confidence)** 내놓은 샘플들을 식별합니다.\n",
    "            \n",
    "        3.  이 불확실한 샘플들을 Doccano로 임포트하여 우선적으로 라벨링합니다.\n",
    "            \n",
    "    -   **기대효과**: 이 방식은 모델이 가장 학습하기 어려워하는 데이터에 집중하게 하므로, 적은 노력으로 모델 성능을 효율적으로 끌어올릴 수 있습니다.\n",
    "        \n",
    "\n",
    "#### 6.2 하이퍼파라미터 튜닝\n",
    "\n",
    "충분한 양의 데이터가 확보되었다면, 모델의 학습 과정을 미세 조정하여 성능을 최적화할 수 있습니다. `4.3 학습 인자(Training Arguments) 및 트레이너(Trainer) 설정` 섹션의 주요 하이퍼파라미터를 조정하며 최적의 조합을 찾아야 합니다.\n",
    "\n",
    "-   **`learning_rate`**: 너무 높으면 학습이 불안정하고, 너무 낮으면 학습이 더디거나 최적점에 도달하지 못할 수 있습니다. 2e-5에서 시작하여 1e-5, 3e-5 등 미세 조정을 시도해볼 수 있습니다.\n",
    "    \n",
    "-   **`num_train_epochs`**: 데이터셋을 몇 번 반복하여 학습할지를 결정합니다. 데이터가 충분할 경우 에포크를 늘리면 성능이 향상될 수 있으나, 너무 많으면 과적합(Overfitting)의 위험이 있습니다. `TrainingArguments`에 `early_stopping_patience`와 같은 콜백을 추가하여 검증 성능이 더 이상 오르지 않으면 학습을 조기 종료하는 전략이 유용합니다.\n",
    "    \n",
    "-   **`per_device_train_batch_size`**: 배치 크기는 학습 안정성과 속도에 영향을 줍니다. GPU 메모리가 허용하는 범위 내에서 다양한 크기(예: 8, 16, 32)를 테스트해볼 수 있습니다.\n",
    "    \n",
    "-   **`weight_decay`**: 과적합을 방지하기 위한 정규화 기법으로, 모델의 가중치가 너무 커지는 것을 막아줍니다.\n",
    "    \n",
    "\n",
    "#### 6.3 모델 아키텍처 및 전이 학습 전략 고도화\n",
    "\n",
    "-   **최신 또는 도메인 특화 모델 활용**: 현재 사용 중인 `klue/bert-base`는 한국어 자연어 처리에서 매우 강력한 기준 모델입니다. 하지만 더 최신의 모델이나 법률과 같은 특정 도메인에 특화된 사전 학습 모델이 있다면, 해당 모델로 교체하여 성능을 비교해볼 수 있습니다. (단, 한국어 법률 특화 공개 모델은 제한적일 수 있습니다.)\n",
    "    \n",
    "-   **모델 앙상블 (Ensemble)**: 단일 모델의 예측에 의존하기보다, 여러 모델의 예측 결과를 결합하여 최종 결정을 내리는 앙상블 기법을 고려할 수 있습니다. 예를 들어, 서로 다른 시드(seed) 값으로 학습된 3~5개의 동일한 모델의 예측 결과를 다수결 투표 방식으로 합치면 단일 모델보다 더 안정적이고 높은 성능을 기대할 수 있습니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kn8YEB_aAe-o"
   },
   "source": [
    "## 7. 프로젝트 회고 및 배운 점\n",
    "\n",
    "이번 프로젝트를 통해 딥러닝 모델을 실제 업무에 적용하는 전체 과정을 경험하며 많은 것을 배우고 느꼈습니다. 단순히 이론으로만 알던 개념들을 직접 부딪히고 해결하며 얻은 교훈들을 정리했습니다.\n",
    "\n",
    "### 기술적 성장 및 경험\n",
    "\n",
    "- **클라우드 기반 GPU 활용의 필요성:** 딥러닝 모델 학습처럼 대규모 연산이 필요한 작업은 일반 로컬 PC 환경에서 수행하기 어렵다는 것을 체감했습니다. **Google Colab**이 제공하는 무료 **T4 GPU**는 이러한 제약을 극복하고, 비용 효율적으로 모델을 학습하고 실험할 수 있는 훌륭한 대안이었습니다.\n",
    "\n",
    "- **Docker와 컨테이너 환경 경험:** 데이터 라벨링 도구인 **Doccano**를 설치하고 실행하기 위해 **Docker**를 처음 사용해보았습니다. 이를 통해 애플리케이션을 격리된 환경에서 손쉽게 배포하고 실행하는 컨테이너 기술의 강력함을 이해하게 되었고, 복잡한 설치 과정 없이 필요한 도구를 빠르게 구축하는 경험을 쌓을 수 있었습니다.\n",
    "\n",
    "- **엔드-투-엔드(End-to-End) 파이프라인 구축:** 데이터 전처리부터 라벨링, 모델 학습, 평가, 그리고 추론에 이르기까지의 전체 머신러닝 파이프라인을 직접 설계하고 구축했습니다. 각 단계가 어떻게 유기적으로 연결되는지, 그리고 각 단계에서 어떤 점을 고려해야 하는지에 대한 실질적인 이해를 높일 수 있었습니다.\n",
    "\n",
    "### 딥러닝 모델과 데이터에 대한 깊은 이해\n",
    "\n",
    "- **'Garbage In, Garbage Out'의 실감:** 모델의 성능은 결국 데이터의 양과 질에 의해 결정된다는 것을 뼈저리게 느꼈습니다. 특히, **라벨링된 데이터 샘플의 수가 많을수록 모델의 정확도가 비례하여 향상**되는 것을 직접 확인했습니다. 초기 단계에서 적은 수의 샘플로 학습했을 때 모델이 거의 작동하지 않았던 경험은 양질의 데이터 확보가 얼마나 중요한지 깨닫게 해주었습니다.\n",
    "\n",
    "- **사전 학습 모델(Pre-trained Model)의 위력:** `klue/bert-base`와 같은 사전 학습된 모델을 기반으로 파인튜닝하는 것이 왜 효율적인지를 이해했습니다. 밑바닥부터 모든 것을 학습시키는 대신, 이미 방대한 한국어 데이터를 학습한 모델을 활용함으로써 비교적 적은 데이터로도 특정 도메인의 작업을 수행할 수 있다는 전이 학습의 개념을 실제로 적용해볼 수 있었습니다.\n",
    "\n",
    "### 현실적인 한계와 성과\n",
    "\n",
    "- **고품질 학습 데이터셋 구축의 어려움:** 지도 학습(Supervised Learning) 기반의 NER 모델을 훈련시키기 위해 2만 개의 모든 데이터를 라벨링할 필요는 없습니다. 핵심은 모델이 전체 데이터의 패턴을 학습할 수 있을 만큼, **충분한 양의 대표적인 샘플들을 고품질로 라벨링**하는 것입니다. 일단 모델이 잘 학습되면, 나머지 라벨링되지 않은 데이터는 모델이 자동으로 처리해줄 수 있습니다. 하지만, 원하는 성능을 내기 위한 '충분한 양의 샘플'을 만드는 것 자체가 혼자서는 매우 힘든 작업이었습니다. 이로 인해 100% 완벽한 정확도를 가진 모델을 만들지는 못했지만, 지도 학습의 핵심 원리와 데이터의 중요성을 체감하는 계기가 되었습니다.\n",
    "\n",
    "- **그럼에도 불구하고, 성공적인 자동화:** 비록 모델이 완벽하지는 않았지만, 이 프로젝트를 통해 개발한 자동화된 파싱 시스템은 기존의 **수작업으로 질의-회신 쌍을 검증하던 방식에 비해 업무 효율을 압도적으로 향상**시켰습니다. 반복적인 작업을 자동화함으로써 시간을 절약하고, 더 중요한 분석 작업에 집중할 수 있게 되었다는 점에서 이 프로젝트는 매우 성공적이었습니다. 이는 '완벽함'을 추구하기보다 '개선'을 목표로 하는 것의 중요성을 일깨워 주었습니다.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}