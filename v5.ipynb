{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **í”„ë¡œì íŠ¸ ê°œìš”**\n",
    "ì´ í”„ë¡œì íŠ¸ëŠ” í•œêµ­ì–´ ë²•ë ¹ ì§ˆì˜íšŒì‹  í…ìŠ¤íŠ¸ì—ì„œ 'ì§ˆì˜ ID', 'ë‹µë³€ ë‚´ìš©' ë“± íŠ¹ì • ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER) ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤. `klue/bert-base` ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.\n",
    "\n",
    "### **í•™ìŠµ ëª©í‘œ**\n",
    "1.  **ë°ì´í„° ì¤€ë¹„**: ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³ , ë¼ë²¨ë§ ë„êµ¬(Doccano)ì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•©ë‹ˆë‹¤.\n",
    "2.  **ë°ì´í„° ë¼ë²¨ë§**: Doccanoë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì— ê°œì²´ëª… íƒœê·¸ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "3.  **ê³ ê¸‰ ê¸°ë²• ì ìš©**: ì•½ ì§€ë„ í•™ìŠµ, ë°ì´í„° ì¦ê°•, ëŠ¥ë™ í•™ìŠµ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
    "4.  **ëª¨ë¸ í•™ìŠµ ë° í‰ê°€**: ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ BERT ëª¨ë¸ í•™ìŠµì— ë§ê²Œ ë³€í™˜í•˜ê³ , ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•œ í›„ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1. í™˜ê²½ ì„¤ì •**\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  Google Driveë¥¼ ì—°ë™í•˜ì—¬ í”„ë¡œì íŠ¸ í™˜ê²½ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install transformers[torch] datasets evaluate seqeval accelerate -q\n",
    "\n",
    "# 1.2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1.3. Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# 1.4. í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "project_root = '/content/gdrive/MyDrive/Colab Notebooks/deep-learning-ner-advanced/'\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "model_dir = os.path.join(project_root, 'model')\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}\")\n",
    "print(f\"ë°ì´í„° í´ë”: {data_dir}\")\n",
    "print(f\"ëª¨ë¸ ì €ì¥ í´ë”: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **2. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬**\n",
    "ëª¨ë¸ í•™ìŠµì˜ ê¸°ë°˜ì´ ë˜ëŠ” ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³ , ë¼ë²¨ë§ì„ ìœ„í•œ í˜•íƒœë¡œ ê°€ê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1. ì›ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì¤€ë¹„**\n",
    "í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•  ì›ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼(`dataset.txt`)ê³¼, ì´ íŒŒì¼ì„ ê°„ë‹¨íˆ ì •ì œí•œ `dataset_cleaned.txt` íŒŒì¼ì´ `data` í´ë”ì— ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "`dataset_cleaned.txt`ëŠ” ê° ì§ˆì˜-íšŒì‹  ìŒì´ ë‘ ê°œì˜ ê°œí–‰ ë¬¸ì(`\\n\\n`)ë¡œ êµ¬ë¶„ëœ íŒŒì¼ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(New) 2.2. ì•½ ì§€ë„ í•™ìŠµ (Weak Supervision)ìœ¼ë¡œ ë¼ë²¨ë§ ê°€ì†í™”** ğŸš€\n",
    "\n",
    "ìˆ˜ì²œ ê°œì˜ ë°ì´í„°ë¥¼ ì²˜ìŒë¶€í„° ìˆ˜ë™ìœ¼ë¡œ ë¼ë²¨ë§í•˜ëŠ” ê²ƒì€ ë§¤ìš° í˜ë“  ì‘ì—…ì…ë‹ˆë‹¤. **ì•½ ì§€ë„ í•™ìŠµ**ì€ ì •ê·œ í‘œí˜„ì‹(Regex)ê³¼ ê°™ì€ ê°„ë‹¨í•œ **ê·œì¹™(Heuristics)ì„ ì´ìš©í•´ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì— ìë™ìœ¼ë¡œ ë¼ë²¨ì„ ë¶€ì—¬**í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë ‡ê²Œ ìƒì„±ëœ 'ë¶€ì •í™•í•˜ì§€ë§Œ ì–‘ì´ ë§ì€' ë¼ë²¨ ë°ì´í„°ëŠ” ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "1.  **ì´ˆë²Œ ë¼ë²¨ë§ ë°ì´í„°ë¡œ ì‚¬ìš©**: ìë™ ìƒì„±ëœ `weakly_labeled.jsonl` íŒŒì¼ì„ Doccanoì— ì„í¬íŠ¸í•˜ë©´, ì‚¬ëŒì€ ì²˜ìŒë¶€í„° ë¼ë²¨ë§í•˜ëŠ” ëŒ€ì‹  **í‹€ë¦° ë¶€ë¶„ë§Œ ë¹ ë¥´ê²Œ ìˆ˜ì •**í•˜ë©´ ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¼ë²¨ë§ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "2.  **ëª¨ë¸ ì‚¬ì „ í•™ìŠµ**: ì–‘ì§ˆì˜ ìˆ˜ë™ ë¼ë²¨ ë°ì´í„°ê°€ ì•„ì£¼ ì ì„ ê²½ìš°, ì•½ ì§€ë„ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œí‚¨ í›„, ì†ŒëŸ‰ì˜ ê³ í’ˆì§ˆ ë°ì´í„°ë¡œ ì¶”ê°€ íŒŒì¸íŠœë‹í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” 1ë²ˆ ë°©ë²•ì„ ìœ„í•´ `weakly_labeled.jsonl` íŒŒì¼ì„ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weak_labels(input_text_path, output_jsonl_path):\n",
    "    \"\"\"ì •ê·œì‹ì„ ì‚¬ìš©í•´ 'ì§ˆì˜ ID'ì™€ 'ë‹µë³€ ID'ë¥¼ ìë™ìœ¼ë¡œ ë¼ë²¨ë§í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        with open(input_text_path, 'r', encoding='utf-8') as f:\n",
    "            # ê° ë¬¸ì„œëŠ” ë‘ ê°œì˜ ê°œí–‰ ë¬¸ìë¡œ êµ¬ë¶„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •\n",
    "            documents = f.read().split('\\n\\n')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: '{input_text_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    # 'ì§ˆì˜' ë˜ëŠ” 'íšŒì‹ ' ë’¤ì— ìˆ«ìì™€ ì (ì„ íƒ)ì´ ì˜¤ëŠ” íŒ¨í„´\n",
    "    question_id_pattern = re.compile(r'(ì§ˆì˜\\s*\\d+\\.?)')\n",
    "    answer_id_pattern = re.compile(r'(íšŒì‹ \\s*\\d+\\.?)')\n",
    "    \n",
    "    labeled_docs = []\n",
    "    for doc_text in documents:\n",
    "        if not doc_text.strip():\n",
    "            continue\n",
    "\n",
    "        spans = []\n",
    "        # QUESTION_ID ë¼ë²¨ë§\n",
    "        for match in question_id_pattern.finditer(doc_text):\n",
    "            start, end = match.span()\n",
    "            spans.append([start, end, \"QUESTION_ID\"])\n",
    "\n",
    "        # ANSWER_ID ë¼ë²¨ë§\n",
    "        for match in answer_id_pattern.finditer(doc_text):\n",
    "            start, end = match.span()\n",
    "            spans.append([start, end, \"ANSWER_ID\"])\n",
    "\n",
    "        # Doccano JSONL í˜•ì‹\n",
    "        labeled_docs.append({\n",
    "            \"text\": doc_text,\n",
    "            \"labels\": spans\n",
    "        })\n",
    "\n",
    "    # íŒŒì¼ë¡œ ì €ì¥\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for doc in labeled_docs:\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"ì•½ ì§€ë„ í•™ìŠµ ì™„ë£Œ! {len(labeled_docs)}ê°œì˜ ë¬¸ì„œê°€ '{output_jsonl_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ì´ íŒŒì¼ì„ Doccanoì— ì„í¬íŠ¸í•˜ì—¬ ë¼ë²¨ì„ ê²€í† í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "cleaned_text_path = os.path.join(data_dir, 'dataset_cleaned.txt')\n",
    "weakly_labeled_path = os.path.join(data_dir, 'weakly_labeled.jsonl')\n",
    "create_weak_labels(cleaned_text_path, weakly_labeled_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3. Doccanoë¥¼ ì´ìš©í•œ ë°ì´í„° ë¼ë²¨ë§**\n",
    "1.  **Doccano ì‹¤í–‰**: ë¡œì»¬ PCì—ì„œ Dockerë¥¼ ì´ìš©í•´ Doccanoë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "2.  **ë°ì´í„° ì„í¬íŠ¸**: ìœ„ì—ì„œ ìƒì„±í•œ `weakly_labeled.jsonl` íŒŒì¼ì„ `JSONL` í˜•ì‹ìœ¼ë¡œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "3.  **ë¼ë²¨ë§ ê²€í†  ë° ìˆ˜ì •**: ìë™ ë¼ë²¨ë§ëœ ê²°ê³¼ë¥¼ ê²€í† í•˜ê³ , ë‚˜ë¨¸ì§€ `QUESTION_CONTENT`, `ANSWER_CONTENT` ë“±ì„ ìˆ˜ë™ìœ¼ë¡œ ë¼ë²¨ë§í•©ë‹ˆë‹¤.\n",
    "4.  **ë°ì´í„° ìµìŠ¤í¬íŠ¸**: ë¼ë²¨ë§ì´ ì™„ë£Œë˜ë©´ `JSONL` í˜•ì‹ìœ¼ë¡œ, **'Export only approved documents'ë¥¼ ì²´í¬**í•˜ì—¬ ë‚´ë³´ëƒ…ë‹ˆë‹¤.\n",
    "5.  **íŒŒì¼ ì—…ë¡œë“œ**: ìµœì¢… ê²°ê³¼ë¬¼(ì˜ˆ: `final_labeled_data.jsonl`)ì„ Colabì˜ `data` í´ë”ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4. Colabì—ì„œ ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ ë° ê°€ê³µ**\n",
    "ìµœì¢… ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `klue/bert-base` í† í¬ë‚˜ì´ì €ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1. ë¼ë²¨ ë° í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "doccano_labels = [\n",
    "    \"QUESTION_ID\", \"QUESTION_CONTENT\", \"ANSWER_ID\", \n",
    "    \"ANSWER_CONTENT\", \"LAW_NAME\", \"MISC_HEADER\"\n",
    "]\n",
    "\n",
    "label_list = [\"O\"] + [f\"{tag}-{label}\" for label in doccano_labels for tag in (\"B\", \"I\")]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "print(\"ì „ì²´ ë¼ë²¨ ëª©ë¡:\", label_list)\n",
    "\n",
    "# 2.4.2. Doccano JSONL íŒŒì¼ ë¡œë“œ\n",
    "labeled_data_filename = 'final_labeled_data.jsonl' # Doccanoì—ì„œ ìµœì¢… ìµìŠ¤í¬íŠ¸í•œ íŒŒì¼\n",
    "labeled_data_filepath = os.path.join(data_dir, labeled_data_filename)\n",
    "\n",
    "try:\n",
    "    doccano_data = []\n",
    "    with open(labeled_data_filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            annotations = item.pop('label', item.pop('labels', []))\n",
    "            item['labels'] = annotations\n",
    "            doccano_data.append(item)\n",
    "            \n",
    "    raw_dataset = Dataset.from_list(doccano_data)\n",
    "    print(f\"\\në¡œë“œëœ ë¼ë²¨ë§ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(raw_dataset)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{labeled_data_filepath}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„ê³¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    # ì˜ˆì‹œë¥¼ ìœ„í•´ ë¹ˆ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    raw_dataset = Dataset.from_dict({'text':[], 'labels':[]})\n",
    "\n",
    "# 2.4.3. í† í°í™” ë° ë¼ë²¨ ì •ë ¬ í•¨ìˆ˜\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, max_length=512, return_offsets_mapping=True)\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, doc_labels in enumerate(examples[\"labels\"]):\n",
    "        offset_mapping = tokenized_inputs[\"offset_mapping\"][i]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        current_labels = np.full(len(offset_mapping), label_to_id['O'])\n",
    "\n",
    "        for start, end, label in doc_labels:\n",
    "            b_label_id = label_to_id.get(f\"B-{label}\")\n",
    "            i_label_id = label_to_id.get(f\"I-{label}\")\n",
    "            if b_label_id is None: continue\n",
    "\n",
    "            token_indices = [idx for idx, (o_start, o_end) in enumerate(offset_mapping) if o_start >= start and o_end <= end]\n",
    "            \n",
    "            if token_indices:\n",
    "                current_labels[token_indices[0]] = b_label_id\n",
    "                for idx in token_indices[1:]:\n",
    "                    current_labels[idx] = i_label_id\n",
    "\n",
    "        final_labels = [-100 if word_id is None else label_id for word_id, label_id in zip(word_ids, current_labels)]\n",
    "        aligned_labels.append(final_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3. ëª¨ë¸ í•™ìŠµ**\n",
    "ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œë˜ë©´ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. ë°ì´í„° ì¦ê°•ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ì˜ ì–‘ì„ ëŠ˜ë ¤ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(New) 3.1. ë°ì´í„° ì¦ê°• (Data Augmentation)ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° í™•ì¥** ğŸª„\n",
    "\n",
    "ë°ì´í„° ì¦ê°•ì€ **ê¸°ì¡´ ë°ì´í„°ë¥¼ ì•½ê°„ ë³€í˜•í•˜ì—¬ ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±**í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ë°ì´í„° ì–‘ì´ ë¶€ì¡±í•  ë•Œ ëª¨ë¸ì´ ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ê²Œ í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤. \n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” **ì—­ë²ˆì—­(Back-translation)**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "1.  ì›ë³¸ í•œêµ­ì–´ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.\n",
    "2.  ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì¥ì„ ë‹¤ì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê³¼ì •ì„ ê±°ì¹˜ë©´ ì›ë³¸ê³¼ ì˜ë¯¸ëŠ” ê°™ì§€ë§Œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ êµ¬ì¡°ê°€ ë¯¸ë¬˜í•˜ê²Œ ë‹¤ë¥¸ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” Hugging Faceì— ê³µê°œëœ ë²ˆì—­ ëª¨ë¸(`Helsinki-NLP/opus-mt-ko-en`, `Helsinki-NLP/opus-mt-en-ko`)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.1. ì—­ë²ˆì—­ì„ ìœ„í•œ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "ko_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ko\")\n",
    "ko_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ko\")\n",
    "\n",
    "def back_translate(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•œ ë’¤ ë‹¤ì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # í•œêµ­ì–´ -> ì˜ì–´\n",
    "    en_tokenized = en_tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    en_outputs = en_model.generate(**en_tokenized)\n",
    "    en_text = en_tokenizer.decode(en_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ì˜ì–´ -> í•œêµ­ì–´\n",
    "    ko_tokenized = ko_tokenizer(en_text, return_tensors=\"pt\", truncation=True)\n",
    "    ko_outputs = ko_model.generate(**ko_tokenized)\n",
    "    ko_text = ko_tokenizer.decode(ko_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return ko_text\n",
    "\n",
    "# 3.1.2. ë°ì´í„° ì¦ê°• í•¨ìˆ˜ (ì£¼ì˜: ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
    "def augment_data(dataset):\n",
    "    augmented_texts = []\n",
    "    print(\"ë°ì´í„° ì¦ê°•(ì—­ë²ˆì—­)ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    for example in tqdm(dataset):\n",
    "        # ì›ë³¸ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , ì—­ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€\n",
    "        original_text = example['text']\n",
    "        # ì—­ë²ˆì—­ ì‹¤í–‰ (API í˜¸ì¶œê³¼ ìœ ì‚¬í•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦¼)\n",
    "        # translated_text = back_translate(original_text)\n",
    "        \n",
    "        # NOTE: ì—­ë²ˆì—­ì€ ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë³€í˜•ìœ¼ë¡œ ëŒ€ì²´ ì‹œì—°í•©ë‹ˆë‹¤.\n",
    "        # ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ìœ„ì˜ back_translate í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "        # ì˜ˆì‹œ: ë¬¸ì¥ ëì— ê³µë°±ê³¼ ë§ˆì¹¨í‘œë¥¼ ì¶”ê°€í•˜ëŠ” ê°„ë‹¨í•œ ì¦ê°•\n",
    "        augmented_text = original_text + \" .\"\n",
    "\n",
    "        # ì¦ê°•ëœ ë°ì´í„° ì¶”ê°€\n",
    "        # ì¦ê°• í›„ì—ëŠ” ë¼ë²¨ì˜ offsetì´ ë³€ê²½ë˜ë¯€ë¡œ, ë¼ë²¨ì„ 'None'ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ \n",
    "        # ë‚˜ì¤‘ì— ì¬ë¼ë²¨ë§í•˜ê±°ë‚˜, offsetì„ ì¬ê³„ì‚°í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "        # ì—¬ê¸°ì„œëŠ” ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœë¡œ, í…ìŠ¤íŠ¸ë§Œ ì¦ê°•í•©ë‹ˆë‹¤.\n",
    "        # ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì¦ê°•ëœ í…ìŠ¤íŠ¸ì— ë§ê²Œ ë¼ë²¨ì„ ì¬ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        augmented_texts.append({'text': augmented_text, 'labels': example['labels']})\n",
    "    \n",
    "    # ê¸°ì¡´ ë°ì´í„°ì™€ ì¦ê°• ë°ì´í„°ë¥¼ í•©ì¹¨\n",
    "    augmented_dataset = Dataset.from_list(dataset.to_list() + augmented_texts)\n",
    "    return augmented_dataset\n",
    "\n",
    "# 3.1.3. ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ì¦ê°• ì ìš©\n",
    "if 'raw_dataset' in locals() and len(raw_dataset) > 0:\n",
    "    # ë°ì´í„° ì¦ê°• ì ìš© (í•™ìŠµ ë°ì´í„°ì—ë§Œ ì ìš©)\n",
    "    # augmented_train_dataset = augment_data(raw_dataset)\n",
    "    # print(f\"ì¦ê°• í›„ ì „ì²´ ë°ì´í„° ìˆ˜: {len(augmented_train_dataset)}\")\n",
    "    # processed_dataset = augmented_train_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=augmented_train_dataset.column_names)\n",
    "\n",
    "    # ì¦ê°• ì—†ì´ ì§„í–‰ (ì‹œê°„ ê´€ê³„ìƒ)\n",
    "    print(\"ë°ì´í„° ì¦ê°•ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ê±´ë„ˆë›°ê³  ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    processed_dataset = raw_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=raw_dataset.column_names)\n",
    "    \n",
    "    # í•™ìŠµìš©/í‰ê°€ìš© ë°ì´í„°ì…‹ ë¶„í• \n",
    "    train_test_split = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test_split['train']\n",
    "    eval_dataset = train_test_split['test']\n",
    "\n",
    "    print(\"\\në°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í•  ì™„ë£Œ:\")\n",
    "    print(f\"í•™ìŠµ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}\")\n",
    "    print(f\"í‰ê°€ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.1. ëª¨ë¸ ë¡œë“œ\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"klue/bert-base\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# 3.2.2. ì„±ëŠ¥ ì§€í‘œ ì •ì˜\n",
    "seqeval_metric = evaluate.load(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [[id_to_label[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[id_to_label[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    results = seqeval_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "\n",
    "# 3.2.3. í•™ìŠµ ì¸ì ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8, # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 4, 8 ë“±ìœ¼ë¡œ ì¡°ì ˆ\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 3.2.4. Trainer ì„¤ì • ë° í•™ìŠµ ì‹œì‘\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "if len(train_dataset) > 0:\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(model_dir, \"best_model\"))\n",
    "    print(f\"\\ní•™ìŠµ ì™„ë£Œ! ìµœì  ëª¨ë¸ì´ '{os.path.join(model_dir, 'best_model')}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"í•™ìŠµ ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆì–´ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. ëª¨ë¸ ì¶”ë¡  (Inference)**\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª…ì„ ì¶”ì¶œí•˜ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
    "model_path = os.path.join(model_dir, \"best_model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    inference_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    inference_model = AutoModelForTokenClassification.from_pretrained(model_path).to(device)\n",
    "    print(f\"ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ({model_path})\")\n",
    "except OSError:\n",
    "    print(f\"ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. '{model_path}' ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì¶”ë¡ ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    inference_model = None\n",
    "\n",
    "# 4.2. ì¶”ë¡  í•¨ìˆ˜\n",
    "def predict_ner(text, tokenizer, model):\n",
    "    if model is None: return []\n",
    "    model.eval()\n",
    "    tokenized_input = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0])\n",
    "    predicted_labels = [model.config.id2label[i] for i in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    results = []\n",
    "    current_entity_tokens = []\n",
    "    current_entity_label = None\n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token in tokenizer.all_special_tokens: continue\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity_tokens: results.append({\"entity\": tokenizer.convert_tokens_to_string(current_entity_tokens), \"label\": current_entity_label})\n",
    "            current_entity_tokens = [token]\n",
    "            current_entity_label = label[2:]\n",
    "        elif label.startswith(\"I-\") and current_entity_label == label[2:]:\n",
    "            current_entity_tokens.append(token)\n",
    "        else:\n",
    "            if current_entity_tokens: results.append({\"entity\": tokenizer.convert_tokens_to_string(current_entity_tokens), \"label\": current_entity_label})\n",
    "            current_entity_tokens = []\n",
    "            current_entity_label = None\n",
    "    if current_entity_tokens: results.append({\"entity\": tokenizer.convert_tokens_to_string(current_entity_tokens), \"label\": current_entity_label})\n",
    "    return results\n",
    "\n",
    "# 4.3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_text = \"ì§ˆì˜ 1 ì—°ë©´ì  450ã¡ì¸ íŠ¹ì •ì†Œë°©ëŒ€ìƒë¬¼ì— ìµœì´ˆ ê±´ì¶•ë¬¼ ì‚¬ìš©ìŠ¹ì¸ì‹œì— ë¹„ìƒê²½ë³´ì„¤ë¹„ ì„¤ì¹˜ê°€ ë˜ì§€ ì•Šì€ ê²½ìš° ê±´ì¶•í—ˆê°€ì¼ê³¼ ì‚¬ìš©ìŠ¹ì¸ì¼ ì¤‘ ì†Œë°©ì‹œì„¤ì„¤ì¹˜ê¸°ì¤€ ì ìš©ì¼ì€? íšŒì‹  1 ê±´ì¶•ë¬¼ ë“±ì˜ ì‹ ì¶•ã†ì¦ì¶•ã†ê°œì¶•ã†ì¬ì¶•ã†ì´ì „ã†ìš©ë„ë³€ê²½ ë˜ëŠ” ëŒ€ìˆ˜ì„ ì˜ í—ˆê°€ ã†í˜‘ì˜ ë° ì‚¬ìš©ìŠ¹ì¸ì˜ ê¶Œí•œì´ ìˆëŠ” í–‰ì •ê¸°ê´€ì€ ì†Œë°©ì‹œì„¤ë²• ì œ7ì¡°ì œ1í•­ì— ë”°ë¼ ì†Œì¬ì§€ë¥¼ ê´€í• í•˜ëŠ” ì†Œë°©ë³¸ë¶€ì¥ì´ë‚˜ ì†Œë°©ì„œì¥ì˜ ë™ì˜ë¥¼ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "predicted_entities = predict_ner(test_text, inference_tokenizer, inference_model)\n",
    "\n",
    "print(\"\\n--- ì¶”ë¡  ê²°ê³¼ ---\")\n",
    "for entity in predicted_entities:\n",
    "    print(f\"- {entity['entity']} ({entity['label']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **(New) 5. ëŠ¥ë™ í•™ìŠµ (Active Learning)ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ë¼ë²¨ë§** ğŸ§ \n",
    "\n",
    "**ëŠ¥ë™ í•™ìŠµ**ì€ ëª¨ë¸ ê°œì„ ì„ ìœ„í•´ **ì–´ë–¤ ë°ì´í„°ë¥¼ ë¼ë²¨ë§í•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì ì¼ì§€ ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ íŒë‹¨**í•˜ê²Œ í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ìˆ˜ì²œ ê°œì˜ unlabeled ë°ì´í„° ì¤‘ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì— ê°€ì¥ ë„ì›€ì´ ë  ë§Œí•œ ë°ì´í„°ë¥¼ ê³¨ë¼ ì‚¬ëŒì—ê²Œ ë¼ë²¨ë§ì„ ìš”ì²­í•¨ìœ¼ë¡œì¨, ìµœì†Œí•œì˜ ë…¸ë ¥ìœ¼ë¡œ ìµœëŒ€ì˜ ì„±ëŠ¥ í–¥ìƒì„ ê¾€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "#### **Active Learning ìˆœí™˜ ê³¼ì •**\n",
    "1.  ì†ŒëŸ‰ì˜ ì´ˆê¸° ë°ì´í„°(`seed data`)ë¡œ 1ì°¨ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "2.  í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¼ë²¨ì´ ì—†ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°(`unlabeled pool`)ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "3.  ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ê°€ì¥ **\"ì–´ë ¤ì›Œí•˜ëŠ”\"**, ì¦‰ **ë¶ˆí™•ì‹¤ì„±(Uncertainty)ì´ ë†’ì€** ë°ì´í„°ë¥¼ ìƒìœ„ Nê°œ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "4.  ì„ íƒëœ ë°ì´í„°ë¥¼ ì‚¬ëŒì´ ë¼ë²¨ë§í•©ë‹ˆë‹¤ (Doccano ì‚¬ìš©).\n",
    "5.  ìƒˆë¡­ê²Œ ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì— ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì„ ì¬í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "6.  ì›í•˜ëŠ” ì„±ëŠ¥ì— ë„ë‹¬í•  ë•Œê¹Œì§€ 2~5ë²ˆ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ëª¨ë¸ ì˜ˆì¸¡ì˜ **ì—”íŠ¸ë¡œí”¼(Entropy)**ë¥¼ ë¶ˆí™•ì‹¤ì„±ì˜ ì²™ë„ë¡œ ì‚¬ìš©í•˜ì—¬, ë¼ë²¨ë§ì´ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì°¾ëŠ” ë°©ë²•ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_scores(texts, model, tokenizer):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ëª©ë¡ì„ ì…ë ¥ë°›ì•„ ê° í…ìŠ¤íŠ¸ì˜ ë¶ˆí™•ì‹¤ì„±(í‰ê·  ì—”íŠ¸ë¡œí”¼) ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if model is None: return [], []\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    \n",
    "    print(\"ë¼ë²¨ ì—†ëŠ” ë°ì´í„°ì— ëŒ€í•´ ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...\")\n",
    "    for text in tqdm(texts):\n",
    "        tokenized_input = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**tokenized_input).logits\n",
    "        \n",
    "        # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        # ê° í† í°ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
    "        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-9), dim=-1)\n",
    "        # ë¬¸ì¥ì˜ í‰ê·  ì—”íŠ¸ë¡œí”¼ë¥¼ ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ë¡œ ì‚¬ìš© (íŠ¹ìˆ˜ í† í° ì œì™¸)\n",
    "        input_ids = tokenized_input.input_ids[0]\n",
    "        valid_token_entropy = [e for e, i in zip(entropy[0], input_ids) if i not in tokenizer.all_special_ids]\n",
    "        \n",
    "        if valid_token_entropy:\n",
    "            scores.append(torch.mean(torch.stack(valid_token_entropy)).item())\n",
    "        else:\n",
    "            scores.append(0)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "# unlabeled ë°ì´í„° í’€ì´ ìˆë‹¤ê³  ê°€ì • (ì˜ˆ: dataset_cleaned.txtì—ì„œ ì¼ë¶€ ì‚¬ìš©)\n",
    "unlabeled_pool_path = os.path.join(data_dir, 'dataset_cleaned.txt')\n",
    "try:\n",
    "    with open(unlabeled_pool_path, 'r', encoding='utf-8') as f:\n",
    "        unlabeled_texts = f.read().split('\\n\\n')\n",
    "except FileNotFoundError:\n",
    "    unlabeled_texts = []\n",
    "\n",
    "if unlabeled_texts and inference_model is not None:\n",
    "    # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ ê³„ì‚°\n",
    "    uncertainty_scores = get_uncertainty_scores(unlabeled_texts, inference_model, inference_tokenizer)\n",
    "    \n",
    "    # ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "    sorted_indices = np.argsort(uncertainty_scores)[::-1]\n",
    "    \n",
    "    # ìƒìœ„ 10ê°œ ë°ì´í„°ë¥¼ ë¼ë²¨ë§ ëŒ€ìƒìœ¼ë¡œ ì„ ì •\n",
    "    num_to_label = 10\n",
    "    print(f\"\\nê°€ì¥ ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ìƒìœ„ {num_to_label}ê°œì˜ ë¬¸ì„œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤. (ë¼ë²¨ë§ ëŒ€ìƒ)\")\n",
    "    \n",
    "    to_label_texts = []\n",
    "    for i in sorted_indices[:num_to_label]:\n",
    "        print(f\"- Score: {uncertainty_scores[i]:.4f}, Text: {unlabeled_texts[i][:80]}...\")\n",
    "        to_label_texts.append(unlabeled_texts[i])\n",
    "\n",
    "    # ì„ ì •ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ Doccanoì— ì„í¬íŠ¸í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„\n",
    "    active_learning_output_path = os.path.join(data_dir, 'needs_labeling.txt')\n",
    "    with open(active_learning_output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n\\n'.join(to_label_texts))\n",
    "    print(f\"\\në¼ë²¨ë§ ëŒ€ìƒ íŒŒì¼ì´ '{active_learning_output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"\\nUnlabeled ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•„ Active Learningì„ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}